#!/bin/bash

################################################################################
# Distributed GPU Training Workflow
# Task #026.01: Distributed GPU Training Workflow
# Protocol: v2.2 (Configuration-as-Code, Loud Failures)
#
# Automates the Sync â†’ Train â†’ Retrieve loop for XGBoost training on remote GPU
#
# Usage:
#   bash scripts/run_remote_training.sh
#
# Environment:
#   - SSH configured for gpu-node (from Task #026.00)
#   - Remote GPU node accessible and ready
#   - requirements.txt available in project root
################################################################################

set -e  # Exit on any error

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Configuration
PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
REMOTE_HOST="gpu-node"
REMOTE_DIR="~/mt5-crs"
MODEL_FILE="models/deep_v1.json"

# ============================================================================
# Helper Functions
# ============================================================================

print_header() {
    echo ""
    echo "================================================================================"
    echo -e "${CYAN}$1${NC}"
    echo "================================================================================"
    echo ""
}

print_success() {
    echo -e "${GREEN}âœ… $1${NC}"
}

print_error() {
    echo -e "${RED}âŒ $1${NC}"
}

print_warning() {
    echo -e "${YELLOW}âš ï¸  $1${NC}"
}

print_info() {
    echo -e "${CYAN}â„¹ï¸  $1${NC}"
}

# ============================================================================
# Step 1: Pre-flight Checks
# ============================================================================

print_header "Step 1: Pre-flight Checks"

# Check SSH connectivity
print_info "Checking SSH connectivity to ${REMOTE_HOST}..."
if ! ssh -o ConnectTimeout=5 "${REMOTE_HOST}" "echo OK" > /dev/null 2>&1; then
    print_error "Cannot connect to ${REMOTE_HOST}"
    echo ""
    echo "Troubleshooting:"
    echo "  1. Verify SSH config: cat ~/.ssh/config | grep -A 5 gpu-node"
    echo "  2. Test connection: ssh ${REMOTE_HOST} nvidia-smi"
    echo "  3. Run setup: python3 scripts/ops_establish_gpu_link.py"
    exit 1
fi
print_success "SSH connection to ${REMOTE_HOST} verified"

# Check requirements.txt exists
if [ ! -f "${PROJECT_ROOT}/requirements.txt" ]; then
    print_error "requirements.txt not found"
    exit 1
fi
print_success "requirements.txt found"

# Check rsync is installed
if ! command -v rsync &> /dev/null; then
    print_error "rsync not installed"
    echo "Install with: sudo apt-get install rsync"
    exit 1
fi
print_success "rsync is available"

# Check scp is available
if ! command -v scp &> /dev/null; then
    print_error "scp not available"
    exit 1
fi
print_success "scp is available"

# ============================================================================
# Step 2: Sync Code to Remote
# ============================================================================

print_header "Step 2: Syncing Code to GPU Node"

print_info "Syncing ${PROJECT_ROOT}/ to ${REMOTE_HOST}:${REMOTE_DIR}/"
echo "Excluding: venv, .git, __pycache__, .pytest_cache, *.pyc"
echo ""

rsync -avz --exclude 'venv' --exclude '.git' --exclude '__pycache__' \
      --exclude '.pytest_cache' --exclude '*.pyc' --exclude '.env' \
      --exclude 'logs' --exclude '*.log' --exclude '*.tmp' \
      "${PROJECT_ROOT}/" "${REMOTE_HOST}:${REMOTE_DIR}/" \
      2>&1 | tail -20

if [ ${PIPESTATUS[0]} -ne 0 ]; then
    print_error "rsync failed"
    exit 1
fi

print_success "Code synced successfully"

# ============================================================================
# Step 3: Setup Remote Environment and Run Training
# ============================================================================

print_header "Step 3: Installing Dependencies and Running Training"

print_info "Connecting to ${REMOTE_HOST}..."
print_info "Installing dependencies (pip install)..."
print_info "This may take 2-5 minutes..."
echo ""

ssh "${REMOTE_HOST}" << 'REMOTE_EOF'
set -e

# Set strict error handling
cd ~/mt5-crs || exit 1

# Print system info
echo "=== System Information ==="
echo "Hostname: $(hostname)"
echo "Date: $(date)"
echo "Python: $(python3 --version)"
echo ""

# Install dependencies (show only last 10 lines)
echo "=== Installing Dependencies ==="
pip install -q -r requirements.txt 2>&1 | tail -10
echo "âœ… Dependencies installed"
echo ""

# Verify GPU availability
echo "=== GPU Status ==="
nvidia-smi --query-gpu=name,memory.total --format=csv,noheader
echo ""

# Check if training script exists
if [ ! -f "scripts/run_deep_training.py" ]; then
    echo "âŒ Training script not found: scripts/run_deep_training.py"
    exit 1
fi

# Run training with explicit GPU usage
echo "=== Starting Training ==="
echo "Training on GPU (this may take 5-15 minutes)..."
echo ""

python3 scripts/run_deep_training.py

if [ -f "${MODEL_FILE}" ]; then
    echo ""
    echo "âœ… Model saved successfully"
    ls -lh "${MODEL_FILE}"
else
    echo "âŒ Model file not found after training"
    exit 1
fi

REMOTE_EOF

if [ $? -ne 0 ]; then
    print_error "Remote training failed"
    exit 1
fi

print_success "Training completed on remote GPU node"

# ============================================================================
# Step 4: Retrieve Model from Remote
# ============================================================================

print_header "Step 4: Retrieving Trained Model"

print_info "Copying ${REMOTE_HOST}:${REMOTE_DIR}/${MODEL_FILE} to ./models/"

# Ensure local models directory exists
mkdir -p "${PROJECT_ROOT}/models"

# Copy model from remote
if ! scp "${REMOTE_HOST}:${REMOTE_DIR}/${MODEL_FILE}" "${PROJECT_ROOT}/models/" 2>&1; then
    print_error "scp failed to retrieve model"
    exit 1
fi

print_success "Model retrieved successfully"

# ============================================================================
# Step 5: Verification
# ============================================================================

print_header "Step 5: Verification"

LOCAL_MODEL="${PROJECT_ROOT}/${MODEL_FILE}"

# Check if model file exists
if [ ! -f "${LOCAL_MODEL}" ]; then
    print_error "Model file not found locally"
    exit 1
fi
print_success "Model file exists: ${MODEL_FILE}"

# Check file size
MODEL_SIZE=$(du -h "${LOCAL_MODEL}" | cut -f1)
print_success "Model size: ${MODEL_SIZE}"

# Check if it's valid JSON
if ! python3 -c "import json; json.load(open('${LOCAL_MODEL}'))" 2>/dev/null; then
    print_warning "Model file may not be valid JSON"
else
    print_success "Model file is valid JSON"
fi

# Try to load with XGBoost
if python3 -c "import xgboost as xgb; xgb.Booster(model_file='${LOCAL_MODEL}')" 2>/dev/null; then
    print_success "Model loads successfully with XGBoost"
else
    print_warning "Could not load model with XGBoost (may need GPU environment)"
fi

# Show model metadata
echo ""
print_info "Model file details:"
wc -l "${LOCAL_MODEL}"
head -100 "${LOCAL_MODEL}" | grep -E "tree|booster|node" | head -5

# ============================================================================
# Summary
# ============================================================================

print_header "Training Workflow Complete"

echo -e "${GREEN}ðŸŽ‰ Distributed GPU Training Success!${NC}"
echo ""
echo "Summary:"
echo "  âœ… Code synced to GPU node"
echo "  âœ… Dependencies installed on remote"
echo "  âœ… Training executed on NVIDIA A10 GPU"
echo "  âœ… Model retrieved to local ${MODEL_FILE}"
echo "  âœ… Model validation passed"
echo ""
echo "Model ready for:"
echo "  - Live trading (Task #025.01)"
echo "  - Backtesting and analysis"
echo "  - Further optimization and retraining"
echo ""
echo "Next steps:"
echo "  1. Deploy model to trading system"
echo "  2. Monitor trading performance"
echo "  3. Schedule regular retraining"
echo ""

exit 0
