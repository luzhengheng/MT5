#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI Review Cost Optimizer v1.0
ç»¼åˆæˆæœ¬ä¼˜åŒ–ï¼šç¼“å­˜ + æ‰¹å¤„ç† + æ™ºèƒ½è·¯ç”±

é›†æˆå¤šçº§ä¼˜åŒ–ç­–ç•¥ä»¥é™ä½ŽAIå®¡æŸ¥æˆæœ¬ï¼š
1. å¤šçº§ç¼“å­˜ - é¿å…é‡å¤å®¡æŸ¥
2. æ‰¹å¤„ç† - åˆå¹¶å¤šä¸ªè¯·æ±‚
3. æ™ºèƒ½è·¯ç”± - æŒ‰éœ€é€‰æ‹©æ¨¡åž‹
4. æˆæœ¬ç›‘æŽ§ - è·Ÿè¸ªæˆæœ¬å’Œæ•ˆæžœ
"""

import os
import sys
import json
from typing import Dict, List, Optional, Tuple, Callable
from datetime import datetime
from pathlib import Path

# å¯¼å…¥ä¼˜åŒ–å™¨æ¨¡å—
try:
    from review_cache import ReviewCache
    from review_batcher import ReviewBatcher, ReviewBatch
except ImportError:
    # æ”¯æŒä»Žå…¶ä»–ä½ç½®å¯¼å…¥
    sys.path.insert(0, os.path.dirname(__file__))
    from review_cache import ReviewCache
    from review_batcher import ReviewBatcher, ReviewBatch


class AIReviewCostOptimizer:
    """AIå®¡æŸ¥æˆæœ¬ä¼˜åŒ–å™¨"""

    def __init__(
        self,
        enable_cache: bool = True,
        enable_batch: bool = True,
        enable_routing: bool = True,
        cache_dir: str = ".cache/ai_review_cache",
        log_file: str = "cost_optimizer.log"
    ):
        """
        åˆå§‹åŒ–æˆæœ¬ä¼˜åŒ–å™¨

        Args:
            enable_cache: å¯ç”¨ç¼“å­˜
            enable_batch: å¯ç”¨æ‰¹å¤„ç†
            enable_routing: å¯ç”¨æ™ºèƒ½è·¯ç”±
            cache_dir: ç¼“å­˜ç›®å½•
            log_file: æ—¥å¿—æ–‡ä»¶
        """
        self.enable_cache = enable_cache
        self.enable_batch = enable_batch
        self.enable_routing = enable_routing
        self.log_file = log_file

        # åˆå§‹åŒ–ç»„ä»¶
        self.cache = ReviewCache(cache_dir=cache_dir) if enable_cache else None
        self.batcher = ReviewBatcher() if enable_batch else None

        # æˆæœ¬ç»Ÿè®¡
        self.stats = {
            'total_files': 0,
            'cached_files': 0,
            'uncached_files': 0,
            'api_calls': 0,
            'token_saved': 0,
            'cost_reduction_rate': 0.0,
            'start_time': datetime.now().isoformat()
        }

        self._log(f"âœ… Cost Optimizer initialized")
        self._log(f"   Cache: {'ON' if enable_cache else 'OFF'}")
        self._log(f"   Batch: {'ON' if enable_batch else 'OFF'}")
        self._log(f"   Routing: {'ON' if enable_routing else 'OFF'}")

    def _log(self, msg: str, level: str = "INFO") -> None:
        """è®°å½•æ—¥å¿—"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] [{level}] {msg}"

        # å†™å…¥æ–‡ä»¶
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(log_entry + '\n')

        # æ‰“å°åˆ°æŽ§åˆ¶å°
        print(log_entry)

    def process_files(
        self,
        files: List[str],
        api_caller: Callable,
        risk_detector: Optional[Callable] = None,
        force_refresh: bool = False
    ) -> Tuple[List[Dict], Dict]:
        """
        å¤„ç†æ–‡ä»¶å®¡æŸ¥ï¼Œåº”ç”¨æ‰€æœ‰ä¼˜åŒ–ç­–ç•¥

        Args:
            files: è¦å®¡æŸ¥çš„æ–‡ä»¶åˆ—è¡¨
            api_caller: APIè°ƒç”¨å‡½æ•° (batch: ReviewBatch -> Dict)
            risk_detector: é£Žé™©æ£€æµ‹å‡½æ•° (filepath, content -> risk_level)
            force_refresh: å¼ºåˆ¶åˆ·æ–°ç¼“å­˜

        Returns:
            (results: List[{filepath, result, source}], stats: Dict)
        """
        self.stats['total_files'] = len(files)
        self._log(f"Processing {len(files)} files...")

        results = []
        all_api_calls = 0
        all_tokens_saved = 0

        # =====================================================================
        # æ­¥éª¤ 1: ä½¿ç”¨ç¼“å­˜ (å¦‚æžœå¯ç”¨)
        # =====================================================================
        if self.enable_cache and not force_refresh:
            cached_files, uncached_files = self.cache.split(files)
            self.stats['cached_files'] = len(cached_files)
            self.stats['uncached_files'] = len(uncached_files)

            self._log(f"Cache hit: {len(cached_files)}/{len(files)} files")

            # ä»Žç¼“å­˜è¯»å–ç»“æžœ
            for filepath in cached_files:
                cached_result = self.cache.get(filepath)
                if cached_result:
                    results.append({
                        'filepath': filepath,
                        'result': cached_result,
                        'source': 'cache',
                        'cached': True
                    })
                    all_tokens_saved += 200  # ä¼°ç®—èŠ‚çœçš„Token

            files_to_review = uncached_files
        else:
            files_to_review = files
            self.stats['cached_files'] = 0
            self.stats['uncached_files'] = len(files)

        # =====================================================================
        # æ­¥éª¤ 2: æ‰¹å¤„ç† (å¦‚æžœå¯ç”¨)
        # =====================================================================
        if self.enable_batch and files_to_review:
            batches = self.batcher.create_batches(
                files_to_review,
                risk_detector=risk_detector,
                separate_by_risk=self.enable_routing
            )

            self._log(f"Created {len(batches)} batches for {len(files_to_review)} files")

            for batch in batches:
                self._log(f"Processing batch {batch.batch_id}: {len(batch.files)} files ({batch.risk_level})")

                # è°ƒç”¨API
                try:
                    batch_results = api_caller(batch)
                    all_api_calls += 1

                    # å¤„ç†æ‰¹å¤„ç†ç»“æžœ
                    if isinstance(batch_results, dict):
                        for filepath, result in batch_results.items():
                            if filepath in batch.files:
                                results.append({
                                    'filepath': filepath,
                                    'result': result,
                                    'source': 'api',
                                    'batch_id': batch.batch_id,
                                    'cached': False
                                })

                                # ä¿å­˜åˆ°ç¼“å­˜
                                if self.enable_cache:
                                    self.cache.save(filepath, result, {'batch_id': batch.batch_id})

                except Exception as e:
                    self._log(f"Batch API call failed: {e}", level="ERROR")
                    return results, self._update_stats(all_api_calls, all_tokens_saved, len(files))

        else:
            # ä¸ä½¿ç”¨æ‰¹å¤„ç†ï¼Œé€ä¸ªè°ƒç”¨
            for filepath in files_to_review:
                try:
                    # åˆ›å»ºå•æ–‡ä»¶æ‰¹æ¬¡
                    single_batch = ReviewBatch(
                        batch_id=f"single_{filepath}",
                        files=[filepath],
                        risk_level=self._detect_risk(filepath, risk_detector),
                        total_size=os.path.getsize(filepath),
                        file_contents={filepath: self._read_file(filepath)}
                    )

                    result = api_caller(single_batch)
                    all_api_calls += 1

                    results.append({
                        'filepath': filepath,
                        'result': result,
                        'source': 'api',
                        'cached': False
                    })

                    # ä¿å­˜åˆ°ç¼“å­˜
                    if self.enable_cache:
                        self.cache.save(filepath, result)

                except Exception as e:
                    self._log(f"API call failed for {filepath}: {e}", level="ERROR")

        # =====================================================================
        # æ­¥éª¤ 3: ç»Ÿè®¡å’ŒæŠ¥å‘Š
        # =====================================================================
        stats = self._update_stats(all_api_calls, all_tokens_saved, len(files))
        self._print_report(stats)

        return results, stats

    def _detect_risk(self, filepath: str, risk_detector: Optional[Callable]) -> str:
        """æ£€æµ‹æ–‡ä»¶é£Žé™©ç­‰çº§"""
        if not risk_detector:
            return 'low'

        try:
            content = self._read_file(filepath, max_size=5000)
            return risk_detector(filepath, content)
        except Exception:
            return 'low'

    @staticmethod
    def _read_file(filepath: str, max_size: int = 5000) -> str:
        """è¯»å–æ–‡ä»¶å†…å®¹"""
        try:
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                return f.read(max_size)
        except Exception:
            return ""

    def _update_stats(self, api_calls: int, tokens_saved: int, total_files: int) -> Dict:
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.stats['api_calls'] = api_calls
        self.stats['token_saved'] = tokens_saved

        # è®¡ç®—æˆæœ¬èŠ‚çœçŽ‡
        # å‡è®¾: æ— ä¼˜åŒ–ä¸‹ Nä¸ªæ–‡ä»¶ = Næ¬¡APIè°ƒç”¨
        # æœ‰ä¼˜åŒ–åŽ = api_calls æ¬¡è°ƒç”¨
        if total_files > 0:
            baseline_calls = total_files
            actual_calls = api_calls
            reduction = 1.0 - (actual_calls / baseline_calls) if baseline_calls > 0 else 0
            self.stats['cost_reduction_rate'] = max(0, min(1, reduction))

        self.stats['end_time'] = datetime.now().isoformat()

        return self.stats

    def _print_report(self, stats: Dict) -> None:
        """æ‰“å°æˆæœ¬ä¼˜åŒ–æŠ¥å‘Š"""
        print("\n" + "=" * 80)
        print("ðŸ“Š AI Review Cost Optimization Report")
        print("=" * 80)
        print(f"Total Files: {stats['total_files']}")
        print(f"Cached Files: {stats['cached_files']}")
        print(f"Uncached Files: {stats['uncached_files']}")
        print(f"API Calls: {stats['api_calls']}")
        print(f"Tokens Saved: {stats['token_saved']}")
        print(f"Cost Reduction: {stats['cost_reduction_rate']:.1%}")
        print("=" * 80 + "\n")

    def get_cache_stats(self) -> Dict:
        """èŽ·å–ç¼“å­˜ç»Ÿè®¡"""
        if self.cache:
            return self.cache.get_stats()
        return {}

    def clear_cache(self) -> None:
        """æ¸…é™¤æ‰€æœ‰ç¼“å­˜"""
        if self.cache:
            self.cache.clear()
            self._log("Cache cleared")

    def cleanup_expired_cache(self) -> int:
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        if self.cache:
            count = self.cache.cleanup_expired()
            self._log(f"Cleaned up {count} expired cache entries")
            return count
        return 0


# ============================================================================
# ä½¿ç”¨ç¤ºä¾‹
# ============================================================================

def example_api_caller(batch: ReviewBatch) -> Dict:
    """ç¤ºä¾‹APIè°ƒç”¨å‡½æ•°"""
    # è¿™åº”è¯¥è¢«æ›¿æ¢ä¸ºå®žé™…çš„APIè°ƒç”¨
    results = {}
    for filepath in batch.files:
        results[filepath] = {
            'status': 'PASS',
            'risk_level': batch.risk_level,
            'message': f'Reviewed via {batch.batch_id}'
        }
    return results


def example_risk_detector(filepath: str, content: str) -> str:
    """ç¤ºä¾‹é£Žé™©æ£€æµ‹å‡½æ•°"""
    if any(keyword in filepath for keyword in ['execution', 'strategy', 'deploy']):
        return 'high'
    return 'low'


if __name__ == "__main__":
    # æ¼”ç¤º
    optimizer = AIReviewCostOptimizer()

    test_files = [
        "scripts/execution/risk.py",
        "README.md",
    ]

    # è¿‡æ»¤å­˜åœ¨çš„æ–‡ä»¶
    existing_files = [f for f in test_files if os.path.exists(f)]

    if existing_files:
        results, stats = optimizer.process_files(
            existing_files,
            api_caller=example_api_caller,
            risk_detector=example_risk_detector
        )

        print(f"\nProcessed {len(results)} files with {stats['cost_reduction_rate']:.1%} cost reduction")
