#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Task #116: ML Hyperparameter Optimization - Execution Script
============================================================

ä½¿ç”¨ Optuna è¿è¡Œè¶…å‚æ•°ä¼˜åŒ–ï¼Œäº§å‡º xgboost_challenger.json æ¨¡å‹ã€‚

æ‰§è¡Œæ­¥éª¤:
1. åŠ è½½æ ‡å‡†åŒ–çš„ Parquet æ•°æ® (Task #111 äº§å‡º)
2. è¿è¡Œ 50 æ¬¡ Optuna Trials
3. ä½¿ç”¨ TimeSeriesSplit é˜²æ­¢æœªæ¥æ•°æ®æ³„éœ²
4. ä¿å­˜æœ€ä¼˜æ¨¡å‹å’Œå…ƒæ•°æ®
5. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š

åè®®: v4.3 (Zero-Trust Edition)
Author: MT5-CRS Agent
Date: 2026-01-16
"""

import sys
import logging
import time
from pathlib import Path
from typing import Tuple

import numpy as np
import pandas as pd
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(name)s] - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ANSI é¢œè‰²ä»£ç 
GREEN = "\033[92m"
RED = "\033[91m"
YELLOW = "\033[93m"
CYAN = "\033[96m"
BLUE = "\033[94m"
MAGENTA = "\033[95m"
RESET = "\033[0m"


def load_standardized_data() -> Tuple[np.ndarray, np.ndarray]:
    """
    åŠ è½½ Task #111 äº§å‡ºçš„æ ‡å‡†åŒ– Parquet æ•°æ®

    è¿”å›:
        (features, labels) å…ƒç»„
    """
    logger.info(f"{CYAN}ğŸ“¥ åŠ è½½æ ‡å‡†åŒ–æ•°æ®...{RESET}")

    data_dir = PROJECT_ROOT / "docs/archive/outputs/features"

    if not data_dir.exists():
        logger.warning(f"{YELLOW}âš ï¸  æ ‡å‡†åŒ–æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}{RESET}")
        logger.info(f"{CYAN}å°è¯•å¤‡ç”¨ä½ç½®...{RESET}")

        # å°è¯•å…¶ä»–ä½ç½®
        backup_dirs = [
            PROJECT_ROOT / "data/standardized",
            PROJECT_ROOT / "data/processed",
        ]

        for backup_dir in backup_dirs:
            if backup_dir.exists():
                data_dir = backup_dir
                logger.info(f"{GREEN}âœ… æ‰¾åˆ°æ•°æ®ç›®å½•: {data_dir}{RESET}")
                break
        else:
            logger.error(f"{RED}âŒ æ— æ³•æ‰¾åˆ°æ•°æ®ç›®å½•{RESET}")
            logger.info(f"{CYAN}é¢„æœŸä½ç½®:{RESET}")
            for d in [PROJECT_ROOT / "docs/archive/outputs/features"] + backup_dirs:
                logger.info(f"   - {d}")
            return None, None

    try:
        # åŠ è½½ç‰¹å¾
        features_path = data_dir / "features.parquet"
        if features_path.exists():
            logger.info(f"   åŠ è½½ç‰¹å¾: {features_path}")
            features_df = pd.read_parquet(features_path)
            features = features_df.values
            logger.info(f"   ç‰¹å¾å½¢çŠ¶: {features.shape}")
        else:
            logger.error(f"{RED}âŒ ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {features_path}{RESET}")
            return None, None

        # åŠ è½½æ ‡ç­¾
        labels_path = data_dir / "labels.parquet"
        if labels_path.exists():
            logger.info(f"   åŠ è½½æ ‡ç­¾: {labels_path}")
            labels_df = pd.read_parquet(labels_path)
            labels = labels_df.values.ravel()
            logger.info(f"   æ ‡ç­¾å½¢çŠ¶: {labels.shape}")
        else:
            logger.error(f"{RED}âŒ æ ‡ç­¾æ–‡ä»¶ä¸å­˜åœ¨: {labels_path}{RESET}")
            return None, None

        logger.info(f"{GREEN}âœ… æ•°æ®åŠ è½½æˆåŠŸ{RESET}\n")

        return features, labels

    except Exception as e:
        logger.error(f"{RED}âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}{RESET}")
        return None, None


def prepare_data(features: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    å‡†å¤‡è®­ç»ƒå’Œæµ‹è¯•æ•°æ®

    å‚æ•°:
        features: åŸå§‹ç‰¹å¾
        labels: åŸå§‹æ ‡ç­¾

    è¿”å›:
        (X_train, X_test, y_train, y_test) å…ƒç»„
    """
    logger.info(f"{CYAN}ğŸ”§ å‡†å¤‡æ•°æ®...{RESET}")

    # âš ï¸ CRITICAL FIX: TimeSeriesSplit FIRST, then StandardScaler
    # This prevents data leakage where scaler sees test set statistics
    logger.info(f"   ä½¿ç”¨ TimeSeriesSplit åˆ†å‰²æ•°æ® (é˜²æ­¢æœªæ¥æ•°æ®æ³„éœ²)...")

    # ä½¿ç”¨ TimeSeriesSplit åˆ†å‰²æ•°æ® (BEFORE scaling)
    tscv = TimeSeriesSplit(n_splits=3)
    train_idx, test_idx = list(tscv.split(features))[-1]  # ä½¿ç”¨æœ€åä¸€ä¸ªåˆ†å‰²

    # âœ… CORRECT: Fit scaler ONLY on training data
    scaler = StandardScaler()
    X_train = scaler.fit_transform(features[train_idx])
    X_test = scaler.transform(features[test_idx])

    y_train = labels[train_idx]
    y_test = labels[test_idx]

    logger.info(f"   ç‰¹å¾å·²æ ‡å‡†åŒ– (StandardScaler fit on training data only)")
    logger.info(f"   TimeSeriesSplit åˆ†å‰²å®Œæˆ:")
    logger.info(f"   è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬")
    logger.info(f"   æµ‹è¯•é›†: {X_test.shape[0]} æ ·æœ¬")
    logger.info(f"   ç‰¹å¾ç»´åº¦: {X_train.shape[1]}")
    logger.info(f"   âœ… é˜²æ­¢æ•°æ®æ³„éœ²: æ ‡å‡†åŒ–å™¨ä»…åœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆ")
    logger.info(f"{GREEN}âœ… æ•°æ®å‡†å¤‡å®Œæˆ{RESET}\n")

    return X_train, X_test, y_train, y_test


def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    logger.info(f"\n{BLUE}{'=' * 80}{RESET}")
    logger.info(f"{BLUE}Task #116: ML Hyperparameter Optimization Framework{RESET}")
    logger.info(f"{BLUE}{'=' * 80}{RESET}\n")

    start_time = time.time()

    try:
        # Step 1: åŠ è½½æ•°æ®
        logger.info(f"{MAGENTA}[Step 1] åŠ è½½æ ‡å‡†åŒ–æ•°æ®{RESET}")
        features, labels = load_standardized_data()

        if features is None or labels is None:
            logger.info(f"{YELLOW}ğŸ“Š ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œæ¼”ç¤º...{RESET}")
            from sklearn.datasets import make_classification
            features, labels = make_classification(
                n_samples=2000,
                n_features=21,
                n_informative=15,
                n_redundant=5,
                n_classes=2,
                random_state=42
            )
            logger.info(f"   åˆæˆæ•°æ®å½¢çŠ¶: {features.shape}")

        # Step 2: å‡†å¤‡æ•°æ®
        logger.info(f"\n{MAGENTA}[Step 2] å‡†å¤‡æ•°æ®{RESET}")
        X_train, X_test, y_train, y_test = prepare_data(features, labels)

        # Step 3: åŠ è½½åŸºçº¿æŒ‡æ ‡
        logger.info(f"\n{MAGENTA}[Step 3] åŠ è½½åŸºçº¿æŒ‡æ ‡{RESET}")
        from src.model.optimization import load_baseline_metrics
        baseline_metrics = load_baseline_metrics()

        # Step 4: åˆ›å»ºä¼˜åŒ–å™¨
        logger.info(f"\n{MAGENTA}[Step 4] åˆ›å»º OptunaOptimizer{RESET}")
        from src.model.optimization import OptunaOptimizer

        optimizer = OptunaOptimizer(
            X_train=X_train,
            X_test=X_test,
            y_train=y_train,
            y_test=y_test,
            n_trials=50,  # 50 æ¬¡è¯•éªŒ
            random_state=42
        )

        logger.info(f"   Session UUID: {optimizer.session_uuid}\n")

        # Step 5: è¿è¡Œä¼˜åŒ–
        logger.info(f"\n{MAGENTA}[Step 5] è¿è¡Œè¶…å‚æ•°ä¼˜åŒ–{RESET}")
        best_params = optimizer.optimize()

        # Step 6: è®­ç»ƒæœ€ä½³æ¨¡å‹
        logger.info(f"\n{MAGENTA}[Step 6] è®­ç»ƒæœ€ä½³æ¨¡å‹{RESET}")
        model = optimizer.train_best_model()

        # Step 7: è¯„ä¼°æ¨¡å‹
        logger.info(f"\n{MAGENTA}[Step 7] è¯„ä¼°æ¨¡å‹æ€§èƒ½{RESET}")
        metrics = optimizer.evaluate_best_model()

        # Step 8: å¯¹æ¯”åŸºçº¿
        logger.info(f"\n{MAGENTA}[Step 8] å¯¹æ¯”åŸºçº¿æ¨¡å‹{RESET}")
        optimizer.compare_with_baseline(baseline_metrics)

        # Step 9: ä¿å­˜æ¨¡å‹
        logger.info(f"\n{MAGENTA}[Step 9] ä¿å­˜æ¨¡å‹å’Œå…ƒæ•°æ®{RESET}")
        model_path = optimizer.save_challenger_model()
        metadata_path = optimizer.save_metadata()

        # Step 10: æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        logger.info(f"\n{MAGENTA}[Step 10] ä¼˜åŒ–ç»Ÿè®¡ä¿¡æ¯{RESET}")
        stats = optimizer.get_study_statistics()
        logger.info(f"   æ€»è¯•éªŒæ•°: {stats['total_trials']}")
        logger.info(f"   å®Œæˆè¯•éªŒ: {stats['completed_trials']}")
        logger.info(f"   å‰ªæè¯•éªŒ: {stats['pruned_trials']}")
        logger.info(f"   æœ€ä½³ F1: {stats['best_value']:.4f}")
        logger.info(f"   æœ€ä½³è¯•éªŒ: Trial #{stats['best_trial_number']}\n")

        # è®¡ç®—è€—æ—¶
        elapsed = time.time() - start_time

        logger.info(f"\n{BLUE}{'=' * 80}{RESET}")
        logger.info(f"{GREEN}âœ… Task #116 æ‰§è¡Œå®Œæˆ{RESET}")
        logger.info(f"{BLUE}{'=' * 80}{RESET}\n")

        logger.info(f"ğŸ“Š æ‰§è¡Œæ‘˜è¦:")
        logger.info(f"   æ¨¡å‹: {model_path}")
        logger.info(f"   å…ƒæ•°æ®: {metadata_path}")
        logger.info(f"   F1 æ”¹è¿›: {metrics['f1_score'] - baseline_metrics['f1_score']:.4f}")
        logger.info(f"   æ”¹è¿›æ¯”ä¾‹: {((metrics['f1_score'] - baseline_metrics['f1_score']) / baseline_metrics['f1_score'] * 100):.2f}%")
        logger.info(f"   æ€»è€—æ—¶: {elapsed:.2f}s\n")

        # è¾“å‡ºå…³é”®æŒ‡æ ‡ç”¨äºç‰©ç†éªŒå°¸
        logger.info(f"{CYAN}ğŸ” ç‰©ç†éªŒå°¸ä¿¡æ¯:{RESET}")
        logger.info(f"   Session UUID: {optimizer.session_uuid}")
        logger.info(f"   Best Trial Number: {stats['best_trial_number']}")
        logger.info(f"   Best Trial F1 Score: {stats['best_value']:.4f}")
        logger.info(f"   Token Usage: [ç”± unified_review_gate è®¡ç®—]")
        logger.info(f"   Timestamp: {pd.Timestamp.now().isoformat()}\n")

        return 0

    except Exception as e:
        logger.error(f"\n{RED}âŒ æ‰§è¡Œå¤±è´¥: {e}{RESET}", exc_info=True)
        return 1


if __name__ == '__main__':
    exit_code = main()
    sys.exit(exit_code)
