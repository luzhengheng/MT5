#!/usr/bin/env python3
"""
æœºå™¨å­¦ä¹ è®­ç»ƒä¸»è„šæœ¬

æ•´åˆ #009 å·¥å•çš„æ‰€æœ‰ç»„ä»¶:
1. æ•°æ®åŠ è½½ä¸ç‰¹å¾å·¥ç¨‹
2. ç‰¹å¾èšç±»ä¸é€‰æ‹©
3. PurgedKFold éªŒè¯
4. Optuna è¶…å‚æ•°ä¼˜åŒ–
5. æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°
6. SHAP åˆ†æ

ç”¨æ³•:
    python bin/run_training.py --mode train --use-optuna --n-trials 50
    python bin/run_training.py --mode eval --model-path outputs/models/best_model.pkl
"""

import sys
import os
import argparse
import logging
from pathlib import Path
from datetime import datetime
import warnings

warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

import numpy as np
import pandas as pd
from src.models.validation import PurgedKFold, WalkForwardValidator
from src.models.feature_selection import FeatureClusterer, MDFeatureImportance
from src.models.trainer import LightGBMTrainer, OptunaOptimizer
from src.models.evaluator import ModelEvaluator

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class MLPipeline:
    """
    æœºå™¨å­¦ä¹ è®­ç»ƒç®¡é“

    æ•´åˆæ‰€æœ‰ç»„ä»¶,æä¾›ç«¯åˆ°ç«¯çš„è®­ç»ƒæµç¨‹
    """

    def __init__(
        self,
        data_path: str,
        output_dir: str = '/opt/mt5-crs/outputs',
        use_purged_kfold: bool = True,
        use_feature_clustering: bool = True
    ):
        """
        Args:
            data_path: ç‰¹å¾æ•°æ®è·¯å¾„ (Parquet æ ¼å¼)
            output_dir: è¾“å‡ºç›®å½•
            use_purged_kfold: æ˜¯å¦ä½¿ç”¨ PurgedKFold
            use_feature_clustering: æ˜¯å¦ä½¿ç”¨ç‰¹å¾èšç±»
        """
        self.data_path = data_path
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        self.use_purged_kfold = use_purged_kfold
        self.use_feature_clustering = use_feature_clustering

        self.model_dir = self.output_dir / 'models'
        self.plot_dir = self.output_dir / 'plots'
        self.model_dir.mkdir(exist_ok=True)
        self.plot_dir.mkdir(exist_ok=True)

        self.data = None
        self.features = None
        self.selected_features = None

    def load_data(self):
        """åŠ è½½ç‰¹å¾æ•°æ®"""
        logger.info(f"åŠ è½½æ•°æ®: {self.data_path}")

        if not Path(self.data_path).exists():
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.data_path}")

        self.data = pd.read_parquet(self.data_path)
        logger.info(f"æ•°æ®åŠ è½½å®Œæˆ: {self.data.shape}")

        # æ£€æŸ¥å¿…è¦çš„åˆ—
        required_cols = ['label', 'sample_weight']
        for col in required_cols:
            if col not in self.data.columns:
                raise ValueError(f"ç¼ºå°‘å¿…è¦çš„åˆ—: {col}")

        # ç¡®ä¿ç´¢å¼•æ˜¯æ—¥æœŸç±»å‹
        if not isinstance(self.data.index, pd.DatetimeIndex):
            if 'date' in self.data.columns:
                self.data['date'] = pd.to_datetime(self.data['date'])
                self.data.set_index('date', inplace=True)
            else:
                raise ValueError("æ•°æ®å¿…é¡»æœ‰æ—¥æœŸç´¢å¼•æˆ– 'date' åˆ—")

        logger.info(f"æ•°æ®æ—¶é—´èŒƒå›´: {self.data.index.min()} è‡³ {self.data.index.max()}")

        # æ’é™¤éç‰¹å¾åˆ—
        exclude_cols = [
            'label', 'sample_weight', 'symbol',
            'barrier_touched', 'holding_period', 'event_end_time',
            'close', 'open', 'high', 'low', 'volume'  # æ’é™¤åŸå§‹ä»·æ ¼
        ]

        self.features = [
            col for col in self.data.columns
            if col not in exclude_cols and self.data[col].dtype in ['float64', 'int64']
        ]

        logger.info(f"ç‰¹å¾æ•°é‡: {len(self.features)}")
        logger.info(f"æ ‡ç­¾åˆ†å¸ƒ: \n{self.data['label'].value_counts()}")

    def select_features(self):
        """ç‰¹å¾èšç±»ä¸é€‰æ‹©"""
        if not self.use_feature_clustering:
            self.selected_features = self.features
            return

        logger.info("=" * 80)
        logger.info("ç‰¹å¾èšç±»ä¸é€‰æ‹©")
        logger.info("=" * 80)

        X = self.data[self.features].fillna(0)

        # ç‰¹å¾èšç±»
        clusterer = FeatureClusterer(correlation_threshold=0.7)
        clusterer.fit(X)

        # ç»˜åˆ¶æ ‘çŠ¶å›¾
        clusterer.plot_dendrogram(
            self.features,
            output_path=str(self.plot_dir / 'feature_dendrogram.png')
        )

        # ç®€å•é€‰æ‹©: æ¯ç»„å–ç¬¬ä¸€ä¸ªç‰¹å¾ (åç»­å¯ç”¨ MDA ä¼˜åŒ–)
        self.selected_features = []
        for cluster_id, features in clusterer.clusters.items():
            self.selected_features.append(features[0])

        logger.info(f"ç‰¹å¾é€‰æ‹©å®Œæˆ: {len(self.features)} -> {len(self.selected_features)}")

    def train_with_optuna(
        self,
        n_trials: int = 50,
        metric: str = 'f1'
    ) -> LightGBMTrainer:
        """ä½¿ç”¨ Optuna ä¼˜åŒ–è¶…å‚æ•°"""
        logger.info("=" * 80)
        logger.info(f"Optuna è¶…å‚æ•°ä¼˜åŒ– ({n_trials} æ¬¡è¯•éªŒ)")
        logger.info("=" * 80)

        # å‡†å¤‡æ•°æ®
        X = self.data[self.selected_features].fillna(0)
        y = self.data['label']
        sample_weight = self.data['sample_weight']

        # ç®€å•æ—¶é—´åˆ†å‰² (80% è®­ç»ƒ, 20% éªŒè¯)
        split_idx = int(len(X) * 0.8)
        X_train, X_val = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train, y_val = y.iloc[:split_idx], y.iloc[split_idx:]
        sw_train, sw_val = sample_weight.iloc[:split_idx], sample_weight.iloc[split_idx:]

        # Optuna ä¼˜åŒ–
        optimizer = OptunaOptimizer(n_trials=n_trials, direction='maximize')
        best_params = optimizer.optimize(
            X_train, y_train, X_val, y_val,
            sw_train, sw_val, metric=metric
        )

        # ä¿å­˜ä¼˜åŒ–å†å²
        optimizer.save_study(str(self.output_dir / 'optuna_study.csv'))

        # ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        logger.info("ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
        trainer = LightGBMTrainer(params=best_params, early_stopping_rounds=50)
        trainer.train(
            X_train, y_train, X_val, y_val,
            sw_train, sw_val, num_boost_round=1000
        )

        return trainer

    def train_with_purged_kfold(
        self,
        params: dict = None,
        n_splits: int = 5
    ) -> LightGBMTrainer:
        """ä½¿ç”¨ PurgedKFold è®­ç»ƒ"""
        logger.info("=" * 80)
        logger.info(f"PurgedKFold è®­ç»ƒ ({n_splits} æŠ˜)")
        logger.info("=" * 80)

        # å‡†å¤‡æ•°æ®
        X = self.data[self.selected_features].fillna(0)
        y = self.data['label']
        sample_weight = self.data['sample_weight']

        # è·å–äº‹ä»¶ç»“æŸæ—¶é—´
        if 'event_end_time' in self.data.columns:
            event_ends = pd.to_datetime(self.data['event_end_time'])
        else:
            logger.warning("æœªæ‰¾åˆ° 'event_end_time',å‡è®¾æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹")
            event_ends = None

        # PurgedKFold
        pkf = PurgedKFold(n_splits=n_splits, embargo_pct=0.01, purge_overlap=True)

        # K-Fold è®­ç»ƒ
        fold_metrics = []
        best_model = None
        best_f1 = 0

        for fold, (train_idx, val_idx) in enumerate(pkf.split(X, y, event_ends), 1):
            logger.info(f"\n{'='*80}")
            logger.info(f"Fold {fold}/{n_splits}")
            logger.info(f"{'='*80}")

            X_train = X.iloc[train_idx]
            X_val = X.iloc[val_idx]
            y_train = y.iloc[train_idx]
            y_val = y.iloc[val_idx]
            sw_train = sample_weight.iloc[train_idx]
            sw_val = sample_weight.iloc[val_idx]

            # è®­ç»ƒæ¨¡å‹
            trainer = LightGBMTrainer(params=params, early_stopping_rounds=30, verbose=-1)
            trainer.train(
                X_train, y_train, X_val, y_val,
                sw_train, sw_val, num_boost_round=500
            )

            # è¯„ä¼°
            y_pred = trainer.predict(X_val)
            y_pred_proba = trainer.predict_proba(X_val)[:, 1]

            from sklearn.metrics import f1_score
            fold_f1 = f1_score(y_val, y_pred, average='weighted')
            fold_metrics.append(fold_f1)

            logger.info(f"Fold {fold} F1-Score: {fold_f1:.4f}")

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if fold_f1 > best_f1:
                best_f1 = fold_f1
                best_model = trainer

        logger.info(f"\nå¹³å‡ F1-Score: {np.mean(fold_metrics):.4f} Â± {np.std(fold_metrics):.4f}")

        return best_model

    def evaluate_model(
        self,
        trainer: LightGBMTrainer,
        prefix: str = 'final'
    ):
        """è¯„ä¼°æ¨¡å‹"""
        logger.info("=" * 80)
        logger.info("æ¨¡å‹è¯„ä¼°")
        logger.info("=" * 80)

        # å‡†å¤‡æµ‹è¯•é›† (æœ€å 20% æ•°æ®)
        X = self.data[self.selected_features].fillna(0)
        y = self.data['label']

        split_idx = int(len(X) * 0.8)
        X_test = X.iloc[split_idx:]
        y_test = y.iloc[split_idx:]

        # é¢„æµ‹
        y_pred = trainer.predict(X_test)
        y_pred_proba = trainer.predict_proba(X_test)[:, 1]

        # è¯„ä¼°
        evaluator = ModelEvaluator(output_dir=str(self.plot_dir))
        metrics = evaluator.evaluate(y_test, y_pred, y_pred_proba, prefix=prefix)

        # SHAP åˆ†æ (å¯é€‰,è€—æ—¶è¾ƒé•¿)
        logger.info("\nè®¡ç®— SHAP å€¼...")
        try:
            evaluator.plot_shap_summary(
                trainer.model,
                X_test.sample(min(1000, len(X_test))),  # æœ€å¤š 1000 æ ·æœ¬
                prefix=prefix
            )
        except Exception as e:
            logger.warning(f"SHAP åˆ†æå¤±è´¥: {e}")

        # ç‰¹å¾é‡è¦æ€§
        importance = trainer.get_feature_importance()
        importance_df = pd.DataFrame({
            'feature': importance.index,
            'importance': importance.values
        })
        importance_df.to_csv(self.output_dir / 'feature_importance.csv', index=False)
        logger.info(f"\nç‰¹å¾é‡è¦æ€§å·²ä¿å­˜: {self.output_dir / 'feature_importance.csv'}")

        return metrics

    def save_model(self, trainer: LightGBMTrainer, model_name: str = 'best_model'):
        """ä¿å­˜æ¨¡å‹"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        model_path = self.model_dir / f'{model_name}_{timestamp}.pkl'

        metadata = {
            'features': self.selected_features,
            'n_features': len(self.selected_features),
            'training_date': timestamp,
            'data_shape': self.data.shape
        }

        trainer.save(str(model_path), metadata=metadata)
        logger.info(f"æ¨¡å‹å·²ä¿å­˜: {model_path}")


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='MT5-CRS æœºå™¨å­¦ä¹ è®­ç»ƒè„šæœ¬')

    parser.add_argument(
        '--mode', type=str, default='train',
        choices=['train', 'optuna', 'eval'],
        help='è¿è¡Œæ¨¡å¼: train (PurgedKFoldè®­ç»ƒ), optuna (è¶…å‚æ•°ä¼˜åŒ–), eval (ä»…è¯„ä¼°)'
    )
    parser.add_argument(
        '--data-path', type=str,
        default='/opt/mt5-crs/data/features/combined_features.parquet',
        help='ç‰¹å¾æ•°æ®è·¯å¾„'
    )
    parser.add_argument(
        '--output-dir', type=str,
        default='/opt/mt5-crs/outputs',
        help='è¾“å‡ºç›®å½•'
    )
    parser.add_argument(
        '--n-trials', type=int, default=50,
        help='Optuna è¯•éªŒæ¬¡æ•°'
    )
    parser.add_argument(
        '--n-splits', type=int, default=5,
        help='PurgedKFold åˆ†å‰²æ•°'
    )
    parser.add_argument(
        '--no-feature-clustering', action='store_true',
        help='ç¦ç”¨ç‰¹å¾èšç±»'
    )

    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()

    logger.info("=" * 80)
    logger.info("MT5-CRS æœºå™¨å­¦ä¹ è®­ç»ƒç®¡é“")
    logger.info("å·¥å• #009: æœºå™¨å­¦ä¹ é¢„æµ‹å¼•æ“ä¸é«˜çº§éªŒè¯ä½“ç³»")
    logger.info("=" * 80)

    # åˆ›å»ºç®¡é“
    pipeline = MLPipeline(
        data_path=args.data_path,
        output_dir=args.output_dir,
        use_purged_kfold=True,
        use_feature_clustering=not args.no_feature_clustering
    )

    # åŠ è½½æ•°æ®
    pipeline.load_data()

    # ç‰¹å¾é€‰æ‹©
    pipeline.select_features()

    # è®­ç»ƒæ¨¡å¼
    if args.mode == 'optuna':
        trainer = pipeline.train_with_optuna(n_trials=args.n_trials)
    elif args.mode == 'train':
        trainer = pipeline.train_with_purged_kfold(n_splits=args.n_splits)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å¼: {args.mode}")

    # è¯„ä¼°æ¨¡å‹
    metrics = pipeline.evaluate_model(trainer)

    # ä¿å­˜æ¨¡å‹
    pipeline.save_model(trainer)

    logger.info("=" * 80)
    logger.info("è®­ç»ƒå®Œæˆ! ğŸ‰")
    logger.info("=" * 80)


if __name__ == '__main__':
    try:
        main()
    except FileNotFoundError as e:
        logger.error(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {e}")
        logger.info("\næç¤º: è¯·å…ˆè¿è¡Œç‰¹å¾å·¥ç¨‹ç”Ÿæˆæ•°æ®:")
        logger.info("  python -m src.feature_engineering.feature_engineer")
        sys.exit(1)
    except Exception as e:
        logger.exception(f"è®­ç»ƒå¤±è´¥: {e}")
        sys.exit(1)
