#!/usr/bin/env python3
"""
MT5-CRS æœºå™¨å­¦ä¹ è®­ç»ƒè„šæœ¬

å¯¹å†²åŸºé‡‘çº§åˆ«çš„é¢„æµ‹å¼•æ“è®­ç»ƒç®¡é“:
1. åŠ è½½ç‰¹å¾ä¸æ ‡ç­¾ (æ¥è‡ª #008 å·¥å•)
2. åº”ç”¨ Purged K-Fold äº¤å‰éªŒè¯
3. ç‰¹å¾å»å™ªä¸èšç±»
4. è®­ç»ƒ Ensemble Stacking æ¨¡å‹
5. Optuna è¶…å‚æ•°ä¼˜åŒ– (å¯é€‰)
6. å…¨é¢è¯„ä¼°ä¸æŠ¥å‘Š

ç†è®ºåŸºç¡€: "Advances in Financial Machine Learning" by Marcos Lopez de Prado

ç”¨æ³•:
    python bin/train_ml_model.py
    python bin/train_ml_model.py --config config/custom_config.yaml
    python bin/train_ml_model.py --quick  # å¿«é€Ÿæµ‹è¯•æ¨¡å¼
"""

import sys
import os
import logging
import argparse
import yaml
import json
import warnings
from pathlib import Path
from datetime import datetime

import numpy as np
import pandas as pd

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.models.validation import PurgedKFold, WalkForwardValidator
from src.models.feature_selection import FeatureClusterer, MDFeatureImportance
from src.models.trainer import (
    LightGBMTrainer,
    CatBoostTrainer,
    EnsembleStacker,
    OptunaOptimizer,
    CATBOOST_AVAILABLE
)
from src.models.evaluator import ModelEvaluator

warnings.filterwarnings('ignore')


class MLTrainingPipeline:
    """æœºå™¨å­¦ä¹ è®­ç»ƒç®¡é“"""

    def __init__(self, config_path: str):
        """
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = self._load_config(config_path)
        self._setup_logging()
        self._setup_output_dirs()

        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.sample_weights = None
        self.pred_times = None

        self.model = None
        self.feature_names = None

        logger.info("=" * 80)
        logger.info("MT5-CRS æœºå™¨å­¦ä¹ è®­ç»ƒç®¡é“")
        logger.info("=" * 80)

    def _load_config(self, config_path: str) -> dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config

    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_config = self.config.get('logging', {})
        log_level = getattr(logging, log_config.get('level', 'INFO'))

        # é…ç½® logger
        logging.basicConfig(
            level=log_level,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[]
        )

        global logger
        logger = logging.getLogger(__name__)

        # æ§åˆ¶å°è¾“å‡º
        if log_config.get('console', True):
            console_handler = logging.StreamHandler()
            console_handler.setLevel(log_level)
            logger.addHandler(console_handler)

        # æ–‡ä»¶è¾“å‡º
        if log_config.get('file', True):
            log_file = Path(log_config.get('file_path', 'outputs/logs/training.log'))
            log_file.parent.mkdir(parents=True, exist_ok=True)
            file_handler = logging.FileHandler(log_file)
            file_handler.setLevel(log_level)
            logger.addHandler(file_handler)

    def _setup_output_dirs(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•"""
        output_config = self.config.get('output', {})

        for key in ['model_dir', 'plots_dir', 'logs_dir']:
            path = Path(output_config.get(key, f'outputs/{key}'))
            path.mkdir(parents=True, exist_ok=True)

    def run(self):
        """æ‰§è¡Œå®Œæ•´è®­ç»ƒæµç¨‹"""
        try:
            logger.info("\n[æ­¥éª¤ 1/7] åŠ è½½æ•°æ®...")
            self.load_data()

            logger.info("\n[æ­¥éª¤ 2/7] æ•°æ®é¢„å¤„ç†...")
            self.preprocess_data()

            logger.info("\n[æ­¥éª¤ 3/7] ç‰¹å¾é€‰æ‹©ä¸å»å™ª...")
            self.feature_selection()

            logger.info("\n[æ­¥éª¤ 4/7] é…ç½®éªŒè¯ç­–ç•¥...")
            cv_splitter = self.setup_validation()

            logger.info("\n[æ­¥éª¤ 5/7] è®­ç»ƒæ¨¡å‹...")
            self.train_model(cv_splitter)

            logger.info("\n[æ­¥éª¤ 6/7] è¯„ä¼°æ¨¡å‹...")
            self.evaluate_model()

            logger.info("\n[æ­¥éª¤ 7/7] ä¿å­˜æ¨¡å‹ä¸æŠ¥å‘Š...")
            self.save_outputs()

            logger.info("\n" + "=" * 80)
            logger.info("âœ… è®­ç»ƒå®Œæˆ!")
            logger.info("=" * 80)

        except Exception as e:
            logger.error(f"è®­ç»ƒå¤±è´¥: {e}", exc_info=True)
            raise

    def load_data(self):
        """åŠ è½½ç‰¹å¾ã€æ ‡ç­¾ã€æ ·æœ¬æƒé‡"""
        data_config = self.config['data']

        # åŠ è½½ç‰¹å¾
        features_path = Path(data_config['features_path'])
        if not features_path.exists():
            raise FileNotFoundError(f"ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {features_path}")

        logger.info(f"åŠ è½½ç‰¹å¾: {features_path}")
        features_df = pd.read_parquet(features_path)

        # åŠ è½½æ ‡ç­¾
        labels_path = Path(data_config['labels_path'])
        logger.info(f"åŠ è½½æ ‡ç­¾: {labels_path}")
        labels_df = pd.read_parquet(labels_path)

        # åŠ è½½æ ·æœ¬æƒé‡ (å¯é€‰)
        weights_path = Path(data_config.get('weights_path', ''))
        if weights_path.exists():
            logger.info(f"åŠ è½½æ ·æœ¬æƒé‡: {weights_path}")
            weights_df = pd.read_parquet(weights_path)
            self.sample_weights = weights_df['weight']
        else:
            logger.warning("æœªæ‰¾åˆ°æ ·æœ¬æƒé‡æ–‡ä»¶,å°†ä½¿ç”¨å‡ç­‰æƒé‡")
            self.sample_weights = None

        # åŠ è½½é¢„æµ‹æ—¶é—´ (ç”¨äº Purged K-Fold)
        pred_times_path = Path(data_config.get('pred_times_path', ''))
        if pred_times_path.exists():
            logger.info(f"åŠ è½½é¢„æµ‹æ—¶é—´: {pred_times_path}")
            pred_times_df = pd.read_parquet(pred_times_path)
            self.pred_times = pred_times_df['pred_time']
        else:
            logger.warning("æœªæ‰¾åˆ°é¢„æµ‹æ—¶é—´æ–‡ä»¶,Purged K-Fold å°†æ— æ³•å·¥ä½œ")
            self.pred_times = None

        # åˆå¹¶æ•°æ® (ç¡®ä¿ç´¢å¼•å¯¹é½)
        data = features_df.join(labels_df, how='inner')

        logger.info(f"æ•°æ®åŠ è½½å®Œæˆ: {len(data)} æ ·æœ¬, {len(features_df.columns)} ç‰¹å¾")

        # åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        train_end_date = pd.to_datetime(data_config.get('train_end_date', '2024-01-01'))

        if isinstance(data.index, pd.DatetimeIndex):
            train_mask = data.index < train_end_date
        else:
            logger.warning("æ•°æ®ç´¢å¼•ä¸æ˜¯ DatetimeIndex,ä½¿ç”¨ 80/20 åˆ†å‰²")
            split_idx = int(len(data) * 0.8)
            train_mask = np.zeros(len(data), dtype=bool)
            train_mask[:split_idx] = True

        # è®­ç»ƒé›†
        train_data = data[train_mask]
        self.X_train = train_data[features_df.columns]
        self.y_train = train_data['label']

        # æµ‹è¯•é›†
        test_data = data[~train_mask]
        self.X_test = test_data[features_df.columns]
        self.y_test = test_data['label']

        # æ ·æœ¬æƒé‡åˆ†å‰²
        if self.sample_weights is not None:
            self.sample_weights = self.sample_weights[train_mask]

        # é¢„æµ‹æ—¶é—´åˆ†å‰²
        if self.pred_times is not None:
            self.pred_times = self.pred_times[train_mask]

        logger.info(f"è®­ç»ƒé›†: {len(self.X_train)} æ ·æœ¬")
        logger.info(f"æµ‹è¯•é›†: {len(self.X_test)} æ ·æœ¬")
        logger.info(f"è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: {self.y_train.value_counts().to_dict()}")

    def preprocess_data(self):
        """æ•°æ®é¢„å¤„ç†"""
        data_config = self.config['data']

        # åˆ é™¤åŸå§‹ä»·æ ¼åˆ— (åªä¿ç•™å¹³ç¨³ç‰¹å¾)
        drop_features = data_config.get('drop_features', [])
        existing_drop_features = [f for f in drop_features if f in self.X_train.columns]

        if existing_drop_features:
            logger.info(f"åˆ é™¤åŸå§‹ä»·æ ¼åˆ—: {existing_drop_features}")
            self.X_train = self.X_train.drop(columns=existing_drop_features)
            self.X_test = self.X_test.drop(columns=existing_drop_features)

        # æ£€æŸ¥ç¼ºå¤±å€¼
        missing_train = self.X_train.isnull().sum().sum()
        if missing_train > 0:
            logger.warning(f"è®­ç»ƒé›†å­˜åœ¨ {missing_train} ä¸ªç¼ºå¤±å€¼,å°†å¡«å……ä¸º 0")
            self.X_train = self.X_train.fillna(0)
            self.X_test = self.X_test.fillna(0)

        # æ£€æŸ¥æ— é™å€¼
        inf_train = np.isinf(self.X_train).sum().sum()
        if inf_train > 0:
            logger.warning(f"è®­ç»ƒé›†å­˜åœ¨ {inf_train} ä¸ªæ— é™å€¼,å°†æ›¿æ¢ä¸º 0")
            self.X_train = self.X_train.replace([np.inf, -np.inf], 0)
            self.X_test = self.X_test.replace([np.inf, -np.inf], 0)

        logger.info("æ•°æ®é¢„å¤„ç†å®Œæˆ!")

    def feature_selection(self):
        """ç‰¹å¾é€‰æ‹©ä¸å»å™ª"""
        fs_config = self.config.get('feature_selection', {})

        if not fs_config.get('enable', False):
            logger.info("ç‰¹å¾é€‰æ‹©å·²ç¦ç”¨,ä½¿ç”¨å…¨éƒ¨ç‰¹å¾")
            self.feature_names = self.X_train.columns.tolist()
            return

        method = fs_config.get('method', 'none')

        if method == 'clustered_importance':
            logger.info("ä½¿ç”¨ç‰¹å¾èšç±»å»å™ª...")
            clustering_config = fs_config.get('clustering', {})

            clusterer = FeatureClusterer(
                correlation_threshold=clustering_config.get('correlation_threshold', 0.75)
            )
            clusterer.fit(self.X_train)

            # ç»˜åˆ¶æ ‘çŠ¶å›¾
            output_dir = Path(self.config['output']['plots_dir'])
            clusterer.plot_dendrogram(
                self.X_train.columns.tolist(),
                output_path=str(output_dir / 'feature_clustering_dendrogram.png')
            )

            logger.info("ç‰¹å¾èšç±»å®Œæˆ!")

        elif method == 'mda':
            logger.info("ä½¿ç”¨ MDA ç‰¹å¾é‡è¦æ€§ç­›é€‰...")
            # éœ€è¦å…ˆè®­ç»ƒä¸€ä¸ªåŸºç¡€æ¨¡å‹
            logger.warning("MDA æ–¹æ³•éœ€è¦é¢„è®­ç»ƒæ¨¡å‹,æš‚æ—¶è·³è¿‡")

        else:
            logger.info(f"æœªçŸ¥çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•: {method}")

        self.feature_names = self.X_train.columns.tolist()
        logger.info(f"æœ€ç»ˆç‰¹å¾æ•°: {len(self.feature_names)}")

    def setup_validation(self):
        """è®¾ç½®äº¤å‰éªŒè¯ç­–ç•¥"""
        val_config = self.config['validation']
        method = val_config.get('method', 'purged_kfold')

        if method == 'purged_kfold':
            pkf_config = val_config['purged_kfold']
            cv_splitter = PurgedKFold(
                n_splits=pkf_config.get('n_splits', 5),
                embargo_pct=pkf_config.get('embargo_pct', 0.01),
                purge_overlap=pkf_config.get('purge_overlap', True)
            )
            logger.info(f"ä½¿ç”¨ Purged K-Fold: {pkf_config['n_splits']} folds")

        elif method == 'walk_forward':
            wf_config = val_config['walk_forward']
            cv_splitter = WalkForwardValidator(
                train_period_days=wf_config.get('train_period_days', 730),
                test_period_days=wf_config.get('test_period_days', 90),
                step_days=wf_config.get('step_days', 90)
            )
            logger.info("ä½¿ç”¨ Walk Forward éªŒè¯")

        else:
            logger.warning(f"æœªçŸ¥çš„éªŒè¯æ–¹æ³•: {method},ä½¿ç”¨é»˜è®¤ 5-Fold")
            cv_splitter = None

        return cv_splitter

    def train_model(self, cv_splitter):
        """è®­ç»ƒæ¨¡å‹"""
        models_config = self.config['models']
        architecture = models_config.get('architecture', 'lightgbm')

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨ Optuna
        optuna_config = self.config.get('optuna', {})
        if optuna_config.get('enable', False):
            logger.info("ğŸ” å¯åŠ¨ Optuna è¶…å‚æ•°ä¼˜åŒ–...")
            self._run_optuna_optimization()
            # ä¼˜åŒ–åä½¿ç”¨æœ€ä½³å‚æ•°é‡æ–°è®­ç»ƒ
            # (è¿™é‡Œç®€åŒ–å¤„ç†,å®é™…åº”è¯¥ç”¨æœ€ä½³å‚æ•°)

        if architecture == 'lightgbm':
            logger.info("è®­ç»ƒ LightGBM æ¨¡å‹...")
            lgb_config = models_config['lightgbm']

            # ç®€å•åˆ†å‰²éªŒè¯é›†
            split_idx = int(len(self.X_train) * 0.8)
            X_train_split = self.X_train.iloc[:split_idx]
            y_train_split = self.y_train.iloc[:split_idx]
            X_val_split = self.X_train.iloc[split_idx:]
            y_val_split = self.y_train.iloc[split_idx:]

            weight_train = self.sample_weights.iloc[:split_idx] if self.sample_weights is not None else None
            weight_val = self.sample_weights.iloc[split_idx:] if self.sample_weights is not None else None

            self.model = LightGBMTrainer(
                params=lgb_config,
                early_stopping_rounds=lgb_config.get('early_stopping_rounds', 50)
            )
            self.model.train(
                X_train_split, y_train_split,
                X_val_split, y_val_split,
                weight_train, weight_val,
                num_boost_round=lgb_config.get('num_boost_round', 1000)
            )

        elif architecture == 'catboost':
            if not CATBOOST_AVAILABLE:
                logger.error("CatBoost æœªå®‰è£…,è¯·è¿è¡Œ: pip install catboost")
                raise ImportError("CatBoost æœªå®‰è£…")

            logger.info("è®­ç»ƒ CatBoost æ¨¡å‹...")
            cb_config = models_config['catboost']

            split_idx = int(len(self.X_train) * 0.8)
            X_train_split = self.X_train.iloc[:split_idx]
            y_train_split = self.y_train.iloc[:split_idx]
            X_val_split = self.X_train.iloc[split_idx:]
            y_val_split = self.y_train.iloc[split_idx:]

            weight_train = self.sample_weights.iloc[:split_idx] if self.sample_weights is not None else None
            weight_val = self.sample_weights.iloc[split_idx:] if self.sample_weights is not None else None

            self.model = CatBoostTrainer(
                params=cb_config,
                early_stopping_rounds=cb_config.get('early_stopping_rounds', 50)
            )
            self.model.train(
                X_train_split, y_train_split,
                X_val_split, y_val_split,
                weight_train, weight_val
            )

        elif architecture == 'ensemble_stacking':
            logger.info("è®­ç»ƒ Ensemble Stacking æ¨¡å‹...")
            ensemble_config = models_config.get('ensemble', {})

            # ç®€å•åˆ†å‰²éªŒè¯é›†
            split_idx = int(len(self.X_train) * 0.8)
            X_train_split = self.X_train.iloc[:split_idx]
            y_train_split = self.y_train.iloc[:split_idx]
            X_val_split = self.X_train.iloc[split_idx:]
            y_val_split = self.y_train.iloc[split_idx:]

            weight_train = self.sample_weights.iloc[:split_idx] if self.sample_weights is not None else None
            pred_times_split = self.pred_times.iloc[:split_idx] if self.pred_times is not None else None

            self.model = EnsembleStacker(
                use_catboost=ensemble_config.get('use_catboost', True),
                lgb_params=models_config.get('lightgbm'),
                cb_params=models_config.get('catboost')
            )
            self.model.train(
                X_train_split, y_train_split,
                X_val_split, y_val_split,
                weight_train,
                cv_splitter,
                pred_times_split
            )

        else:
            raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹æ¶æ„: {architecture}")

        logger.info("æ¨¡å‹è®­ç»ƒå®Œæˆ!")

    def _run_optuna_optimization(self):
        """è¿è¡Œ Optuna è¶…å‚æ•°ä¼˜åŒ–"""
        optuna_config = self.config['optuna']

        # ç®€å•åˆ†å‰²
        split_idx = int(len(self.X_train) * 0.8)
        X_train_split = self.X_train.iloc[:split_idx]
        y_train_split = self.y_train.iloc[:split_idx]
        X_val_split = self.X_train.iloc[split_idx:]
        y_val_split = self.y_train.iloc[split_idx:]

        weight_train = self.sample_weights.iloc[:split_idx] if self.sample_weights is not None else None
        weight_val = self.sample_weights.iloc[split_idx:] if self.sample_weights is not None else None

        optimizer = OptunaOptimizer(
            n_trials=optuna_config.get('n_trials', 50),
            timeout=optuna_config.get('timeout'),
            direction=optuna_config.get('direction', 'maximize')
        )

        best_params = optimizer.optimize(
            X_train_split, y_train_split,
            X_val_split, y_val_split,
            weight_train, weight_val,
            metric=optuna_config.get('metric', 'f1')
        )

        # ä¿å­˜æœ€ä½³å‚æ•°
        output_dir = Path(self.config['output']['model_dir'])
        with open(output_dir / 'best_params.json', 'w') as f:
            json.dump(best_params, f, indent=2)

        logger.info(f"æœ€ä½³å‚æ•°å·²ä¿å­˜è‡³: {output_dir / 'best_params.json'}")

    def evaluate_model(self):
        """è¯„ä¼°æ¨¡å‹"""
        logger.info("åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹...")

        evaluator = ModelEvaluator(
            output_dir=self.config['output']['plots_dir']
        )

        # é¢„æµ‹
        y_pred = self.model.predict(self.X_test)
        y_pred_proba = self.model.predict_proba(self.X_test)[:, 1]

        # è¯„ä¼°
        metrics = evaluator.evaluate(
            self.y_test,
            y_pred,
            y_pred_proba,
            prefix='test_set'
        )

        # ä¿å­˜æŒ‡æ ‡
        output_dir = Path(self.config['output']['model_dir'])
        with open(output_dir / 'test_metrics.json', 'w') as f:
            json.dump(metrics, f, indent=2)

        logger.info(f"æµ‹è¯•é›†æŒ‡æ ‡å·²ä¿å­˜è‡³: {output_dir / 'test_metrics.json'}")

        # ç‰¹å¾é‡è¦æ€§
        if hasattr(self.model, 'get_feature_importance'):
            importance = self.model.get_feature_importance()
            importance.to_csv(output_dir / 'feature_importance.csv')
            logger.info(f"ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜è‡³: {output_dir / 'feature_importance.csv'}")

    def save_outputs(self):
        """ä¿å­˜æ¨¡å‹ä¸æŠ¥å‘Š"""
        output_config = self.config['output']

        if output_config.get('save_best_model', True):
            model_dir = Path(output_config['model_dir'])
            model_path = model_dir / 'model.pkl'

            if hasattr(self.model, 'save'):
                self.model.save(str(model_path))
                logger.info(f"æ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")

        logger.info("æ‰€æœ‰è¾“å‡ºå·²ä¿å­˜!")


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='MT5-CRS æœºå™¨å­¦ä¹ è®­ç»ƒè„šæœ¬')

    parser.add_argument(
        '--config',
        type=str,
        default='config/ml_training_config.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„'
    )

    parser.add_argument(
        '--quick',
        action='store_true',
        help='å¿«é€Ÿæµ‹è¯•æ¨¡å¼ (å‡å°‘æ•°æ®é‡å’Œè¿­ä»£æ¬¡æ•°)'
    )

    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()

    # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    config_path = Path(args.config)
    if not config_path.exists():
        print(f"é”™è¯¯: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        print(f"è¯·å…ˆåˆ›å»ºé…ç½®æ–‡ä»¶ï¼Œå‚è€ƒ: config/ml_training_config.yaml")
        sys.exit(1)

    # å¿«é€Ÿæ¨¡å¼ (ä¿®æ”¹é…ç½®)
    if args.quick:
        print("âš¡ å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
        # TODO: ä¿®æ”¹é…ç½®å‡å°‘æ•°æ®é‡

    # åˆ›å»ºè®­ç»ƒç®¡é“
    pipeline = MLTrainingPipeline(str(config_path))

    # æ‰§è¡Œè®­ç»ƒ
    pipeline.run()


if __name__ == "__main__":
    main()
