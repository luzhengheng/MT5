"""
æ€§èƒ½åŸºå‡†æµ‹è¯• - Notion Bridge (Protocol v4.4 ä¼˜åŒ– 2)

è¦†ç›–èŒƒå›´:
  1. æ­£åˆ™è¡¨è¾¾å¼æ€§èƒ½åŸºå‡†
  2. å¼‚å¸¸å¤„ç†æ€§èƒ½åŸºå‡†
  3. ç³»ç»Ÿçº§æ€§èƒ½åŸºå‡†

è¿è¡Œ: pytest tests/test_notion_bridge_performance.py -v -m performance
"""

import pytest
import sys
import time
import tempfile
import re
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from scripts.ops.notion_bridge import (
    validate_regex_safety,
    extract_report_summary,
    sanitize_task_id,
    TASK_ID_STRICT_PATTERN,
    DANGEROUS_CHARS_PATTERN,
    SUMMARY_PATTERN,
)


# ============================================================================
# æ€§èƒ½åŸºå‡†é…ç½®
# ============================================================================

@pytest.mark.performance
class TestRegexPerformance:
    """æ­£åˆ™è¡¨è¾¾å¼æ€§èƒ½åŸºå‡†æµ‹è¯•"""

    def test_precompiled_vs_dynamic_task_id(self):
        """âœ… é¢„ç¼–è¯‘ vs åŠ¨æ€ç¼–è¯‘æ€§èƒ½å¯¹æ¯” (ç›®æ ‡: é¢„ç¼–è¯‘å¿« 50%+)"""
        test_string = '130.2'
        iterations = 10000

        # é¢„ç¼–è¯‘æ€§èƒ½
        start = time.perf_counter()
        for _ in range(iterations):
            TASK_ID_STRICT_PATTERN.match(test_string)
        precompiled_time = time.perf_counter() - start

        # åŠ¨æ€ç¼–è¯‘æ€§èƒ½
        start = time.perf_counter()
        for _ in range(iterations):
            pattern = re.compile(r'^[0-9]{1,3}(?:\.[0-9]{1,2})?$')
            pattern.match(test_string)
        dynamic_time = time.perf_counter() - start

        # é¢„ç¼–è¯‘åº”è¯¥å¿« 50%+
        speedup = dynamic_time / precompiled_time
        print(f"\nPrecompiled: {precompiled_time:.4f}s")
        print(f"Dynamic:     {dynamic_time:.4f}s")
        print(f"Speedup:     {speedup:.2f}x")

        assert precompiled_time < dynamic_time, \
            f"Precompiled ({precompiled_time}s) should be faster than dynamic ({dynamic_time}s)"
        assert speedup >= 1.5, f"Speedup only {speedup}x, expected >1.5x"

    def test_dangerous_chars_detection_performance(self):
        """âœ… å±é™©å­—ç¬¦æ£€æµ‹æ€§èƒ½"""
        safe_input = 'normal_task_id_130'
        unsafe_input = 'malicious$(whoami)'
        iterations = 5000

        # å®‰å…¨è¾“å…¥æ€§èƒ½
        start = time.perf_counter()
        for _ in range(iterations):
            DANGEROUS_CHARS_PATTERN.search(safe_input)
        safe_time = time.perf_counter() - start

        # ä¸å®‰å…¨è¾“å…¥æ€§èƒ½
        start = time.perf_counter()
        for _ in range(iterations):
            DANGEROUS_CHARS_PATTERN.search(unsafe_input)
        unsafe_time = time.perf_counter() - start

        # ä¸¤è€…éƒ½åº”è¯¥å¾ˆå¿« (<1ms æ€»æ—¶é—´)
        assert safe_time < 0.1, f"Safe detection too slow: {safe_time}s"
        assert unsafe_time < 0.1, f"Unsafe detection too slow: {unsafe_time}s"

        print(f"\nSafe input:   {safe_time:.4f}s ({iterations} iterations)")
        print(f"Unsafe input: {unsafe_time:.4f}s ({iterations} iterations)")

    def test_summary_pattern_performance(self):
        """âœ… æ‘˜è¦æ¨¡å¼åŒ¹é…æ€§èƒ½"""
        text = """# æŠ¥å‘Š

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

è¿™æ˜¯æ‘˜è¦å†…å®¹ã€‚

## å…¶ä»–éƒ¨åˆ†

å…¶ä»–å†…å®¹ã€‚
"""
        iterations = 1000

        start = time.perf_counter()
        for _ in range(iterations):
            SUMMARY_PATTERN.search(text)
        elapsed = time.perf_counter() - start

        # åº”è¯¥å¾ˆå¿« (<0.1s for 1000 iterations = <0.1ms per iteration)
        assert elapsed < 0.1, f"Pattern matching too slow: {elapsed}s for {iterations} iterations"

        print(f"\nSummary pattern: {elapsed:.4f}s ({iterations} iterations)")
        print(f"Per iteration:   {elapsed / iterations * 1000:.2f}ms")


# ============================================================================
# å¼‚å¸¸å¤„ç†æ€§èƒ½
# ============================================================================

@pytest.mark.performance
class TestExceptionPerformance:
    """å¼‚å¸¸å¤„ç†çš„æ€§èƒ½åŸºå‡†"""

    def test_exception_creation_performance(self):
        """âœ… å¼‚å¸¸åˆ›å»ºæ€§èƒ½"""
        from scripts.ops.notion_bridge import (
            PathTraversalError,
            FileTooLargeError,
            TaskMetadataError,
        )

        exception_classes = [
            PathTraversalError,
            FileTooLargeError,
            TaskMetadataError,
        ]
        iterations = 10000

        for exc_class in exception_classes:
            start = time.perf_counter()
            for _ in range(iterations):
                exc = exc_class("test message")
            elapsed = time.perf_counter() - start

            # å¼‚å¸¸åˆ›å»ºåº”è¯¥å¾ˆå¿« (<0.1ms æ¯ä¸ª)
            per_op = elapsed / iterations * 1000
            assert per_op < 0.1, \
                f"{exc_class.__name__} creation too slow: {per_op:.4f}ms"

            print(f"\n{exc_class.__name__}: {per_op:.4f}ms per creation")

    def test_exception_catching_performance(self):
        """âœ… å¼‚å¸¸æ•è·æ€§èƒ½"""
        from scripts.ops.notion_bridge import NotionBridgeException

        iterations = 5000

        start = time.perf_counter()
        for _ in range(iterations):
            try:
                raise NotionBridgeException("test")
            except NotionBridgeException:
                pass
        elapsed = time.perf_counter() - start

        per_op = elapsed / iterations * 1000
        # å¼‚å¸¸æ•è·åº”è¯¥ç›¸å¯¹å¿«é€Ÿ
        assert per_op < 1.0, f"Exception catching too slow: {per_op:.4f}ms"

        print(f"\nException catching: {per_op:.4f}ms per operation")


# ============================================================================
# ç³»ç»Ÿçº§æ€§èƒ½
# ============================================================================

@pytest.mark.performance
class TestSystemLevelPerformance:
    """ç³»ç»Ÿçº§æ€§èƒ½åŸºå‡†"""

    def test_sanitize_task_id_throughput(self):
        """âœ… ä»»åŠ¡ ID æ¸…æ´—ååé‡"""
        task_ids = [
            '130',
            'TASK_130',
            'TASK_130.2',
            '130.2',
        ]
        iterations = 1000

        start = time.perf_counter()
        processed = 0
        for _ in range(iterations):
            for task_id in task_ids:
                try:
                    sanitize_task_id(task_id)
                    processed += 1
                except Exception:
                    # æŸäº›æµ‹è¯•å¯èƒ½ä¼šæŠ›å‡ºå¼‚å¸¸ï¼Œè¿™æ˜¯æ­£å¸¸çš„
                    pass
        elapsed = time.perf_counter() - start

        throughput = processed / elapsed
        per_op = elapsed / processed * 1000

        assert throughput > 1000, f"Too slow: {throughput:.0f} ops/sec"
        assert per_op < 1.0, f"Per operation: {per_op:.4f}ms"

        print(f"\nTask ID sanitization:")
        print(f"  Throughput: {throughput:.0f} ops/sec")
        print(f"  Per op:     {per_op:.4f}ms")

    def test_extract_summary_performance(self):
        """âœ… æ‘˜è¦æå–æ€§èƒ½"""
        with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False) as f:
            content = """# æŠ¥å‘Š

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

è¿™æ˜¯æ‘˜è¦å†…å®¹ï¼ŒåŒ…å«äº†ä»»åŠ¡çš„å…³é”®ä¿¡æ¯ã€‚

## è¯¦ç»†ç»“æœ

æ›´å¤šè¯¦ç»†å†…å®¹...
"""
            f.write(content)
            temp_path = Path(f.name)

        try:
            iterations = 100

            start = time.perf_counter()
            for _ in range(iterations):
                extract_report_summary(temp_path)
            elapsed = time.perf_counter() - start

            per_op = elapsed / iterations * 1000

            assert per_op < 10.0, f"Too slow: {per_op:.2f}ms per extraction"

            print(f"\nReport summary extraction:")
            print(f"  Per op: {per_op:.2f}ms")
        finally:
            temp_path.unlink(missing_ok=True)

    def test_validate_regex_safety_performance(self):
        """âœ… ReDoS é˜²æŠ¤æ€§èƒ½å¼€é”€"""
        pattern = re.compile(r'^\d+$')
        test_input = '12345'
        iterations = 1000

        start = time.perf_counter()
        for _ in range(iterations):
            validate_regex_safety(pattern, test_input)
        elapsed = time.perf_counter() - start

        per_op = elapsed / iterations * 1000

        # è¶…æ—¶æ£€æµ‹åº”è¯¥æœ‰æœ€å°çš„å¼€é”€
        assert per_op < 1.0, f"ReDoS check too slow: {per_op:.4f}ms"

        print(f"\nRegex safety validation:")
        print(f"  Per op: {per_op:.4f}ms")


# ============================================================================
# å¹¶å‘æ€§èƒ½æµ‹è¯•
# ============================================================================

@pytest.mark.performance
class TestConcurrentPerformance:
    """å¹¶å‘åœºæ™¯æ€§èƒ½"""

    def test_concurrent_task_id_processing(self):
        """âœ… å¹¶å‘ä»»åŠ¡ ID å¤„ç†æ€§èƒ½"""
        import concurrent.futures

        task_ids = ['TASK_130', '130.2', 'TASK#130.1'] * 20  # 60 ä¸ªä»»åŠ¡

        def process_task_id(task_id):
            try:
                return sanitize_task_id(task_id)
            except Exception:
                return None

        # å•çº¿ç¨‹æ€§èƒ½
        start = time.perf_counter()
        results = [process_task_id(tid) for tid in task_ids]
        single_thread_time = time.perf_counter() - start

        # å¤šçº¿ç¨‹æ€§èƒ½
        start = time.perf_counter()
        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
            results = list(executor.map(process_task_id, task_ids))
        multi_thread_time = time.perf_counter() - start

        print(f"\nConcurrent task ID processing:")
        print(f"  Single thread: {single_thread_time:.4f}s")
        print(f"  Multi thread:  {multi_thread_time:.4f}s")

        # å¤šçº¿ç¨‹åº”è¯¥ç›¸å¯¹å¿«é€Ÿï¼ˆè™½ç„¶ GIL çš„å½±å“å¯èƒ½ä½¿å…¶ä¸æ˜æ˜¾å¿«ï¼‰
        assert len(results) == len(task_ids)


# ============================================================================
# æ€§èƒ½åŸºçº¿æµ‹è¯•
# ============================================================================

@pytest.mark.performance
class TestPerformanceBaselines:
    """æ€§èƒ½åŸºçº¿å»ºç«‹"""

    def test_establish_baseline_regex_operations(self):
        """å»ºç«‹æ­£åˆ™æ“ä½œçš„æ€§èƒ½åŸºçº¿"""
        baselines = {
            'é¢„ç¼–è¯‘æ¨¡å¼åŒ¹é…': None,
            'åŠ¨æ€æ¨¡å¼ç¼–è¯‘': None,
            'å±é™©å­—ç¬¦æ£€æµ‹': None,
        }

        # é¢„ç¼–è¯‘æ¨¡å¼åŒ¹é…
        start = time.perf_counter()
        for _ in range(10000):
            TASK_ID_STRICT_PATTERN.match('130.2')
        baselines['é¢„ç¼–è¯‘æ¨¡å¼åŒ¹é…'] = time.perf_counter() - start

        # åŠ¨æ€æ¨¡å¼ç¼–è¯‘
        start = time.perf_counter()
        for _ in range(10000):
            re.compile(r'^[0-9]{1,3}(?:\.[0-9]{1,2})?$').match('130.2')
        baselines['åŠ¨æ€æ¨¡å¼ç¼–è¯‘'] = time.perf_counter() - start

        # å±é™©å­—ç¬¦æ£€æµ‹
        start = time.perf_counter()
        for _ in range(10000):
            DANGEROUS_CHARS_PATTERN.search('normal_text')
        baselines['å±é™©å­—ç¬¦æ£€æµ‹'] = time.perf_counter() - start

        print("\næ€§èƒ½åŸºçº¿:")
        for name, time_taken in baselines.items():
            print(f"  {name}: {time_taken:.4f}s")

        # éªŒè¯æ‰€æœ‰æ“ä½œéƒ½åœ¨åˆç†æ—¶é—´å†…
        for time_taken in baselines.values():
            assert time_taken < 1.0, "Operation took too long"

    def test_establish_baseline_exception_operations(self):
        """å»ºç«‹å¼‚å¸¸æ“ä½œçš„æ€§èƒ½åŸºçº¿"""
        from scripts.ops.notion_bridge import TaskMetadataError

        baselines = {
            'å¼‚å¸¸åˆ›å»º': None,
            'å¼‚å¸¸æ•è·': None,
        }

        # å¼‚å¸¸åˆ›å»º
        start = time.perf_counter()
        for _ in range(5000):
            exc = TaskMetadataError("test")
        baselines['å¼‚å¸¸åˆ›å»º'] = time.perf_counter() - start

        # å¼‚å¸¸æ•è·
        start = time.perf_counter()
        for _ in range(1000):
            try:
                raise TaskMetadataError("test")
            except TaskMetadataError:
                pass
        baselines['å¼‚å¸¸æ•è·'] = time.perf_counter() - start

        print("\nå¼‚å¸¸æ“ä½œåŸºçº¿:")
        for name, time_taken in baselines.items():
            print(f"  {name}: {time_taken:.4f}s")


if __name__ == '__main__':
    pytest.main([__file__, '-v', '-m', 'performance'])
