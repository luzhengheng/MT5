# æ ¸å¿ƒä»£ç æ–‡ä»¶

## src/strategy/risk_manager.py

```python
"""
é£é™©ç®¡ç†æ¨¡å— - Kelly Criterion æ³¨ç ç­–ç•¥ä¸åŠ¨æ€é£æ§

æ ¸å¿ƒç»„ä»¶ï¼š
1. KellySizer: åŸºäº Kelly å…¬å¼çš„åŠ¨æ€ä»“ä½ç®¡ç†
2. DynamicRiskManager: è´¦æˆ·çº§é£é™©ç›‘æ§å’Œç†”æ–­æœºåˆ¶
"""

import backtrader as bt
import numpy as np
import logging
from typing import Optional
from src.strategy.session_risk_manager import SessionRiskManager, get_session_risk_manager

logger = logging.getLogger(__name__)


class KellySizer(bt.Sizer):
    """
    Kelly Criterion ä»“ä½ç®¡ç†å™¨ï¼ˆé€šç”¨å…¬å¼ç‰ˆæœ¬ï¼‰

    å…¬å¼ï¼š
        f* = [p(b+1) - 1] / b

    å…¶ä¸­ï¼š
        - f*: Kelly é£é™©æ¯”ä¾‹ï¼ˆOptimal Risk Fractionï¼‰
        - p: é¢„æµ‹èƒœç‡ï¼ˆä» ML æ¨¡å‹è¾“å‡ºçš„ y_pred_proba æˆ– HierarchicalSignalFusion ç½®ä¿¡åº¦ï¼‰
        - b: èµ”ç‡ï¼ˆç­–ç•¥çš„ take_profit_ratioï¼Œå³ç›ˆäºæ¯”ï¼‰

    ä»“ä½è®¡ç®—ï¼š
        Risk Amount = Account Value * f* * kelly_fraction
        Position Size = Risk Amount / (ATR * stop_loss_multiplier)

    é‡è¦åŒºåˆ«ï¼š
        - f* æ˜¯"é£é™©æ¯”ä¾‹"ï¼Œä¸æ˜¯"æŒä»“æ¯”ä¾‹"
        - å®é™…ä»“ä½å¤§å°å–å†³äºæ­¢æŸè·ç¦»ï¼ˆATR * multiplierï¼‰
        - èµ”ç‡ b=2.0 æ„å‘³ç€ç›ˆåˆ©æ˜¯äºæŸçš„ 2 å€

    å‚æ•°ï¼š
        kelly_fraction (float): Kelly æ¯”ä¾‹ (0-1)ï¼Œå»ºè®® 0.25 (å››åˆ†ä¹‹ä¸€ Kelly)
        max_position_pct (float): å•ç¬”æœ€å¤§ä»“ä½å æ¯” (é»˜è®¤ 50%)
        min_position_pct (float): å•ç¬”æœ€å°ä»“ä½å æ¯” (é»˜è®¤ 1%)
        stop_loss_multiplier (float): æ­¢æŸå€æ•° (é»˜è®¤ 2.0, å³ 2*ATR)
        use_hierarchical_signals (bool): ä¼˜å…ˆä½¿ç”¨ HierarchicalSignalFusion ç½®ä¿¡åº¦ (é»˜è®¤ True)

    P2-03 æ”¹è¿› (2025-12-21):
        - æ·»åŠ  _get_win_probability() æ–¹æ³•
        - ä¼˜å…ˆä» HierarchicalSignalFusion è·å–ç½®ä¿¡åº¦
        - å›é€€åˆ°æ•°æ®æºçš„ y_pred_proba
        - ç¡®ä¿ Kelly å…¬å¼è·å¾—é«˜è´¨é‡çš„èƒœç‡è¾“å…¥
    """

    params = (
        ('kelly_fraction', 0.25),  # ä¿å®ˆçš„å››åˆ†ä¹‹ä¸€ Kelly
        ('max_position_pct', 0.50),  # æœ€å¤§ 50% ä»“ä½ï¼ˆå› ä¸ºæ˜¯æŒä»“æ¯”ï¼Œè€Œéé£é™©æ¯”ï¼‰
        ('min_position_pct', 0.01),  # æœ€å° 1% ä»“ä½
        ('stop_loss_multiplier', 2.0),  # æ­¢æŸè·ç¦»ä¸º 2*ATR
        ('max_leverage', 3.0),  # æœ€å¤§æ æ†å€æ•° (Geminiå»ºè®®æ·»åŠ ç¡¬çº¦æŸ)
        ('max_risk_per_trade', 0.02),  # å•ç¬”æœ€å¤§é£é™©é‡‘é¢å æ¯” (é»˜è®¤ 2%, Geminiå»ºè®®)
        ('use_hierarchical_signals', True),  # P2-03: ä¼˜å…ˆä½¿ç”¨åˆ†å±‚ä¿¡å·ç½®ä¿¡åº¦
    )

    def _get_win_probability(self, data, isbuy: bool) -> Optional[float]:
        """
        è·å–äº¤æ˜“çš„èµ¢ç‡æ¦‚ç‡

        P2-03 æ”¹è¿›: æ”¯æŒå¤šä¸ªæ¦‚ç‡æ¥æº
        1. ä¼˜å…ˆä» HierarchicalSignalFusion è·å–ç½®ä¿¡åº¦ (highest quality)
        2. å…¶æ¬¡ä»æ•°æ®æºè·å– y_pred_proba (fallback)
        3. è¿”å› None å¦‚æœéƒ½æ— æ³•è·å–

        Args:
            data: Backtrader æ•°æ®æº
            isbuy: True ä¸ºä¹°å…¥ï¼ŒFalse ä¸ºå–å‡º

        Returns:
            float: èµ¢ç‡æ¦‚ç‡ (0-1)ï¼Œæˆ– None å¦‚æœæ— æ³•è·å–
        """
        p_win = None

        # P2-03: æ–¹å¼ 1 - ä» HierarchicalSignalFusion è·å–ç½®ä¿¡åº¦
        if self.params.use_hierarchical_signals:
            try:
                if hasattr(self.strategy, 'hierarchical_signals'):
                    fusion_engine = self.strategy.hierarchical_signals
                    last_result = fusion_engine.get_last_signal()

                    if last_result is not None:
                        # ä½¿ç”¨èåˆç»“æœçš„ç½®ä¿¡åº¦ä½œä¸ºèµ¢ç‡
                        p_win = last_result.confidence
                        logger.debug(
                            f"ä» HierarchicalSignalFusion è·å–èµ¢ç‡: {p_win:.4f} "
                            f"(ä¿¡å·: {last_result.final_signal})"
                        )
                        return p_win
            except (AttributeError, Exception) as e:
                logger.debug(f"æ— æ³•ä» HierarchicalSignalFusion è·å–èµ¢ç‡: {e}")

        # P2-03: æ–¹å¼ 2 - ä»æ•°æ®æºè·å– y_pred_proba (å›é€€æ–¹æ¡ˆ)
        try:
            if isbuy:
                p_win = data.y_pred_proba_long[0]
            else:
                p_win = data.y_pred_proba_short[0]

            # éªŒè¯æœ‰æ•ˆæ€§
            if np.isnan(p_win) or p_win <= 0:
                return None

            logger.debug(
                f"ä»æ•°æ®æºè·å–èµ¢ç‡ (å›é€€): {p_win:.4f} "
                f"({'y_pred_proba_long' if isbuy else 'y_pred_proba_short'})"
            )
            return p_win

        except (AttributeError, IndexError):
            logger.debug("æ— æ³•ä»æ•°æ®æºè·å– y_pred_proba")
            return None

    def _getsizing(self, comminfo, cash, data, isbuy):
        """
        è®¡ç®—ä»“ä½å¤§å°ï¼ˆä½¿ç”¨é€šç”¨ Kelly å…¬å¼ï¼‰

        Returns:
            int: äº¤æ˜“æ‰‹æ•°ï¼ˆæ­£æ•°ä¸ºä¹°å…¥ï¼Œè´Ÿæ•°ä¸ºå–å‡ºï¼‰
        """
        # ============================================================
        # Gemini Pro å®¡æŸ¥å»ºè®®ä¿®å¤:
        # ä½¿ç”¨ getcash() æˆ– Balance è€Œé getvalue() (Equity)
        # åŸå› : Equity ä¼šå› æŒä»“æµ®åŠ¨ç›ˆäºå‰§çƒˆè·³åŠ¨ï¼Œå¯¼è‡´ä»“ä½éœ‡è¡
        # ============================================================

        # ä¼˜å…ˆä½¿ç”¨ getcash() è·å–å¯ç”¨èµ„é‡‘
        # å¦‚æœç­–ç•¥ä¸­æœ‰æ˜ç¡®çš„ initial_capital è®¾ç½®ï¼Œä½¿ç”¨è¯¥å€¼
        if hasattr(self.broker, 'getcash'):
            account_value = self.broker.getcash() + self.broker.getvalue() - self.broker.getcash()
            # ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨å¯ç”¨ç°é‡‘ + å½“å‰æŒä»“æˆæœ¬
            # é¿å…ä½¿ç”¨åŠ¨æ€æƒç›Š (Equity = Balance + æµ®åŠ¨ç›ˆäº)
            try:
                # è·å–åˆå§‹èµ„é‡‘ä½œä¸ºåŸºå‡†
                if hasattr(self.strategy, 'initial_capital'):
                    account_value = self.strategy.initial_capital
                else:
                    # å›é€€åˆ°å¯ç”¨ç°é‡‘
                    account_value = self.broker.getcash()
            except (AttributeError, Exception):
                # æœ€ç»ˆå›é€€æ–¹æ¡ˆ
                account_value = self.broker.getvalue()
                logger.debug("ä½¿ç”¨ getvalue() ä½œä¸ºè´¦æˆ·ä»·å€¼ï¼ˆå¯èƒ½å—æµ®åŠ¨ç›ˆäºå½±å“ï¼‰")
        else:
            account_value = self.broker.getvalue()

        current_price = data.close[0]

        if current_price <= 0:
            return 0

        # P2-03: ä½¿ç”¨æ–°çš„ _get_win_probability() æ–¹æ³•è·å–èµ¢ç‡
        # ä¼˜å…ˆä» HierarchicalSignalFusion è·å–ç½®ä¿¡åº¦ï¼Œå†å›é€€åˆ°æ•°æ®æº
        p_win = self._get_win_probability(data, isbuy)

        if p_win is None or p_win <= 0:
            logger.debug("æ— æ³•è·å–æœ‰æ•ˆçš„èµ¢ç‡æ¦‚ç‡ï¼Œè·³è¿‡ä»“ä½è®¡ç®—")
            return 0

        # æ³¨æ„ï¼šè¿™é‡Œä¸å†è¦æ±‚ p_win > 0.5
        # é€šç”¨ Kelly å…¬å¼å¯ä»¥å¤„ç†ä½èƒœç‡é«˜èµ”ç‡çš„æƒ…å†µ

        # è·å–ç­–ç•¥çš„ç›ˆäºæ¯”ï¼ˆèµ”ç‡ bï¼‰
        try:
            if hasattr(self.strategy, 'params') and hasattr(self.strategy.params, 'take_profit_ratio'):
                b = self.strategy.params.take_profit_ratio
            else:
                # å¦‚æœç­–ç•¥æ²¡æœ‰å®šä¹‰ï¼Œä½¿ç”¨ä¿å®ˆé»˜è®¤å€¼
                b = 2.0
                logger.debug("ç­–ç•¥æœªå®šä¹‰ take_profit_ratioï¼Œä½¿ç”¨é»˜è®¤å€¼ 2.0")
        except (AttributeError, Exception) as e:
            logger.warning(f"è·å– take_profit_ratio å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼ 2.0")
            b = 2.0

        # é˜²æ­¢é™¤é›¶é”™è¯¯
        if b <= 0:
            logger.warning(f"æ— æ•ˆçš„èµ”ç‡ b={b}ï¼Œè·³è¿‡ä»“ä½è®¡ç®—")
            return 0

        # ============================================================
        # é€šç”¨ Kelly å…¬å¼ï¼šf* = [p(b+1) - 1] / b
        # ============================================================
        kelly_f = (p_win * (b + 1) - 1) / b

        # å¦‚æœ Kelly æ¯”ä¾‹ä¸ºè´Ÿæ•°ï¼Œè¯´æ˜è¯¥äº¤æ˜“æœŸæœ›å€¼ä¸ºè´Ÿï¼Œä¸åº”å¼€ä»“
        if kelly_f <= 0:
            logger.debug(f"Kelly f*={kelly_f:.4f} <= 0 (p={p_win:.3f}, b={b:.2f})ï¼Œè·³è¿‡äº¤æ˜“")
            return 0

        # åº”ç”¨ä¿å®ˆç³»æ•°ï¼ˆå››åˆ†ä¹‹ä¸€ Kellyï¼‰
        risk_pct = kelly_f * self.params.kelly_fraction

        # ============================================================
        # Gemini Pro å»ºè®®: æ·»åŠ ç¡¬æ€§çº¦æŸ
        # é˜²æ­¢æç«¯æƒ…å†µä¸‹çš„è¿‡é«˜æ æ†
        # ============================================================

        # 1. é£é™©é‡‘é¢ç¡¬çº¦æŸ (æ¯ç¬”æœ€å¤§äºæŸ)
        max_risk_amount = account_value * self.params.max_risk_per_trade
        risk_pct = min(risk_pct, self.params.max_risk_per_trade)

        logger.debug(
            f"åº”ç”¨é£é™©çº¦æŸ: risk_pct={risk_pct:.2%}, max_allowed={self.params.max_risk_per_trade:.2%}"
        )

        # è®¡ç®— ATRï¼ˆç”¨äºç¡®å®šæ­¢æŸè·ç¦»ï¼‰
        try:
            if hasattr(self.strategy, 'atr'):
                atr_value = self.strategy.atr[0]
            else:
                # ä½¿ç”¨ç®€åŒ–çš„ ATR ä¼°è®¡ï¼ˆHigh-Lowï¼‰
                atr_value = abs(data.high[0] - data.low[0])
        except (AttributeError, IndexError):
            # å¦‚æœæ— æ³•è·å–ï¼Œä½¿ç”¨ä»·æ ¼çš„ 1% ä½œä¸ºä¼°è®¡
            atr_value = current_price * 0.01
            logger.debug(f"æ— æ³•è·å– ATRï¼Œä½¿ç”¨ä¼°è®¡å€¼ {atr_value:.5f}")

        if atr_value <= 0:
            logger.warning(f"ATR å€¼æ— æ•ˆ ({atr_value})ï¼Œè·³è¿‡ä»“ä½è®¡ç®—")
            return 0

        # ============================================================
        # ä»“ä½è®¡ç®—ï¼šä»"é£é™©æ¯”ä¾‹"è½¬æ¢ä¸º"æŒä»“æ•°é‡"
        # ============================================================
        # 1. è®¡ç®—ç›®æ ‡é£é™©é‡‘é¢
        risk_amount = account_value * risk_pct

        # 2. è®¡ç®—å•è‚¡é£é™©ï¼ˆæ­¢æŸè·ç¦»ï¼‰
        risk_per_share = atr_value * self.params.stop_loss_multiplier

        if risk_per_share <= 0:
            logger.warning(f"å•è‚¡é£é™©æ— æ•ˆ ({risk_per_share})ï¼Œè·³è¿‡ä»“ä½è®¡ç®—")
            return 0

        # 3. è®¡ç®—ç›®æ ‡è‚¡æ•°
        target_shares = risk_amount / risk_per_share

        # 4. è®¡ç®—å®é™…æŒä»“ä»·å€¼å æ¯”ï¼ˆç”¨äºè¾¹ç•Œæ£€æŸ¥ï¼‰
        position_value = target_shares * current_price
        position_pct = position_value / account_value

        # ============================================================
        # Gemini Pro å»ºè®®: æ æ†ç¡¬çº¦æŸ
        # é˜²æ­¢æç«¯èƒœç‡å¯¼è‡´çš„è¿‡é«˜æ æ†
        # ============================================================
        # 2. æ æ†çº¦æŸ (position_value / account_value)
        leverage = position_value / account_value
        if leverage > self.params.max_leverage:
            logger.warning(
                f"æ æ† {leverage:.2f}x è¶…è¿‡æœ€å¤§é™åˆ¶ {self.params.max_leverage}xï¼Œç¼©å‡ä»“ä½"
            )
            target_shares = target_shares * (self.params.max_leverage / leverage)
            position_value = target_shares * current_price
            position_pct = position_value / account_value

        # 5. åº”ç”¨æŒä»“æ¯”ä¾‹é™åˆ¶
        if position_pct > self.params.max_position_pct:
            # è¶…è¿‡æœ€å¤§æŒä»“æ¯”ä¾‹ï¼ŒæŒ‰æ¯”ä¾‹ç¼©å‡
            target_shares = target_shares * (self.params.max_position_pct / position_pct)
            position_pct = self.params.max_position_pct
            logger.debug(f"æŒä»“æ¯”ä¾‹è¶…é™ï¼Œç¼©å‡è‡³ {self.params.max_position_pct:.1%}")

        if position_pct < self.params.min_position_pct:
            # ä½äºæœ€å°æŒä»“æ¯”ä¾‹ï¼Œä¸å¼€ä»“
            logger.debug(f"æŒä»“æ¯”ä¾‹ {position_pct:.3%} < æœ€å°å€¼ {self.params.min_position_pct:.1%}ï¼Œè·³è¿‡äº¤æ˜“")
            return 0

        # 6. å‘ä¸‹å–æ•´ï¼ˆç¡®ä¿ä¸è¶…å‡ºèµ„é‡‘é™åˆ¶ï¼‰
        size = int(target_shares)

        # 7. æœ€ç»ˆéªŒè¯
        if size <= 0:
            return 0

        # è®°å½•è¯¦ç»†æ—¥å¿—
        logger.debug(
            f"Kelly Sizer - "
            f"P={p_win:.3f}, b={b:.2f}, f*={kelly_f:.4f}, "
            f"Risk%={risk_pct:.2%}, Position%={position_pct:.2%}, "
            f"ATR={atr_value:.5f}, Size={size}"
        )

        return size


class FixedFractionalSizer(bt.Sizer):
    """
    å›ºå®šæ¯”ä¾‹ä»“ä½ç®¡ç†å™¨ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰

    æ¯æ¬¡ä½¿ç”¨å›ºå®šç™¾åˆ†æ¯”çš„è´¦æˆ·èµ„é‡‘å¼€ä»“

    å‚æ•°ï¼š
        percents (float): æ¯æ¬¡äº¤æ˜“ä½¿ç”¨çš„èµ„é‡‘æ¯”ä¾‹ (é»˜è®¤ 10%)
    """

    params = (
        ('percents', 10),
    )

    def _getsizing(self, comminfo, cash, data, isbuy):
        account_value = self.broker.getvalue()
        current_price = data.close[0]

        if current_price <= 0:
            return 0

        position_value = account_value * (self.params.percents / 100.0)
        size = int(position_value / current_price)

        return size


class DynamicRiskManager:
    """
    åŠ¨æ€é£é™©ç®¡ç†å™¨

    åŠŸèƒ½ï¼š
    1. ç›‘æ§è´¦æˆ·å›æ’¤ï¼Œè§¦å‘ç†”æ–­æœºåˆ¶
    2. è·Ÿè¸ªæœ€é«˜å‡€å€¼ï¼Œè®¡ç®—å®æ—¶å›æ’¤
    3. æä¾›é£é™©æŠ¥å‘Š

    å‚æ•°ï¼š
        max_drawdown_pct (float): æœ€å¤§å›æ’¤æ¯”ä¾‹ï¼Œè¶…è¿‡åˆ™ç†”æ–­ (é»˜è®¤ 10%)
        stop_trading_on_breach (bool): è§¦å‘ç†”æ–­æ—¶æ˜¯å¦åœæ­¢äº¤æ˜“ (é»˜è®¤ True)
    """

    def __init__(self, broker: bt.brokers.BackBroker,
                 max_drawdown_pct: float = 10.0,
                 stop_trading_on_breach: bool = True,
                 daily_loss_limit: float = -0.05):
        self.broker = broker
        self.max_drawdown_pct = max_drawdown_pct / 100.0
        self.stop_trading_on_breach = stop_trading_on_breach

        self.peak_value = broker.getvalue()
        self.is_halted = False
        self.breach_datetime = None

        # ä¼šè¯é£æ§ç®¡ç†å™¨ - ç›‘æ§æ¯æ—¥æŸå¤±é™åˆ¶
        self.session_risk = get_session_risk_manager(daily_loss_limit)

        logger.info(f"é£é™©ç®¡ç†å™¨åˆå§‹åŒ– - æœ€å¤§å›æ’¤: {max_drawdown_pct}%, æ¯æ—¥æŸå¤±é™åˆ¶: {daily_loss_limit*100:.0f}%")

    def update(self, current_datetime=None) -> dict:
        """
        æ›´æ–°é£é™©çŠ¶æ€

        Returns:
            dict: é£é™©æŠ¥å‘Š
                - current_value: å½“å‰è´¦æˆ·ä»·å€¼
                - peak_value: å†å²æœ€é«˜ä»·å€¼
                - drawdown: å½“å‰å›æ’¤æ¯”ä¾‹
                - is_halted: æ˜¯å¦å·²ç†”æ–­
        """
        current_value = self.broker.getvalue()

        # æ›´æ–°å³°å€¼
        if current_value > self.peak_value:
            self.peak_value = current_value
            # å¦‚æœæ¢å¤åˆ°æ–°é«˜ï¼Œè§£é™¤ç†”æ–­
            if self.is_halted:
                logger.info(f"è´¦æˆ·æ¢å¤è‡³æ–°é«˜ {current_value:.2f}ï¼Œè§£é™¤ç†”æ–­")
                self.is_halted = False
                self.breach_datetime = None

        # è®¡ç®—å›æ’¤
        drawdown = (self.peak_value - current_value) / self.peak_value

        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§å›æ’¤
        if drawdown > self.max_drawdown_pct and not self.is_halted:
            self.is_halted = True
            self.breach_datetime = current_datetime
            logger.warning(f"ğŸš¨ è§¦å‘ç†”æ–­ï¼å›æ’¤: {drawdown:.2%}, é˜ˆå€¼: {self.max_drawdown_pct:.2%}")

        return {
            'current_value': current_value,
            'peak_value': self.peak_value,
            'drawdown': drawdown,
            'is_halted': self.is_halted,
            'breach_datetime': self.breach_datetime
        }

    def can_trade(self) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦å¯ä»¥äº¤æ˜“

        Returns:
            bool: True è¡¨ç¤ºå¯ä»¥äº¤æ˜“ï¼ŒFalse è¡¨ç¤ºè¢«ç†”æ–­æˆ–æ¯æ—¥åœæŸ
        """
        # æ£€æŸ¥æœ€å¤§å›æ’¤é™åˆ¶
        if self.stop_trading_on_breach and self.is_halted:
            logger.warning("âš ï¸ è´¦æˆ·å›æ’¤ç†”æ–­ï¼Œç¦æ­¢äº¤æ˜“")
            return False

        # æ£€æŸ¥æ¯æ—¥æŸå¤±é™åˆ¶
        if not self.session_risk.can_trade():
            daily_stats = self.session_risk.get_daily_stats()
            if daily_stats:
                logger.warning(f"âš ï¸ æ¯æ—¥æŸå¤±é™åˆ¶è§¦å‘ï¼Œå½“æ—¥æŸå¤±: {daily_stats['daily_loss_pct']}")
            return False

        return True

    def get_summary(self) -> str:
        """
        è·å–é£é™©ç®¡ç†æ‘˜è¦

        Returns:
            str: æ ¼å¼åŒ–çš„é£é™©æŠ¥å‘Š
        """
        report = self.update()
        daily_stats = self.session_risk.get_daily_stats()

        summary = f"""
========== é£é™©ç®¡ç†æŠ¥å‘Š ==========
å½“å‰è´¦æˆ·ä»·å€¼: ${report['current_value']:,.2f}
å†å²æœ€é«˜ä»·å€¼: ${report['peak_value']:,.2f}
å½“å‰å›æ’¤: {report['drawdown']:.2%}
æœ€å¤§å›æ’¤é™åˆ¶: {self.max_drawdown_pct:.2%}
ç†”æ–­çŠ¶æ€: {'ğŸš¨ å·²è§¦å‘' if report['is_halted'] else 'âœ… æ­£å¸¸'}
"""
        if report['breach_datetime']:
            summary += f"ç†”æ–­æ—¶é—´: {report['breach_datetime']}\n"

        # æ·»åŠ æ¯æ—¥æŸå¤±ä¿¡æ¯
        if daily_stats:
            summary += f"""
========== æ¯æ—¥æŸå¤±æŠ¥å‘Š ==========
å½“æ—¥å·²å®ç° P&L: {daily_stats['daily_realized_pnl']}
å½“æ—¥æœªå®ç° P&L: {daily_stats['daily_unrealized_pnl']}
å½“æ—¥æ€» P&L: {daily_stats['daily_total_pnl']}
å½“æ—¥æŸå¤±ç™¾åˆ†æ¯”: {daily_stats['daily_loss_pct']}
"""

        summary += "=================================\n"

        return summary


class PositionSizer:
    """
    é€šç”¨ä»“ä½è®¡ç®—å·¥å…·ç±»

    æä¾›å¤šç§ä»“ä½è®¡ç®—æ–¹æ³•çš„é™æ€å·¥å…·
    """

    @staticmethod
    def kelly_criterion(p_win: float, p_lose: float, win_amount: float, lose_amount: float) -> float:
        """
        æ ‡å‡† Kelly Criterion å…¬å¼

        Args:
            p_win: èƒœç‡
            p_lose: è´¥ç‡
            win_amount: å¹³å‡ç›ˆåˆ©é‡‘é¢
            lose_amount: å¹³å‡äºæŸé‡‘é¢

        Returns:
            float: Kelly æ¯”ä¾‹ (0-1)
        """
        if lose_amount == 0:
            return 0.0

        kelly = (p_win * win_amount - p_lose * lose_amount) / lose_amount
        return max(0.0, min(kelly, 1.0))

    @staticmethod
    def optimal_f(trades: list, account_size: float) -> float:
        """
        Optimal F (æœ€ä¼˜ f å€¼)

        Args:
            trades: äº¤æ˜“ç»“æœåˆ—è¡¨ï¼ˆç›ˆäºé‡‘é¢ï¼‰
            account_size: è´¦æˆ·è§„æ¨¡

        Returns:
            float: æœ€ä¼˜ f å€¼ (0-1)
        """
        if not trades:
            return 0.0

        max_loss = abs(min(trades))
        if max_loss == 0:
            return 0.0

        # ä½¿ç”¨äºŒåˆ†æœç´¢æ‰¾åˆ°æœ€ä¼˜ f
        best_f = 0.0
        best_twr = 0.0  # Terminal Wealth Relative

        for f in np.linspace(0.01, 1.0, 100):
            twr = 1.0
            for trade in trades:
                hpr = 1.0 + f * (trade / max_loss)  # Holding Period Return
                if hpr <= 0:
                    twr = 0
                    break
                twr *= hpr

            if twr > best_twr:
                best_twr = twr
                best_f = f

        return best_f

    @staticmethod
    def volatility_adjusted_position(account_value: float,
                                     current_price: float,
                                     target_risk_pct: float,
                                     atr: float) -> int:
        """
        åŸºäºæ³¢åŠ¨ç‡è°ƒæ•´çš„ä»“ä½

        Args:
            account_value: è´¦æˆ·ä»·å€¼
            current_price: å½“å‰ä»·æ ¼
            target_risk_pct: ç›®æ ‡é£é™©æ¯”ä¾‹ (å¦‚ 2% = 0.02)
            atr: ATR å€¼

        Returns:
            int: å»ºè®®æ‰‹æ•°
        """
        if atr == 0 or current_price == 0:
            return 0

        # è®¡ç®—å•ä½é£é™©
        risk_per_unit = atr

        # è®¡ç®—ç›®æ ‡é£é™©é‡‘é¢
        target_risk = account_value * target_risk_pct

        # è®¡ç®—ä»“ä½
        position_size = int(target_risk / risk_per_unit)

        return max(0, position_size)

```

## src/feature_engineering/basic_features.py

```python
"""
åŸºç¡€æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾è®¡ç®—
ä½¿ç”¨ pandas å’Œ numpy å®ç°ï¼ˆé¿å… ta-lib ä¾èµ–ï¼‰
"""

import numpy as np
import pandas as pd
import logging

logger = logging.getLogger(__name__)


class BasicFeatures:
    """åŸºç¡€æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾è®¡ç®—å™¨"""

    @staticmethod
    def compute_ema(series: pd.Series, period: int) -> pd.Series:
        """è®¡ç®—æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰"""
        return series.ewm(span=period, adjust=False).mean()

    @staticmethod
    def compute_sma(series: pd.Series, period: int) -> pd.Series:
        """è®¡ç®—ç®€å•ç§»åŠ¨å¹³å‡ï¼ˆSMAï¼‰"""
        return series.rolling(window=period).mean()

    @staticmethod
    def compute_rsi(series: pd.Series, period: int = 14) -> pd.Series:
        """è®¡ç®—ç›¸å¯¹å¼ºå¼±æŒ‡æ ‡ï¼ˆRSIï¼‰"""
        delta = series.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()

        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi

    @staticmethod
    def compute_macd(series: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9) -> pd.DataFrame:
        """è®¡ç®— MACD æŒ‡æ ‡"""
        ema_fast = BasicFeatures.compute_ema(series, fast)
        ema_slow = BasicFeatures.compute_ema(series, slow)

        macd_line = ema_fast - ema_slow
        signal_line = BasicFeatures.compute_ema(macd_line, signal)
        histogram = macd_line - signal_line

        return pd.DataFrame({
            'macd': macd_line,
            'macd_signal': signal_line,
            'macd_hist': histogram
        })

    @staticmethod
    def compute_atr(high: pd.Series, low: pd.Series, close: pd.Series, period: int = 14) -> pd.Series:
        """è®¡ç®—å¹³å‡çœŸå®æ³¢å¹…ï¼ˆATRï¼‰"""
        # True Range
        tr1 = high - low
        tr2 = abs(high - close.shift())
        tr3 = abs(low - close.shift())

        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)

        # ATR æ˜¯ TR çš„ç§»åŠ¨å¹³å‡
        atr = tr.rolling(window=period).mean()
        return atr

    @staticmethod
    def compute_bollinger_bands(series: pd.Series, period: int = 20, std: int = 2) -> pd.DataFrame:
        """è®¡ç®—å¸ƒæ—å¸¦"""
        sma = BasicFeatures.compute_sma(series, period)
        rolling_std = series.rolling(window=period).std()

        upper = sma + (rolling_std * std)
        lower = sma - (rolling_std * std)

        return pd.DataFrame({
            'bbands_upper': upper,
            'bbands_middle': sma,
            'bbands_lower': lower,
            'bbands_width': (upper - lower) / sma
        })

    @staticmethod
    def compute_stochastic(high: pd.Series, low: pd.Series, close: pd.Series,
                          k_period: int = 14, d_period: int = 3) -> pd.DataFrame:
        """è®¡ç®—éšæœºéœ‡è¡æŒ‡æ ‡ï¼ˆStochasticï¼‰"""
        lowest_low = low.rolling(window=k_period).min()
        highest_high = high.rolling(window=k_period).max()

        k = 100 * (close - lowest_low) / (highest_high - lowest_low)
        d = k.rolling(window=d_period).mean()

        return pd.DataFrame({
            'stochastic_k': k,
            'stochastic_d': d
        })

    @staticmethod
    def compute_williams_r(high: pd.Series, low: pd.Series, close: pd.Series, period: int = 14) -> pd.Series:
        """è®¡ç®—å¨å»‰æŒ‡æ ‡ï¼ˆWilliams %Rï¼‰"""
        highest_high = high.rolling(window=period).max()
        lowest_low = low.rolling(window=period).min()

        williams_r = -100 * (highest_high - close) / (highest_high - lowest_low)
        return williams_r

    @staticmethod
    def compute_roc(series: pd.Series, period: int = 10) -> pd.Series:
        """è®¡ç®—å˜åŒ–ç‡ï¼ˆROC - Rate of Changeï¼‰"""
        roc = ((series - series.shift(period)) / series.shift(period)) * 100
        return roc

    @staticmethod
    def compute_obv(close: pd.Series, volume: pd.Series) -> pd.Series:
        """è®¡ç®—èƒ½é‡æ½®ï¼ˆOBV - On-Balance Volumeï¼‰"""
        obv = (np.sign(close.diff()) * volume).fillna(0).cumsum()
        return obv

    @staticmethod
    def compute_realized_volatility(returns: pd.Series, period: int = 20) -> pd.Series:
        """è®¡ç®—å·²å®ç°æ³¢åŠ¨ç‡"""
        return returns.rolling(window=period).std() * np.sqrt(252)  # å¹´åŒ–

    @staticmethod
    def compute_returns(close: pd.Series, periods: list = [1, 3, 5, 10, 20]) -> pd.DataFrame:
        """è®¡ç®—å¤šæœŸæ»åå›æŠ¥"""
        returns_df = pd.DataFrame()

        for period in periods:
            returns_df[f'return_{period}d'] = close.pct_change(period)

        return returns_df

    @staticmethod
    def compute_all_basic_features(df: pd.DataFrame) -> pd.DataFrame:
        """
        è®¡ç®—æ‰€æœ‰åŸºç¡€ç‰¹å¾ï¼ˆ30 ç»´ï¼‰

        Args:
            df: å¿…é¡»åŒ…å« ['open', 'high', 'low', 'close', 'volume'] åˆ—

        Returns:
            æ·»åŠ äº†åŸºç¡€ç‰¹å¾çš„ DataFrame
        """
        df = df.copy()

        logger.info("è®¡ç®—åŸºç¡€æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾...")

        # è¶‹åŠ¿ç±»ç‰¹å¾ (10 ç»´)
        df['ema_12'] = BasicFeatures.compute_ema(df['close'], 12)
        df['ema_26'] = BasicFeatures.compute_ema(df['close'], 26)
        df['ema_50'] = BasicFeatures.compute_ema(df['close'], 50)
        df['ema_200'] = BasicFeatures.compute_ema(df['close'], 200)
        df['sma_20'] = BasicFeatures.compute_sma(df['close'], 20)
        df['sma_60'] = BasicFeatures.compute_sma(df['close'], 60)

        df['price_vs_ema200'] = (df['close'] - df['ema_200']) / df['ema_200']
        df['golden_cross'] = (df['ema_50'] > df['ema_200']).astype(int)
        df['death_cross'] = (df['ema_50'] < df['ema_200']).astype(int)
        df['trend_strength'] = (df['ema_12'] - df['ema_200']) / df['ema_200']

        # åŠ¨é‡ç±»ç‰¹å¾ (8 ç»´)
        df['rsi_14'] = BasicFeatures.compute_rsi(df['close'], 14)
        macd_df = BasicFeatures.compute_macd(df['close'])
        df = pd.concat([df, macd_df], axis=1)
        df['roc_10'] = BasicFeatures.compute_roc(df['close'], 10)

        stoch_df = BasicFeatures.compute_stochastic(df['high'], df['low'], df['close'])
        df = pd.concat([df, stoch_df], axis=1)

        df['williams_r'] = BasicFeatures.compute_williams_r(df['high'], df['low'], df['close'], 14)

        # æ³¢åŠ¨ç±»ç‰¹å¾ (6 ç»´)
        df['atr_14'] = BasicFeatures.compute_atr(df['high'], df['low'], df['close'], 14)

        bbands_df = BasicFeatures.compute_bollinger_bands(df['close'])
        df = pd.concat([df, bbands_df], axis=1)

        df['return_1d'] = df['close'].pct_change()
        df['realized_volatility_20'] = BasicFeatures.compute_realized_volatility(df['return_1d'], 20)

        # æˆäº¤é‡ç±»ç‰¹å¾ (3 ç»´)
        df['volume_sma20'] = BasicFeatures.compute_sma(df['volume'], 20)
        df['volume_ratio'] = df['volume'] / df['volume_sma20']
        df['obv'] = BasicFeatures.compute_obv(df['close'], df['volume'])

        # æ»åå›æŠ¥ç±»ç‰¹å¾ (5 ç»´) - return_1d å·²è®¡ç®—
        returns_df = BasicFeatures.compute_returns(df['close'], periods=[3, 5, 10, 20])
        df = pd.concat([df, returns_df], axis=1)

        logger.info(f"åŸºç¡€ç‰¹å¾è®¡ç®—å®Œæˆï¼Œæ–°å¢ {len(df.columns) - 6} ä¸ªç‰¹å¾")

        return df


def main():
    """æµ‹è¯•åŸºç¡€ç‰¹å¾è®¡ç®—"""
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    dates = pd.date_range('2023-01-01', periods=300, freq='D')
    np.random.seed(42)

    test_df = pd.DataFrame({
        'date': dates,
        'open': 100 + np.random.randn(300).cumsum(),
        'high': 101 + np.random.randn(300).cumsum(),
        'low': 99 + np.random.randn(300).cumsum(),
        'close': 100 + np.random.randn(300).cumsum(),
        'volume': np.random.randint(1000000, 10000000, 300)
    })

    print("åŸå§‹æ•°æ®:")
    print(test_df.head())
    print(f"\nåŸå§‹åˆ—æ•°: {len(test_df.columns)}")

    # è®¡ç®—åŸºç¡€ç‰¹å¾
    result_df = BasicFeatures.compute_all_basic_features(test_df)

    print(f"\næ·»åŠ ç‰¹å¾ååˆ—æ•°: {len(result_df.columns)}")
    print("\næ–°å¢ç‰¹å¾:")
    new_features = [col for col in result_df.columns if col not in test_df.columns]
    for i, feat in enumerate(new_features, 1):
        print(f"{i:2d}. {feat}")

    print(f"\nç‰¹å¾æ•°æ®é¢„è§ˆ:")
    print(result_df[new_features].tail(5))

    # æ£€æŸ¥ç¼ºå¤±å€¼
    print(f"\nç¼ºå¤±å€¼ç»Ÿè®¡:")
    missing_counts = result_df[new_features].isnull().sum()
    print(missing_counts[missing_counts > 0])

    print(f"\nç‰¹å¾å®Œæ•´ç‡: {(1 - result_df[new_features].isnull().sum().sum() / (len(result_df) * len(new_features))):.2%}")


if __name__ == '__main__':
    main()

```

## src/feature_engineering/advanced_features.py

```python
"""
é«˜çº§ç‰¹å¾å·¥ç¨‹æ¨¡å— - å®ç° 40 ç»´é«˜çº§ç‰¹å¾
åŒ…æ‹¬:
1. Fractional Differentiation (6 ç»´)
2. Rolling Statistics (12 ç»´)
3. Cross-Sectional Rank (6 ç»´)
4. Sentiment Momentum (8 ç»´)
5. Adaptive Window Features (3 ç»´)
6. Cross-Asset Features (5 ç»´)
"""

import logging
import numpy as np
import pandas as pd
from typing import Dict, List, Optional

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AdvancedFeatures:
    """é«˜çº§ç‰¹å¾è®¡ç®—ç±»"""

    @staticmethod
    def fractional_diff(series: pd.Series, d: float = 0.5, threshold: float = 1e-5) -> pd.Series:
        """
        åˆ†æ•°é˜¶å·®åˆ† (Fractional Differentiation)
        ä¿ç•™è®°å¿†æ€§çš„åŒæ—¶å®ç°å¹³ç¨³åŒ–

        æ¥æº: "Advances in Financial Machine Learning" by Marcos Lopez de Prado

        Args:
            series: æ—¶é—´åºåˆ—
            d: å·®åˆ†é˜¶æ•° (0 < d < 1)ï¼Œd=1 ä¸ºå®Œå…¨å·®åˆ†ï¼Œd=0.5 ä¸ºåŠå·®åˆ†
            threshold: æƒé‡æˆªæ–­é˜ˆå€¼

        Returns:
            åˆ†æ•°é˜¶å·®åˆ†åçš„åºåˆ—
        """
        # è®¡ç®—æƒé‡
        weights = [1.0]
        k = 1

        # è¿­ä»£è®¡ç®—æƒé‡ç›´åˆ°å°äºé˜ˆå€¼
        while True:
            weight = -weights[-1] * (d - k + 1) / k
            if abs(weight) < threshold:
                break
            weights.append(weight)
            k += 1

        weights = np.array(weights[::-1])  # åè½¬æƒé‡

        # åº”ç”¨å·ç§¯
        result = pd.Series(index=series.index, dtype=float)
        for i in range(len(weights) - 1, len(series)):
            result.iloc[i] = np.dot(weights, series.iloc[i - len(weights) + 1:i + 1])

        return result

    @staticmethod
    def compute_fractional_features(df: pd.DataFrame) -> pd.DataFrame:
        """
        è®¡ç®— Fractional Differentiation ç‰¹å¾ (6 ç»´)

        ç‰¹å¾åˆ—è¡¨:
        1. frac_diff_close_05: æ”¶ç›˜ä»· d=0.5 åˆ†æ•°å·®åˆ†
        2. frac_diff_close_07: æ”¶ç›˜ä»· d=0.7 åˆ†æ•°å·®åˆ†
        3. frac_diff_volume_05: æˆäº¤é‡ d=0.5 åˆ†æ•°å·®åˆ†
        4. frac_diff_returns_05: æ”¶ç›Šç‡ d=0.5 åˆ†æ•°å·®åˆ†
        5. frac_diff_volatility_05: æ³¢åŠ¨ç‡ d=0.5 åˆ†æ•°å·®åˆ†
        6. frac_diff_sentiment_05: æƒ…æ„Ÿ d=0.5 åˆ†æ•°å·®åˆ†

        Args:
            df: åŒ…å« OHLCV å’Œæƒ…æ„Ÿæ•°æ®çš„ DataFrame

        Returns:
            æ·»åŠ äº† Fractional Differentiation ç‰¹å¾çš„ DataFrame
        """
        logger.info("è®¡ç®— Fractional Differentiation ç‰¹å¾...")

        # 1. æ”¶ç›˜ä»· d=0.5
        df['frac_diff_close_05'] = AdvancedFeatures.fractional_diff(
            df['close'], d=0.5
        )

        # 2. æ”¶ç›˜ä»· d=0.7 (æ›´å¼ºçš„å·®åˆ†)
        df['frac_diff_close_07'] = AdvancedFeatures.fractional_diff(
            df['close'], d=0.7
        )

        # 3. æˆäº¤é‡ d=0.5
        df['frac_diff_volume_05'] = AdvancedFeatures.fractional_diff(
            df['volume'], d=0.5
        )

        # 4. æ”¶ç›Šç‡ d=0.5
        returns = df['close'].pct_change()
        df['frac_diff_returns_05'] = AdvancedFeatures.fractional_diff(
            returns, d=0.5
        )

        # 5. æ³¢åŠ¨ç‡ d=0.5 (ä½¿ç”¨ 20 æ—¥æ»šåŠ¨æ ‡å‡†å·®)
        volatility = df['close'].pct_change().rolling(window=20).std()
        df['frac_diff_volatility_05'] = AdvancedFeatures.fractional_diff(
            volatility, d=0.5
        )

        # 6. æƒ…æ„Ÿ d=0.5
        if 'sentiment_mean' in df.columns:
            df['frac_diff_sentiment_05'] = AdvancedFeatures.fractional_diff(
                df['sentiment_mean'], d=0.5
            )
        else:
            df['frac_diff_sentiment_05'] = 0.0

        logger.info("Fractional Differentiation ç‰¹å¾è®¡ç®—å®Œæˆ (6 ç»´)")
        return df

    @staticmethod
    def compute_rolling_statistics(df: pd.DataFrame) -> pd.DataFrame:
        """
        è®¡ç®—æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾ (12 ç»´)

        ç‰¹å¾åˆ—è¡¨:
        1. roll_skew_20: 20 æ—¥æ”¶ç›Šç‡ååº¦
        2. roll_kurt_20: 20 æ—¥æ”¶ç›Šç‡å³°åº¦
        3. roll_skew_60: 60 æ—¥æ”¶ç›Šç‡ååº¦
        4. roll_kurt_60: 60 æ—¥æ”¶ç›Šç‡å³°åº¦
        5. roll_autocorr_1: æ”¶ç›Šç‡è‡ªç›¸å…³(lag=1)
        6. roll_autocorr_5: æ”¶ç›Šç‡è‡ªç›¸å…³(lag=5)
        7. roll_max_drawdown_20: 20 æ—¥æœ€å¤§å›æ’¤
        8. roll_max_drawdown_60: 60 æ—¥æœ€å¤§å›æ’¤
        9. roll_sharpe_20: 20 æ—¥ Sharpe æ¯”ç‡
        10. roll_sortino_20: 20 æ—¥ Sortino æ¯”ç‡
        11. roll_calmar_60: 60 æ—¥ Calmar æ¯”ç‡
        12. roll_tail_ratio_20: 20 æ—¥å°¾éƒ¨æ¯”ç‡ (95th/5th percentile)

        Args:
            df: DataFrame

        Returns:
            æ·»åŠ äº†æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾çš„ DataFrame
        """
        logger.info("è®¡ç®— Rolling Statistics ç‰¹å¾...")

        returns = df['close'].pct_change()

        # 1-2. 20 æ—¥ååº¦å’Œå³°åº¦
        df['roll_skew_20'] = returns.rolling(window=20).skew()
        df['roll_kurt_20'] = returns.rolling(window=20).kurt()

        # 3-4. 60 æ—¥ååº¦å’Œå³°åº¦
        df['roll_skew_60'] = returns.rolling(window=60).skew()
        df['roll_kurt_60'] = returns.rolling(window=60).kurt()

        # 5-6. è‡ªç›¸å…³
        df['roll_autocorr_1'] = returns.rolling(window=20).apply(
            lambda x: x.autocorr(lag=1) if len(x) > 1 else np.nan
        )
        df['roll_autocorr_5'] = returns.rolling(window=20).apply(
            lambda x: x.autocorr(lag=5) if len(x) > 5 else np.nan
        )

        # 7-8. æœ€å¤§å›æ’¤
        def max_drawdown(prices):
            cumulative = (1 + prices).cumprod()
            running_max = cumulative.expanding().max()
            drawdown = (cumulative - running_max) / running_max
            return drawdown.min()

        df['roll_max_drawdown_20'] = returns.rolling(window=20).apply(max_drawdown)
        df['roll_max_drawdown_60'] = returns.rolling(window=60).apply(max_drawdown)

        # 9. Sharpe æ¯”ç‡ (å‡è®¾æ— é£é™©åˆ©ç‡=0)
        df['roll_sharpe_20'] = (
            returns.rolling(window=20).mean() / returns.rolling(window=20).std()
        ) * np.sqrt(252)  # å¹´åŒ–

        # 10. Sortino æ¯”ç‡ (åªè€ƒè™‘ä¸‹è¡Œæ³¢åŠ¨)
        def sortino_ratio(rets):
            downside = rets[rets < 0]
            if len(downside) > 0:
                downside_std = downside.std()
                if downside_std > 0:
                    return (rets.mean() / downside_std) * np.sqrt(252)
            return np.nan

        df['roll_sortino_20'] = returns.rolling(window=20).apply(sortino_ratio)

        # 11. Calmar æ¯”ç‡ (æ”¶ç›Šç‡ / æœ€å¤§å›æ’¤)
        df['roll_calmar_60'] = (
            returns.rolling(window=60).mean() * 252 /  # å¹´åŒ–æ”¶ç›Š
            df['roll_max_drawdown_60'].abs()
        )

        # 12. å°¾éƒ¨æ¯”ç‡
        def tail_ratio(rets):
            if len(rets) > 0:
                p95 = np.percentile(rets, 95)
                p05 = np.percentile(rets, 5)
                if p05 != 0:
                    return abs(p95 / p05)
            return np.nan

        df['roll_tail_ratio_20'] = returns.rolling(window=20).apply(tail_ratio)

        logger.info("Rolling Statistics ç‰¹å¾è®¡ç®—å®Œæˆ (12 ç»´)")
        return df

    @staticmethod
    def compute_cross_sectional_rank(df: pd.DataFrame, all_dfs: Dict[str, pd.DataFrame] = None) -> pd.DataFrame:
        """
        è®¡ç®—æ¨ªæˆªé¢æ’åç‰¹å¾ (6 ç»´)
        ç›¸å¯¹äºæ‰€æœ‰èµ„äº§çš„æ’å

        ç‰¹å¾åˆ—è¡¨:
        1. cs_rank_return_1d: 1 æ—¥æ”¶ç›Šç‡æ¨ªæˆªé¢æ’å
        2. cs_rank_return_5d: 5 æ—¥æ”¶ç›Šç‡æ¨ªæˆªé¢æ’å
        3. cs_rank_volatility: æ³¢åŠ¨ç‡æ¨ªæˆªé¢æ’å
        4. cs_rank_volume: æˆäº¤é‡æ¨ªæˆªé¢æ’å
        5. cs_rank_rsi: RSI æ¨ªæˆªé¢æ’å
        6. cs_rank_sentiment: æƒ…æ„Ÿæ¨ªæˆªé¢æ’å

        Args:
            df: å½“å‰èµ„äº§çš„ DataFrame
            all_dfs: æ‰€æœ‰èµ„äº§çš„ DataFrame å­—å…¸ {symbol: df}

        Returns:
            æ·»åŠ äº†æ¨ªæˆªé¢æ’åç‰¹å¾çš„ DataFrame
        """
        logger.info("è®¡ç®— Cross-Sectional Rank ç‰¹å¾...")

        if all_dfs is None or len(all_dfs) < 2:
            # å¦‚æœæ²¡æœ‰å…¶ä»–èµ„äº§æ•°æ®,è®¾ç½®ä¸ºä¸­ä½æ•° 0.5
            logger.warning("æ²¡æœ‰å…¶ä»–èµ„äº§æ•°æ®,æ¨ªæˆªé¢æ’åç‰¹å¾è®¾ä¸º 0.5")
            df['cs_rank_return_1d'] = 0.5
            df['cs_rank_return_5d'] = 0.5
            df['cs_rank_volatility'] = 0.5
            df['cs_rank_volume'] = 0.5
            df['cs_rank_rsi'] = 0.5
            df['cs_rank_sentiment'] = 0.5
            return df

        # ä¸ºæ¯ä¸ªæ—¥æœŸè®¡ç®—æ¨ªæˆªé¢æ’å
        dates = df['date'].unique()

        for date in dates:
            # æ”¶é›†æ‰€æœ‰èµ„äº§åœ¨è¯¥æ—¥æœŸçš„æ•°æ®
            cross_section = []
            for symbol, other_df in all_dfs.items():
                date_data = other_df[other_df['date'] == date]
                if not date_data.empty:
                    cross_section.append({
                        'symbol': symbol,
                        'return_1d': date_data['return_1d'].iloc[0] if 'return_1d' in date_data.columns else np.nan,
                        'return_5d': date_data['return_5d'].iloc[0] if 'return_5d' in date_data.columns else np.nan,
                        'volatility': date_data['volatility_20d'].iloc[0] if 'volatility_20d' in date_data.columns else np.nan,
                        'volume': date_data['volume'].iloc[0],
                        'rsi': date_data['rsi_14'].iloc[0] if 'rsi_14' in date_data.columns else np.nan,
                        'sentiment': date_data['sentiment_mean'].iloc[0] if 'sentiment_mean' in date_data.columns else 0,
                    })

            if len(cross_section) > 1:
                cs_df = pd.DataFrame(cross_section)

                # è®¡ç®—æ’å (ç™¾åˆ†ä½æ•°)
                current_symbol = df[df['date'] == date]['symbol'].iloc[0] if 'symbol' in df.columns else None

                if current_symbol:
                    for col, feature in [
                        ('return_1d', 'cs_rank_return_1d'),
                        ('return_5d', 'cs_rank_return_5d'),
                        ('volatility', 'cs_rank_volatility'),
                        ('volume', 'cs_rank_volume'),
                        ('rsi', 'cs_rank_rsi'),
                        ('sentiment', 'cs_rank_sentiment'),
                    ]:
                        if col in cs_df.columns:
                            cs_df[feature] = cs_df[col].rank(pct=True)
                            rank_value = cs_df[cs_df['symbol'] == current_symbol][feature].iloc[0] if not cs_df[cs_df['symbol'] == current_symbol].empty else 0.5
                            df.loc[df['date'] == date, feature] = rank_value

        logger.info("Cross-Sectional Rank ç‰¹å¾è®¡ç®—å®Œæˆ (6 ç»´)")
        return df

    @staticmethod
    def compute_sentiment_momentum(df: pd.DataFrame) -> pd.DataFrame:
        """
        è®¡ç®—æƒ…æ„ŸåŠ¨é‡ç‰¹å¾ (8 ç»´)

        ç‰¹å¾åˆ—è¡¨:
        1. sentiment_momentum_5d: 5 æ—¥æƒ…æ„ŸåŠ¨é‡
        2. sentiment_momentum_20d: 20 æ—¥æƒ…æ„ŸåŠ¨é‡
        3. sentiment_acceleration: æƒ…æ„ŸåŠ é€Ÿåº¦ (åŠ¨é‡çš„å˜åŒ–)
        4. sentiment_divergence: æƒ…æ„Ÿ-ä»·æ ¼èƒŒç¦» (æƒ…æ„Ÿä¸Šæ¶¨ä½†ä»·æ ¼ä¸‹è·Œ)
        5. sentiment_consistency_5d: 5 æ—¥æƒ…æ„Ÿä¸€è‡´æ€§ (è¿ç»­æ­£/è´Ÿçš„æ¯”ä¾‹)
        6. sentiment_intensity: æƒ…æ„Ÿå¼ºåº¦ (ç»å¯¹å€¼çš„å‡å€¼)
        7. sentiment_volatility_20d: 20 æ—¥æƒ…æ„Ÿæ³¢åŠ¨ç‡
        8. news_frequency_ma20: 20 æ—¥æ–°é—»é¢‘ç‡ç§»åŠ¨å¹³å‡

        Args:
            df: DataFrame

        Returns:
            æ·»åŠ äº†æƒ…æ„ŸåŠ¨é‡ç‰¹å¾çš„ DataFrame
        """
        logger.info("è®¡ç®— Sentiment Momentum ç‰¹å¾...")

        if 'sentiment_mean' not in df.columns:
            logger.warning("æ²¡æœ‰æƒ…æ„Ÿæ•°æ®,æƒ…æ„ŸåŠ¨é‡ç‰¹å¾è®¾ä¸º 0")
            for feat in ['sentiment_momentum_5d', 'sentiment_momentum_20d',
                        'sentiment_acceleration', 'sentiment_divergence',
                        'sentiment_consistency_5d', 'sentiment_intensity',
                        'sentiment_volatility_20d', 'news_frequency_ma20']:
                df[feat] = 0.0
            return df

        # 1-2. æƒ…æ„ŸåŠ¨é‡
        df['sentiment_momentum_5d'] = df['sentiment_mean'] - df['sentiment_mean'].shift(5)
        df['sentiment_momentum_20d'] = df['sentiment_mean'] - df['sentiment_mean'].shift(20)

        # 3. æƒ…æ„ŸåŠ é€Ÿåº¦
        df['sentiment_acceleration'] = df['sentiment_momentum_5d'] - df['sentiment_momentum_5d'].shift(5)

        # 4. æƒ…æ„Ÿ-ä»·æ ¼èƒŒç¦»
        price_momentum = df['close'].pct_change(5)
        sentiment_momentum = df['sentiment_momentum_5d']
        df['sentiment_divergence'] = (
            ((sentiment_momentum > 0) & (price_momentum < 0)).astype(int) -
            ((sentiment_momentum < 0) & (price_momentum > 0)).astype(int)
        )

        # 5. æƒ…æ„Ÿä¸€è‡´æ€§ (5 æ—¥çª—å£å†…åŒå‘æ¯”ä¾‹)
        def consistency(series):
            if len(series) == 0:
                return 0
            positive = (series > 0).sum()
            negative = (series < 0).sum()
            return max(positive, negative) / len(series)

        df['sentiment_consistency_5d'] = df['sentiment_mean'].rolling(window=5).apply(consistency)

        # 6. æƒ…æ„Ÿå¼ºåº¦
        df['sentiment_intensity'] = df['sentiment_mean'].abs().rolling(window=20).mean()

        # 7. æƒ…æ„Ÿæ³¢åŠ¨ç‡
        df['sentiment_volatility_20d'] = df['sentiment_mean'].rolling(window=20).std()

        # 8. æ–°é—»é¢‘ç‡ç§»åŠ¨å¹³å‡
        if 'news_count' in df.columns:
            df['news_frequency_ma20'] = df['news_count'].rolling(window=20).mean()
        else:
            df['news_frequency_ma20'] = 0.0

        logger.info("Sentiment Momentum ç‰¹å¾è®¡ç®—å®Œæˆ (8 ç»´)")
        return df

    @staticmethod
    def compute_adaptive_window_features(df: pd.DataFrame) -> pd.DataFrame:
        """
        è®¡ç®—è‡ªé€‚åº”çª—å£ç‰¹å¾ (3 ç»´)
        æ ¹æ®å¸‚åœºæ³¢åŠ¨ç‡åŠ¨æ€è°ƒæ•´çª—å£å¤§å°

        ç‰¹å¾åˆ—è¡¨:
        1. adaptive_ma: è‡ªé€‚åº”ç§»åŠ¨å¹³å‡ (åŸºäºæ³¢åŠ¨ç‡è°ƒæ•´çª—å£)
        2. adaptive_momentum: è‡ªé€‚åº”åŠ¨é‡
        3. adaptive_volatility_ratio: è‡ªé€‚åº”æ³¢åŠ¨ç‡æ¯”ç‡

        Args:
            df: DataFrame

        Returns:
            æ·»åŠ äº†è‡ªé€‚åº”çª—å£ç‰¹å¾çš„ DataFrame
        """
        logger.info("è®¡ç®— Adaptive Window Features...")

        # è®¡ç®—åŸºå‡†æ³¢åŠ¨ç‡
        returns = df['close'].pct_change()
        baseline_vol = returns.rolling(window=60).std()
        current_vol = returns.rolling(window=20).std()

        # æ³¢åŠ¨ç‡æ¯”ç‡ (å½“å‰/åŸºå‡†)
        vol_ratio = current_vol / baseline_vol
        vol_ratio = vol_ratio.fillna(1.0).clip(0.5, 2.0)  # é™åˆ¶åœ¨ 0.5-2 å€

        # 1. è‡ªé€‚åº”ç§»åŠ¨å¹³å‡ (é«˜æ³¢åŠ¨æ—¶ç”¨çŸ­çª—å£,ä½æ³¢åŠ¨æ—¶ç”¨é•¿çª—å£)
        # çª—å£: 10 åˆ° 50 å¤©
        adaptive_window = (50 - 40 * (vol_ratio - 0.5) / 1.5).clip(10, 50).astype(int)

        df['adaptive_ma'] = np.nan
        for i in range(len(df)):
            if i >= 50:  # ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®
                window = adaptive_window.iloc[i]
                df.loc[df.index[i], 'adaptive_ma'] = df['close'].iloc[i-window:i].mean()

        # 2. è‡ªé€‚åº”åŠ¨é‡
        df['adaptive_momentum'] = df['close'] / df['adaptive_ma'] - 1

        # 3. è‡ªé€‚åº”æ³¢åŠ¨ç‡æ¯”ç‡
        df['adaptive_volatility_ratio'] = vol_ratio

        logger.info("Adaptive Window Features è®¡ç®—å®Œæˆ (3 ç»´)")
        return df

    @staticmethod
    def compute_cross_asset_features(df: pd.DataFrame, reference_df: pd.DataFrame = None,
                                    reference_symbol: str = 'GSPC.INDX') -> pd.DataFrame:
        """
        è®¡ç®—è·¨èµ„äº§ç‰¹å¾ (5 ç»´)
        ç›¸å¯¹äºåŸºå‡†èµ„äº§(å¦‚ S&P 500)çš„ç‰¹å¾

        ç‰¹å¾åˆ—è¡¨:
        1. beta_to_market: ç›¸å¯¹å¸‚åœºçš„ Beta (60æ—¥)
        2. correlation_to_market: ä¸å¸‚åœºçš„ç›¸å…³æ€§ (60æ—¥)
        3. relative_strength: ç›¸å¯¹å¼ºåº¦ (å½“å‰èµ„äº§æ”¶ç›Š / å¸‚åœºæ”¶ç›Š)
        4. alpha_to_market: ç›¸å¯¹å¸‚åœºçš„ Alpha
        5. tracking_error: è·Ÿè¸ªè¯¯å·®

        Args:
            df: å½“å‰èµ„äº§çš„ DataFrame
            reference_df: åŸºå‡†èµ„äº§çš„ DataFrame
            reference_symbol: åŸºå‡†èµ„äº§åç§°

        Returns:
            æ·»åŠ äº†è·¨èµ„äº§ç‰¹å¾çš„ DataFrame
        """
        logger.info("è®¡ç®— Cross-Asset Features...")

        if reference_df is None or reference_df.empty:
            logger.warning("æ²¡æœ‰åŸºå‡†èµ„äº§æ•°æ®,è·¨èµ„äº§ç‰¹å¾è®¾ä¸º 0")
            df['beta_to_market'] = 0.0
            df['correlation_to_market'] = 0.0
            df['relative_strength'] = 0.0
            df['alpha_to_market'] = 0.0
            df['tracking_error'] = 0.0
            return df

        # ç¡®ä¿æ—¥æœŸå¯¹é½
        df['date'] = pd.to_datetime(df['date'])
        reference_df['date'] = pd.to_datetime(reference_df['date'])

        # åˆå¹¶æ•°æ®
        merged = pd.merge(
            df[['date', 'close']],
            reference_df[['date', 'close']],
            on='date',
            how='left',
            suffixes=('', '_market')
        )

        # è®¡ç®—æ”¶ç›Šç‡
        merged['return'] = merged['close'].pct_change()
        merged['return_market'] = merged['close_market'].pct_change()

        # 1. Beta (60æ—¥æ»šåŠ¨)
        def rolling_beta(returns, market_returns):
            """è®¡ç®— Beta"""
            # å»é™¤ NaN
            mask = ~(np.isnan(returns) | np.isnan(market_returns))
            if mask.sum() < 10:
                return np.nan

            returns_clean = returns[mask]
            market_returns_clean = market_returns[mask]

            # è®¡ç®—åæ–¹å·®å’Œæ–¹å·®
            if len(returns_clean) < 10:
                return np.nan

            covariance = np.cov(returns_clean, market_returns_clean)[0, 1]
            market_variance = np.var(market_returns_clean)

            if market_variance > 0:
                return covariance / market_variance
            return np.nan

        # è®¡ç®—æ»šåŠ¨ Beta
        beta_values = []
        for i in range(len(merged)):
            if i < 60:
                beta_values.append(np.nan)
            else:
                window_returns = merged['return'].iloc[i-60:i].values
                window_market = merged['return_market'].iloc[i-60:i].values
                beta = rolling_beta(window_returns, window_market)
                beta_values.append(beta)

        merged['beta_to_market'] = beta_values

        # 2. ç›¸å…³æ€§ (60æ—¥)
        merged['correlation_to_market'] = merged['return'].rolling(window=60).corr(merged['return_market'])

        # 3. ç›¸å¯¹å¼ºåº¦ (20æ—¥ç´¯è®¡æ”¶ç›Šæ¯”)
        merged['relative_strength'] = (
            (1 + merged['return']).rolling(window=20).apply(lambda x: x.prod()) /
            (1 + merged['return_market']).rolling(window=20).apply(lambda x: x.prod())
        )

        # 4. Alpha (æ”¶ç›Šç‡ - Beta * å¸‚åœºæ”¶ç›Šç‡)
        merged['alpha_to_market'] = merged['return'] - merged['beta_to_market'] * merged['return_market']

        # 5. è·Ÿè¸ªè¯¯å·® (60æ—¥)
        merged['tracking_error'] = (merged['return'] - merged['return_market']).rolling(window=60).std() * np.sqrt(252)

        # åˆå¹¶å›åŸ DataFrame
        df['beta_to_market'] = merged['beta_to_market'].values
        df['correlation_to_market'] = merged['correlation_to_market'].values
        df['relative_strength'] = merged['relative_strength'].values
        df['alpha_to_market'] = merged['alpha_to_market'].values
        df['tracking_error'] = merged['tracking_error'].values

        logger.info("Cross-Asset Features è®¡ç®—å®Œæˆ (5 ç»´)")
        return df

    @staticmethod
    def compute_all_advanced_features(df: pd.DataFrame, all_dfs: Dict[str, pd.DataFrame] = None,
                                     reference_df: pd.DataFrame = None) -> pd.DataFrame:
        """
        è®¡ç®—æ‰€æœ‰é«˜çº§ç‰¹å¾ (40 ç»´)

        Args:
            df: å½“å‰èµ„äº§çš„ DataFrame
            all_dfs: æ‰€æœ‰èµ„äº§çš„ DataFrame å­—å…¸ (ç”¨äºæ¨ªæˆªé¢ç‰¹å¾)
            reference_df: åŸºå‡†èµ„äº§çš„ DataFrame (ç”¨äºè·¨èµ„äº§ç‰¹å¾)

        Returns:
            æ·»åŠ äº†æ‰€æœ‰é«˜çº§ç‰¹å¾çš„ DataFrame
        """
        logger.info("å¼€å§‹è®¡ç®—æ‰€æœ‰é«˜çº§ç‰¹å¾ (40 ç»´)...")

        # 1. Fractional Differentiation (6 ç»´)
        df = AdvancedFeatures.compute_fractional_features(df)

        # 2. Rolling Statistics (12 ç»´)
        df = AdvancedFeatures.compute_rolling_statistics(df)

        # 3. Cross-Sectional Rank (6 ç»´)
        df = AdvancedFeatures.compute_cross_sectional_rank(df, all_dfs)

        # 4. Sentiment Momentum (8 ç»´)
        df = AdvancedFeatures.compute_sentiment_momentum(df)

        # 5. Adaptive Window Features (3 ç»´)
        df = AdvancedFeatures.compute_adaptive_window_features(df)

        # 6. Cross-Asset Features (5 ç»´)
        df = AdvancedFeatures.compute_cross_asset_features(df, reference_df)

        logger.info("æ‰€æœ‰é«˜çº§ç‰¹å¾è®¡ç®—å®Œæˆ (40 ç»´) âœ“")
        return df


# æµ‹è¯•ä»£ç 
if __name__ == '__main__':
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    np.random.seed(42)
    dates = pd.date_range('2023-01-01', '2024-01-01', freq='D')

    test_df = pd.DataFrame({
        'date': dates,
        'symbol': 'TEST',
        'open': 100 + np.cumsum(np.random.randn(len(dates))),
        'high': 102 + np.cumsum(np.random.randn(len(dates))),
        'low': 98 + np.cumsum(np.random.randn(len(dates))),
        'close': 100 + np.cumsum(np.random.randn(len(dates))),
        'volume': np.random.randint(1000000, 10000000, len(dates)),
        'sentiment_mean': np.random.randn(len(dates)) * 0.5,
        'news_count': np.random.randint(0, 10, len(dates)),
    })

    # æ·»åŠ åŸºç¡€ç‰¹å¾
    test_df['return_1d'] = test_df['close'].pct_change()
    test_df['return_5d'] = test_df['close'].pct_change(5)
    test_df['volatility_20d'] = test_df['return_1d'].rolling(20).std()
    test_df['rsi_14'] = 50.0

    print("æµ‹è¯•é«˜çº§ç‰¹å¾è®¡ç®—...")
    print("=" * 60)

    # æµ‹è¯• Fractional Differentiation
    test_df = AdvancedFeatures.compute_fractional_features(test_df)
    print("\nFractional Differentiation ç‰¹å¾:")
    print(test_df[['date', 'frac_diff_close_05', 'frac_diff_close_07']].tail())

    # æµ‹è¯• Rolling Statistics
    test_df = AdvancedFeatures.compute_rolling_statistics(test_df)
    print("\nRolling Statistics ç‰¹å¾:")
    print(test_df[['date', 'roll_skew_20', 'roll_sharpe_20']].tail())

    # æµ‹è¯• Sentiment Momentum
    test_df = AdvancedFeatures.compute_sentiment_momentum(test_df)
    print("\nSentiment Momentum ç‰¹å¾:")
    print(test_df[['date', 'sentiment_momentum_5d', 'sentiment_divergence']].tail())

    # æµ‹è¯• Adaptive Window Features
    test_df = AdvancedFeatures.compute_adaptive_window_features(test_df)
    print("\nAdaptive Window Features:")
    print(test_df[['date', 'adaptive_ma', 'adaptive_momentum']].tail())

    print("\n" + "=" * 60)
    print("é«˜çº§ç‰¹å¾æµ‹è¯•å®Œæˆ!")
    print(f"æ€»ç‰¹å¾æ•°: {len(test_df.columns)} åˆ—")

```

## src/feature_engineering/labeling.py

```python
"""
æ ‡ç­¾ç”Ÿæˆæ¨¡å— - Triple Barrier Labeling
ç”¨äºç›‘ç£å­¦ä¹ çš„æ ‡ç­¾ç”Ÿæˆ

æ¥æº: "Advances in Financial Machine Learning" by Marcos Lopez de Prado
"""

import logging
import numpy as np
import pandas as pd
from typing import Optional, Tuple

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TripleBarrierLabeling:
    """
    ä¸‰é‡å£å’æ ‡ç­¾æ³• (Triple Barrier Method)

    ä¸ºæ¯ä¸ªæ ·æœ¬è®¾ç½®ä¸‰ä¸ªé€€å‡ºæ¡ä»¶:
    1. ä¸Šç•Œ (Upper Barrier): ä»·æ ¼ä¸Šæ¶¨è¾¾åˆ°ç›®æ ‡æ”¶ç›Š -> æ ‡ç­¾ = 1 (åšå¤š)
    2. ä¸‹ç•Œ (Lower Barrier): ä»·æ ¼ä¸‹è·Œè¾¾åˆ°æ­¢æŸ -> æ ‡ç­¾ = -1 (åšç©º/æ­¢æŸ)
    3. æ—¶é—´ç•Œ (Vertical Barrier): æŒæœ‰æœŸåˆ°æœŸ -> æ ‡ç­¾ = 0 æˆ–åŸºäºæ”¶ç›Šæ–¹å‘

    ä¼˜åŠ¿:
    - é¿å…å›ºå®šæŒæœ‰æœŸçš„åå·®
    - è€ƒè™‘æ­¢æŸå’Œæ­¢ç›ˆ
    - æ›´ç¬¦åˆå®é™…äº¤æ˜“é€»è¾‘
    """

    @staticmethod
    def apply_triple_barrier(
        prices: pd.Series,
        upper_barrier: float = 0.02,
        lower_barrier: float = -0.02,
        max_holding_period: int = 5,
        stop_loss: Optional[float] = None
    ) -> pd.DataFrame:
        """
        åº”ç”¨ä¸‰é‡å£å’æ ‡ç­¾æ³•

        Args:
            prices: ä»·æ ¼åºåˆ— (Series with datetime index)
            upper_barrier: ä¸Šç•Œæ”¶ç›Šç‡é˜ˆå€¼ (é»˜è®¤ 2%)
            lower_barrier: ä¸‹ç•Œæ”¶ç›Šç‡é˜ˆå€¼ (é»˜è®¤ -2%)
            max_holding_period: æœ€å¤§æŒæœ‰æœŸ (å¤©æ•°)
            stop_loss: æ­¢æŸé˜ˆå€¼ (å¯é€‰,å¦‚ -1% æ­¢æŸ)

        Returns:
            DataFrame åŒ…å«:
                - label: æ ‡ç­¾ (1=ä¸Šæ¶¨, -1=ä¸‹è·Œ, 0=ä¸­æ€§)
                - barrier_touched: è§¦å‘çš„å£å’ç±»å‹ ('upper', 'lower', 'vertical')
                - holding_period: å®é™…æŒæœ‰æœŸ
                - return: å®é™…æ”¶ç›Šç‡
        """
        logger.info("åº”ç”¨ Triple Barrier Labeling...")

        results = []

        for i in range(len(prices) - max_holding_period):
            entry_price = prices.iloc[i]
            entry_date = prices.index[i]

            # æœªæ¥ä»·æ ¼åºåˆ—
            future_prices = prices.iloc[i+1:i+1+max_holding_period]

            if len(future_prices) == 0:
                continue

            # è®¡ç®—æ”¶ç›Šç‡
            returns = (future_prices - entry_price) / entry_price

            # æ£€æŸ¥å£å’è§¦å‘
            label = 0
            barrier_touched = 'vertical'
            holding_period = max_holding_period
            actual_return = returns.iloc[-1] if len(returns) > 0 else 0

            # 1. æ£€æŸ¥ä¸Šç•Œ
            upper_touch = returns >= upper_barrier
            if upper_touch.any():
                upper_idx = upper_touch.idxmax()
                upper_day = returns.index.get_loc(upper_idx)

                # 2. æ£€æŸ¥ä¸‹ç•Œ
                lower_touch = returns <= lower_barrier
                if lower_touch.any():
                    lower_idx = lower_touch.idxmax()
                    lower_day = returns.index.get_loc(lower_idx)

                    # å“ªä¸ªå…ˆè§¦å‘
                    if upper_day <= lower_day:
                        label = 1
                        barrier_touched = 'upper'
                        holding_period = upper_day + 1
                        actual_return = returns.iloc[upper_day]
                    else:
                        label = -1
                        barrier_touched = 'lower'
                        holding_period = lower_day + 1
                        actual_return = returns.iloc[lower_day]
                else:
                    # åªè§¦å‘ä¸Šç•Œ
                    label = 1
                    barrier_touched = 'upper'
                    holding_period = upper_day + 1
                    actual_return = returns.iloc[upper_day]

            else:
                # 3. æ£€æŸ¥ä¸‹ç•Œ
                lower_touch = returns <= lower_barrier
                if lower_touch.any():
                    lower_idx = lower_touch.idxmax()
                    lower_day = returns.index.get_loc(lower_idx)
                    label = -1
                    barrier_touched = 'lower'
                    holding_period = lower_day + 1
                    actual_return = returns.iloc[lower_day]
                else:
                    # åˆ°è¾¾æ—¶é—´ç•Œ
                    # åŸºäºæœ€ç»ˆæ”¶ç›Šæ–¹å‘åˆ¤æ–­æ ‡ç­¾
                    if actual_return > 0:
                        label = 1
                    elif actual_return < 0:
                        label = -1
                    else:
                        label = 0

            # 4. æ£€æŸ¥æ­¢æŸ (å¯é€‰)
            if stop_loss is not None and stop_loss < 0:
                stop_touch = returns <= stop_loss
                if stop_touch.any():
                    stop_idx = stop_touch.idxmax()
                    stop_day = returns.index.get_loc(stop_idx)

                    # æ­¢æŸä¼˜å…ˆ
                    if stop_day < holding_period:
                        label = -1
                        barrier_touched = 'stop_loss'
                        holding_period = stop_day + 1
                        actual_return = returns.iloc[stop_day]

            results.append({
                'date': entry_date,
                'label': label,
                'barrier_touched': barrier_touched,
                'holding_period': holding_period,
                'return': actual_return,
                'entry_price': entry_price,
            })

        result_df = pd.DataFrame(results)
        result_df.set_index('date', inplace=True)

        # ç»Ÿè®¡
        label_counts = result_df['label'].value_counts()
        logger.info(f"æ ‡ç­¾åˆ†å¸ƒ: {label_counts.to_dict()}")
        logger.info(f"å¹³å‡æŒæœ‰æœŸ: {result_df['holding_period'].mean():.2f} å¤©")
        logger.info(f"è§¦å‘ç»Ÿè®¡: {result_df['barrier_touched'].value_counts().to_dict()}")

        return result_df

    @staticmethod
    def compute_meta_labels(
        predictions: pd.Series,
        actual_returns: pd.Series,
        threshold: float = 0.0
    ) -> pd.Series:
        """
        è®¡ç®—å…ƒæ ‡ç­¾ (Meta-Labeling)

        å…ƒæ ‡ç­¾ç”¨äºäºŒçº§æ¨¡å‹,åˆ¤æ–­ä¸»æ¨¡å‹çš„é¢„æµ‹æ˜¯å¦åº”è¯¥è¢«ä¿¡ä»»

        Args:
            predictions: ä¸»æ¨¡å‹çš„é¢„æµ‹ (1=åšå¤š, -1=åšç©º)
            actual_returns: å®é™…æ”¶ç›Šç‡
            threshold: æ”¶ç›Šç‡é˜ˆå€¼ (é»˜è®¤ 0,å³ç›ˆåˆ©=1,äºæŸ=0)

        Returns:
            å…ƒæ ‡ç­¾åºåˆ— (1=åº”è¯¥äº¤æ˜“, 0=ä¸åº”è¯¥äº¤æ˜“)
        """
        logger.info("è®¡ç®— Meta-Labels...")

        meta_labels = pd.Series(0, index=predictions.index)

        # åšå¤šä¿¡å· -> æ£€æŸ¥æ˜¯å¦ç›ˆåˆ©
        long_signals = predictions == 1
        meta_labels[long_signals & (actual_returns > threshold)] = 1

        # åšç©ºä¿¡å· -> æ£€æŸ¥æ˜¯å¦ç›ˆåˆ©
        short_signals = predictions == -1
        meta_labels[short_signals & (actual_returns < -threshold)] = 1

        positive_rate = meta_labels.mean()
        logger.info(f"å…ƒæ ‡ç­¾æ­£æ ·æœ¬æ¯”ä¾‹: {positive_rate:.2%}")

        return meta_labels

    @staticmethod
    def compute_sample_weights(
        labels: pd.Series,
        returns: pd.Series,
        decay: float = 0.95
    ) -> pd.Series:
        """
        è®¡ç®—æ ·æœ¬æƒé‡ (Sample Weights)

        è€ƒè™‘å› ç´ :
        1. æ—¶é—´è¡°å‡: è¿‘æœŸæ ·æœ¬æƒé‡æ›´é«˜
        2. æ”¶ç›Šå¹…åº¦: æ”¶ç›Šè¶Šå¤§æƒé‡è¶Šé«˜
        3. ç±»åˆ«å¹³è¡¡: å°‘æ•°ç±»æ ·æœ¬æƒé‡æ›´é«˜

        Args:
            labels: æ ‡ç­¾åºåˆ—
            returns: æ”¶ç›Šç‡åºåˆ—
            decay: æ—¶é—´è¡°å‡ç³»æ•° (é»˜è®¤ 0.95)

        Returns:
            æ ·æœ¬æƒé‡åºåˆ—
        """
        logger.info("è®¡ç®—æ ·æœ¬æƒé‡...")

        weights = pd.Series(1.0, index=labels.index)

        # 1. æ—¶é—´è¡°å‡æƒé‡
        n = len(labels)
        time_weights = np.array([decay ** (n - i - 1) for i in range(n)])
        weights *= time_weights

        # 2. æ”¶ç›Šå¹…åº¦æƒé‡ (ç»å¯¹æ”¶ç›Šè¶Šå¤§æƒé‡è¶Šé«˜)
        return_weights = 1 + np.abs(returns)
        weights *= return_weights

        # 3. ç±»åˆ«å¹³è¡¡æƒé‡
        label_counts = labels.value_counts()
        max_count = label_counts.max()

        for label, count in label_counts.items():
            class_weight = max_count / count
            weights[labels == label] *= class_weight

        # å½’ä¸€åŒ–
        weights = weights / weights.sum() * len(weights)

        logger.info(f"æƒé‡ç»Ÿè®¡: mean={weights.mean():.4f}, std={weights.std():.4f}, "
                   f"min={weights.min():.4f}, max={weights.max():.4f}")

        return weights

    @staticmethod
    def add_labels_to_dataframe(
        df: pd.DataFrame,
        price_column: str = 'close',
        upper_barrier: float = 0.02,
        lower_barrier: float = -0.02,
        max_holding_period: int = 5,
        stop_loss: Optional[float] = None
    ) -> pd.DataFrame:
        """
        å°† Triple Barrier æ ‡ç­¾æ·»åŠ åˆ° DataFrame

        Args:
            df: åŒ…å«ä»·æ ¼æ•°æ®çš„ DataFrame
            price_column: ä»·æ ¼åˆ—å
            upper_barrier: ä¸Šç•Œé˜ˆå€¼
            lower_barrier: ä¸‹ç•Œé˜ˆå€¼
            max_holding_period: æœ€å¤§æŒæœ‰æœŸ
            stop_loss: æ­¢æŸé˜ˆå€¼

        Returns:
            æ·»åŠ äº†æ ‡ç­¾åˆ—çš„ DataFrame
        """
        logger.info(f"ä¸º DataFrame æ·»åŠ  Triple Barrier æ ‡ç­¾...")

        # åº”ç”¨ä¸‰é‡å£å’
        prices = df.set_index('date')[price_column] if 'date' in df.columns else df[price_column]
        labels_df = TripleBarrierLabeling.apply_triple_barrier(
            prices,
            upper_barrier=upper_barrier,
            lower_barrier=lower_barrier,
            max_holding_period=max_holding_period,
            stop_loss=stop_loss
        )

        # åˆå¹¶æ ‡ç­¾
        df = df.set_index('date') if 'date' in df.columns else df
        df = df.join(labels_df[['label', 'barrier_touched', 'holding_period', 'return']], how='left')

        # å¡«å……æœªæ ‡æ³¨çš„è¡Œ
        df['label'] = df['label'].fillna(0).astype(int)
        df['barrier_touched'] = df['barrier_touched'].fillna('none')
        df['holding_period'] = df['holding_period'].fillna(0).astype(int)
        df['return'] = df['return'].fillna(0)

        # è®¡ç®—æ ·æœ¬æƒé‡
        df['sample_weight'] = TripleBarrierLabeling.compute_sample_weights(
            df['label'],
            df['return']
        )

        df = df.reset_index()

        logger.info("æ ‡ç­¾æ·»åŠ å®Œæˆ âœ“")
        return df


# æµ‹è¯•ä»£ç 
if __name__ == '__main__':
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    np.random.seed(42)
    dates = pd.date_range('2023-01-01', '2024-01-01', freq='D')

    # æ¨¡æ‹Ÿä»·æ ¼åºåˆ— (å¸¦è¶‹åŠ¿å’Œå™ªéŸ³)
    trend = np.linspace(100, 120, len(dates))
    noise = np.random.randn(len(dates)) * 2
    prices = pd.Series(trend + noise, index=dates)

    print("æµ‹è¯• Triple Barrier Labeling...")
    print("=" * 60)

    # åº”ç”¨ä¸‰é‡å£å’
    labels_df = TripleBarrierLabeling.apply_triple_barrier(
        prices,
        upper_barrier=0.03,
        lower_barrier=-0.02,
        max_holding_period=5,
        stop_loss=-0.01
    )

    print("\næ ‡ç­¾ç»“æœ:")
    print(labels_df.head(10))

    print("\næ ‡ç­¾ç»Ÿè®¡:")
    print(labels_df['label'].value_counts())
    print(f"\nå¹³å‡æŒæœ‰æœŸ: {labels_df['holding_period'].mean():.2f} å¤©")

    print("\nè§¦å‘ç±»å‹ç»Ÿè®¡:")
    print(labels_df['barrier_touched'].value_counts())

    print("\næ”¶ç›Šç‡ç»Ÿè®¡:")
    print(labels_df['return'].describe())

    # æµ‹è¯•æ·»åŠ åˆ° DataFrame
    test_df = pd.DataFrame({
        'date': dates,
        'close': prices.values,
        'volume': np.random.randint(1000000, 10000000, len(dates)),
    })

    test_df = TripleBarrierLabeling.add_labels_to_dataframe(
        test_df,
        upper_barrier=0.03,
        lower_barrier=-0.02,
        max_holding_period=5
    )

    print("\næ·»åŠ æ ‡ç­¾åçš„ DataFrame:")
    print(test_df[['date', 'close', 'label', 'barrier_touched', 'holding_period', 'sample_weight']].head(10))

    print("\n" + "=" * 60)
    print("Triple Barrier Labeling æµ‹è¯•å®Œæˆ!")

```

## nexus_with_proxy.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Notion Nexus - æ”¯æŒ API ä¸­è½¬çš„ç‰ˆæœ¬
æ”¯æŒå¤šç§ API ä¸­è½¬æ–¹æ¡ˆ
"""

import os
import sys
import time
import textwrap
import requests
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
NOTION_TOKEN = os.getenv("NOTION_TOKEN")
GEMINI_KEY = os.getenv("GEMINI_API_KEY")
PROXY_API_KEY = os.getenv("PROXY_API_KEY")
PROXY_API_URL = os.getenv("PROXY_API_URL")
DATABASE_ID = os.getenv("NOTION_DB_ID")
PROJECT_ROOT = os.getenv("PROJECT_ROOT", "/opt/mt5-crs/")

# Notion API é…ç½®
NOTION_VERSION = "2022-06-28"
NOTION_BASE_URL = "https://api.notion.com/v1"

def notion_headers():
    """è·å– Notion API è¯·æ±‚å¤´"""
    return {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Content-Type": "application/json",
        "Notion-Version": NOTION_VERSION
    }

def call_gemini_direct(prompt):
    """ç›´æ¥è°ƒç”¨ Google Gemini API"""
    try:
        # ä½¿ç”¨ç¨³å®šç‰ˆæœ¬ gemini-1.5-flash (ä¿®æ­£ï¼šgemini-2.5-flash ä¸å­˜åœ¨)
        # å‚è€ƒ Gemini Pro å®¡æŸ¥å»ºè®®ï¼šéªŒè¯æ­£ç¡®çš„APIæ¨¡å‹åç§°
        url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={GEMINI_KEY}"

        data = {
            "contents": [{
                "parts": [{
                    "text": prompt
                }]
            }]
        }

        response = requests.post(url, json=data, timeout=30)

        if response.status_code == 200:
            result = response.json()
            if "candidates" in result and result["candidates"]:
                return result["candidates"][0]["content"]["parts"][0]["text"]
            else:
                return "âŒ Gemini è¿”å›ç©ºå“åº”"
        else:
            return f"âŒ Gemini API Error: {response.status_code} - {response.text}"

    except Exception as e:
        return f"âŒ è°ƒç”¨ Gemini æ—¶å‡ºé”™: {e}"

def call_gemini_proxy_1(prompt):
    """ä½¿ç”¨ä¸­è½¬æœåŠ¡ 1 - Gemini Proxy"""
    try:
        url = "https://api.aiproxy.io/v1/chat/completions"

        headers = {
            "Authorization": f"Bearer {PROXY_API_KEY}",
            "Content-Type": "application/json"
        }

        data = {
            "model": "gemini-2.0-flash-exp",
            "messages": [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„é‡åŒ–å¼€å‘åŠ©æ‰‹ï¼Œè¯·æä¾›ä¸“ä¸šçš„æŠ€æœ¯åˆ†æå’Œå»ºè®®ã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "temperature": 0.7,
            "max_tokens": 4000
        }

        response = requests.post(url, headers=headers, json=data, timeout=30)

        if response.status_code == 200:
            result = response.json()
            if "choices" in result and result["choices"]:
                return result["choices"][0]["message"]["content"]
            else:
                return "âŒ ä¸­è½¬æœåŠ¡è¿”å›ç©ºå“åº”"
        else:
            return f"âŒ ä¸­è½¬æœåŠ¡ Error: {response.status_code} - {response.text}"

    except Exception as e:
        return f"âŒ è°ƒç”¨ä¸­è½¬æœåŠ¡æ—¶å‡ºé”™: {e}"

def call_gemini_proxy_2(prompt):
    """ä½¿ç”¨ä¸­è½¬æœåŠ¡ 2 - å›ºå®šä½¿ç”¨ Gemini 3 Pro Preview"""
    try:
        url = f"{PROXY_API_URL}/v1/chat/completions"

        headers = {
            'Accept': 'application/json',
            'Authorization': f'Bearer {PROXY_API_KEY}',
            'Content-Type': 'application/json'
        }

        data = {
            "model": "gemini-3-pro-preview",  # å›ºå®šä½¿ç”¨ Gemini 3 Pro Preview (å®æµ‹å¯ç”¨)
            "messages": [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„é‡åŒ–å¼€å‘åŠ©æ‰‹ï¼Œè¯·æä¾›ä¸“ä¸šçš„æŠ€æœ¯åˆ†æå’Œå»ºè®®ã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "temperature": 0.7,
            "max_tokens": 4000
        }

        print(f"   -> ä½¿ç”¨æ¨¡å‹: gemini-3-pro-preview (Gemini 3 Pro)")
        response = requests.post(url, headers=headers, json=data, timeout=60)

        if response.status_code == 200:
            result = response.json()
            if "choices" in result and result["choices"]:
                return result["choices"][0]["message"]["content"]
            else:
                return "âŒ ä¸­è½¬æœåŠ¡è¿”å›ç©ºå“åº”"
        else:
            return f"âŒ ä¸­è½¬æœåŠ¡ Error: {response.status_code} - {response.text}"

    except Exception as e:
        return f"âŒ è°ƒç”¨ Gemini 3 Pro æ—¶å‡ºé”™: {e}"

def call_gemini_api(prompt):
    """æ™ºèƒ½é€‰æ‹© Gemini API è°ƒç”¨æ–¹å¼"""
    print("   -> ğŸ”„ é€‰æ‹© API æœåŠ¡...")

    # ä¼˜å…ˆä½¿ç”¨é…ç½®çš„ä¸­è½¬æœåŠ¡
    if PROXY_API_URL and PROXY_API_KEY and not PROXY_API_KEY.startswith("your_"):
        print("   -> ğŸ“¡ ä½¿ç”¨è´­ä¹°çš„ä¸­è½¬æœåŠ¡...")
        result = call_gemini_proxy_2(prompt)
        if not result.startswith("âŒ"):
            print("   -> âœ… ä¸­è½¬æœåŠ¡æˆåŠŸ")
            return result
        else:
            print(f"   -> âš ï¸ ä¸­è½¬æœåŠ¡å¤±è´¥: {result}")

    # å¤‡é€‰ï¼šå°è¯•å…¶ä»–ä¸­è½¬æœåŠ¡
    if PROXY_API_KEY and not PROXY_API_KEY.startswith("your_"):
        print("   -> ğŸ“¡ å°è¯•å¤‡ç”¨ä¸­è½¬æœåŠ¡...")
        result = call_gemini_proxy_1(prompt)
        if not result.startswith("âŒ"):
            print("   -> âœ… å¤‡ç”¨ä¸­è½¬æˆåŠŸ")
            return result
        else:
            print(f"   -> âš ï¸ å¤‡ç”¨ä¸­è½¬å¤±è´¥: {result}")

    # æœ€åå°è¯•ç›´æ¥ API
    print("   -> ğŸ”— å°è¯•ç›´æ¥ API...")
    result = call_gemini_direct(prompt)
    if result.startswith("âŒ"):
        print(f"   -> âŒ æ‰€æœ‰ API éƒ½å¤±è´¥äº†")
    else:
        print("   -> âœ… ç›´æ¥ API æˆåŠŸ")

    return result

def read_local_file(filepath):
    """è¯»å–æœ¬åœ°æ–‡ä»¶å†…å®¹"""
    safe_path = os.path.normpath(os.path.join(PROJECT_ROOT, filepath.strip()))
    if not safe_path.startswith(PROJECT_ROOT):
        return f"\n[Security Alert: Access denied to {filepath}]\n"

    if os.path.exists(safe_path):
        try:
            with open(safe_path, 'r', encoding='utf-8') as f:
                content = f.read()
                if len(content) > 5000:
                    content = content[:5000] + "\n... [æ–‡ä»¶è¿‡é•¿å·²æˆªæ–­]"
                return f"\n\n--- FILE: {filepath} ---\n{content}\n--- END FILE ---\n"
        except Exception as e:
            return f"\n[Error reading file: {e}]\n"
    return f"\n[WARNING: File not found: {filepath}]\n"

def add_response_to_page(page_id, response_text):
    """å°†å›å¤æ·»åŠ åˆ°é¡µé¢"""
    try:
        chunks = textwrap.wrap(response_text, width=1800, replace_whitespace=False)

        children = [
            {
                "object": "block",
                "type": "heading_2",
                "heading_2": {
                    "rich_text": [{"text": {"content": "ğŸ¤– AI Response (via Proxy)"}}]
                }
            },
            {
                "object": "block",
                "type": "divider",
                "divider": {}
            }
        ]

        for chunk in chunks:
            children.append({
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [{"text": {"content": chunk}}]
                }
            })

        url = f"{NOTION_BASE_URL}/blocks/{page_id}/children"
        response = requests.patch(url, headers=notion_headers(), json={"children": children})

        if response.status_code != 200:
            print(f"âš ï¸ æ·»åŠ å›å¤å¤±è´¥: {response.status_code}")
            return False

        return True

    except Exception as e:
        print(f"âš ï¸ æ·»åŠ å›å¤æ—¶å‡ºé”™: {e}")
        return False

def process_page(page):
    """å¤„ç†å•ä¸ªé¡µé¢"""
    page_id = page["id"]
    props = page["properties"]

    # è·å–æ ‡é¢˜
    title = "Untitled Task"
    for field_name in ["åç§°", "Topic", "Title", "Name"]:
        if field_name in props and props[field_name].get("title"):
            title = props[field_name]["title"][0]["plain_text"]
            break

    print(f"ğŸš€ å¤„ç†ä»»åŠ¡: {title}")

    # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰å›å¤
    if page.get("has_children"):
        print("   -> é¡µé¢å·²æœ‰å†…å®¹ï¼Œè·³è¿‡å¤„ç†")
        return

    # æ™ºèƒ½æ–‡ä»¶å…³è”
    context_str = ""
    if "é£é™©ç®¡ç†" in title or "risk" in title.lower():
        files_to_read = ["src/strategy/risk_manager.py", "docs/BACKTEST_GUIDE.md"]
    elif "ç‰¹å¾å·¥ç¨‹" in title or "feature" in title.lower():
        files_to_read = ["src/feature_engineering/", "docs/ML_GUIDE.md"]
    elif "å›æµ‹" in title or "backtest" in title.lower():
        files_to_read = ["bin/run_backtest.py", "src/reporting/"]
    elif "ä»£ç " in title or "code" in title.lower():
        files_to_read = ["src/"]
    else:
        files_to_read = []

    for filepath in files_to_read:
        print(f"   -> è¯»å–æ–‡ä»¶: {filepath}")
        context_str += read_local_file(filepath)

    # æ„å»ºæç¤º
    full_prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„é‡åŒ–å¼€å‘åŠ©æ‰‹ã€‚ç”¨æˆ·æå‡ºäº†ä»¥ä¸‹é—®é¢˜æˆ–ä»»åŠ¡ï¼š

ä»»åŠ¡æ ‡é¢˜: {title}

ç›¸å…³ä»£ç ä¸Šä¸‹æ–‡:
{context_str}

è¯·æ ¹æ®ä»»åŠ¡æ ‡é¢˜å’Œä¸Šä¸‹æ–‡ï¼Œæä¾›ä¸“ä¸šçš„æŠ€æœ¯å›ç­”ã€‚å¦‚æœæ¶‰åŠä»£ç åˆ†æï¼Œè¯·æä¾›å…·ä½“çš„å»ºè®®å’Œæ”¹è¿›æ–¹æ¡ˆã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œæ ¼å¼ä½¿ç”¨ Markdownã€‚"""

    # è°ƒç”¨ AI
    print("   -> ğŸ§  AI æ€è€ƒä¸­...")
    reply_text = call_gemini_api(full_prompt)

    if "âŒ" in reply_text:
        print(f"   -> {reply_text}")

        # å¦‚æœæ‰€æœ‰ API éƒ½å¤±è´¥ï¼Œæ·»åŠ ä¸€ä¸ªå¤‡ç”¨å›å¤
        fallback_response = f"""# ğŸ¤– AI åˆ†æç»“æœ

## ä»»åŠ¡ï¼š{title}

å¾ˆæŠ±æ­‰ï¼ŒAI æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚ä½†æ ¹æ®ä»»åŠ¡æ ‡é¢˜ï¼Œæˆ‘å¯ä»¥æä¾›ä»¥ä¸‹åŸºç¡€åˆ†æï¼š

## å»ºè®®çš„åˆ†ææ–¹å‘ï¼š

### 1. ä»£ç è´¨é‡æ£€æŸ¥
- æ£€æŸ¥é”™è¯¯å¤„ç†æœºåˆ¶
- åˆ†ææ€§èƒ½ç“¶é¢ˆ
- å®¡æŸ¥ä»£ç ç»“æ„

### 2. åŠŸèƒ½åˆ†æ
- éªŒè¯ä¸šåŠ¡é€»è¾‘æ­£ç¡®æ€§
- æ£€æŸ¥è¾¹ç•Œæ¡ä»¶å¤„ç†
- è¯„ä¼°å¯ç»´æŠ¤æ€§

### 3. ä¼˜åŒ–å»ºè®®
- æå‡ä»£ç å¯è¯»æ€§
- ä¼˜åŒ–ç®—æ³•æ•ˆç‡
- å¢å¼ºé”™è¯¯å¤„ç†

### 4. æ‰‹åŠ¨æ£€æŸ¥æ–‡ä»¶
è¯·æ£€æŸ¥ä»¥ä¸‹ç›¸å…³æ–‡ä»¶ï¼š
{chr(10).join([f"- {file}" for file in files_to_read])}

---
*ç³»ç»Ÿå°†åœ¨ API æœåŠ¡æ¢å¤åæä¾›å®Œæ•´çš„ AI åˆ†æç»“æœã€‚*"""

        print("   -> ğŸ“ å†™å…¥å¤‡ç”¨å›å¤...")
        add_response_to_page(page_id, fallback_response)
        print("âœ… å¤„ç†å®Œæˆï¼ˆå¤‡ç”¨å›å¤ï¼‰")
        return

    # å†™å…¥å›å¤
    print("   -> ğŸ“ å†™å…¥å›å¤...")
    if add_response_to_page(page_id, reply_text):
        print("âœ… å¤„ç†å®Œæˆ")
    else:
        print("âŒ å†™å…¥å¤±è´¥")

def monitor_database():
    """ç›‘æ§æ•°æ®åº“å¹¶å¤„ç†æ–°é¡µé¢"""
    print(f"ğŸ‘€ æ­£åœ¨ç›‘æ§ Notion æ•°æ®åº“...")
    print("æ£€æµ‹åˆ°æ–°é¡µé¢æ—¶ä¼šè‡ªåŠ¨è°ƒç”¨ AI å¤„ç†")
    print("æ”¯æŒå¤šç§ API ä¸­è½¬æ–¹æ¡ˆ")
    print("æŒ‰ Ctrl+C åœæ­¢ç›‘æ§\n")

    processed_pages = set()

    while True:
        try:
            url = f"{NOTION_BASE_URL}/databases/{DATABASE_ID}/query"
            response = requests.post(url, headers=notion_headers(), json={})

            if response.status_code == 200:
                query_result = response.json()
                pages = query_result.get("results", [])

                new_pages = [page for page in pages if page["id"] not in processed_pages]

                if new_pages:
                    print(f"å‘ç° {len(new_pages)} ä¸ªæ–°é¡µé¢")
                    for page in new_pages:
                        process_page(page)
                        processed_pages.add(page["id"])
                        print("-" * 40)
                else:
                    sys.stdout.write(".")
                    sys.stdout.flush()

        except KeyboardInterrupt:
            print("\nğŸ‘‹ ç›‘æ§å·²åœæ­¢")
            break
        except Exception as e:
            print(f"\nâš ï¸ ç›‘æ§é”™è¯¯: {e}")

        time.sleep(5)

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ Notion Nexus - API ä¸­è½¬ç‰ˆ")
    print("=" * 60)

    # æ˜¾ç¤ºå½“å‰é…ç½®
    print("ğŸ“‹ å½“å‰é…ç½®:")
    print(f"   Notion Token: {'âœ… å·²é…ç½®' if NOTION_TOKEN else 'âŒ æœªé…ç½®'}")
    print(f"   Gemini Key: {'âœ… å·²é…ç½®' if GEMINI_KEY else 'âŒ æœªé…ç½®'}")
    print(f"   ä»£ç† API Key: {'âœ… å·²é…ç½®' if PROXY_API_KEY and not PROXY_API_KEY.startswith('your_') else 'âŒ æœªé…ç½®'}")
    print(f"   ä»£ç† API URL: {'âœ… å·²é…ç½®' if PROXY_API_URL else 'âŒ æœªé…ç½®'}")

    if not DATABASE_ID:
        print("âŒ NOTION_DB_ID æœªé…ç½®")
        return

    print(f"\nğŸ”„ API è°ƒç”¨ç­–ç•¥:")
    print("   1. å°è¯•ä¸­è½¬æœåŠ¡ (å¦‚æœé…ç½®äº†ä»£ç†)")
    print("   2. å°è¯•è‡ªå®šä¹‰ä¸­è½¬ (å¦‚æœé…ç½®äº†URL)")
    print("   3. å°è¯•ç›´æ¥ Gemini API")
    print("   4. å¦‚æœéƒ½å¤±è´¥ï¼Œæä¾›åŸºç¡€åˆ†æ")

    # æµ‹è¯•è¿æ¥
    print(f"\nğŸ”§ æµ‹è¯•è¿æ¥...")
    try:
        url = f"{NOTION_BASE_URL}/databases/{DATABASE_ID}"
        response = requests.get(url, headers=notion_headers())

        if response.status_code == 200:
            print("âœ… Notion è¿æ¥æˆåŠŸï¼")
            db_info = response.json()
            print(f"ğŸ“Š æ•°æ®åº“: {db_info.get('title', [{}])[0].get('plain_text', 'Unknown')}")

            print("\nğŸ“ ä½¿ç”¨è¯´æ˜:")
            print("1. åœ¨ Notion æ•°æ®åº“ä¸­åˆ›å»ºæ–°é¡µé¢")
            print("2. è®¾ç½®é¡µé¢æ ‡é¢˜ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†")
            print("3. æ”¯æŒå¤šç§ API ä¸­è½¬æ–¹æ¡ˆ")
            print("4. å¦‚æœ API ä¸å¯ç”¨ï¼Œä¼šæä¾›åŸºç¡€åˆ†æ")
            print("\nå¼€å§‹ç›‘æ§...\n")

            monitor_database()
        else:
            print(f"âŒ æ— æ³•è®¿é—®æ•°æ®åº“: {response.status_code}")

    except Exception as e:
        print(f"âŒ è¿æ¥æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    main()
```

## gemini_review_bridge.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Gemini Pro å¤–éƒ¨å®¡æŸ¥æ¡¥æ¥ç³»ç»Ÿ
ä¸º Gemini 3 Pro æä¾›é¡¹ç›®ä»£ç å’Œä¸Šä¸‹æ–‡å®¡æŸ¥èƒ½åŠ›
"""

import os
import sys
import json
import subprocess
import requests
from datetime import datetime
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
NOTION_TOKEN = os.getenv("NOTION_TOKEN")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
PROXY_API_KEY = os.getenv("PROXY_API_KEY")
PROXY_API_URL = os.getenv("PROXY_API_URL")
PROJECT_ROOT = os.getenv("PROJECT_ROOT", "/opt/mt5-crs/")

class GeminiReviewBridge:
    def __init__(self):
        self.project_root = PROJECT_ROOT
        self.review_cache = {}
        self.notion_base_url = "https://api.notion.com/v1"
        self.notion_headers = {
            "Authorization": f"Bearer {NOTION_TOKEN}",
            "Content-Type": "application/json",
            "Notion-Version": "2022-06-28"
        }

    def get_project_overview(self):
        """è·å–é¡¹ç›®æœ€æ–°æ¦‚è§ˆ"""
        print("ğŸ” è·å–é¡¹ç›®æœ€æ–°æ¦‚è§ˆ...")

        overview = {
            "project_name": "MT5-CRS",
            "last_updated": datetime.now().isoformat(),
            "development_stage": "å·¥å• #011 å®ç›˜äº¤æ˜“ç³»ç»Ÿå¯¹æ¥",
            "git_status": self.get_git_status(),
            "recent_changes": self.get_recent_changes(days=7),
            "current_focus": "MT5 API é›†æˆä¸å®ç›˜äº¤æ˜“ç³»ç»Ÿ",
            "priority_tasks": self.get_priority_tasks()
        }

        return overview

    def get_git_status(self):
        """è·å– Git ä»“åº“çŠ¶æ€"""
        try:
            # è·å–å½“å‰åˆ†æ”¯
            branch = subprocess.check_output(
                ["git", "branch", "--show-current"],
                cwd=self.project_root,
                text=True
            ).strip()

            # è·å–æœ€æ–°æäº¤ä¿¡æ¯
            latest_commit = subprocess.check_output(
                ["git", "log", "-1", "--format=%H|%an|%s|%cd", "--date=iso"],
                cwd=self.project_root,
                text=True
            ).strip().split('|')

            # è·å–æœªæäº¤çš„æ›´æ”¹
            status = subprocess.check_output(
                ["git", "status", "--porcelain"],
                cwd=self.project_root,
                text=True
            ).strip()

            return {
                "current_branch": branch,
                "latest_commit": {
                    "hash": latest_commit[0],
                    "author": latest_commit[1],
                    "message": latest_commit[2],
                    "date": latest_commit[3]
                },
                "uncommitted_changes": len(status.split('\n')) if status else 0
            }
        except Exception as e:
            return {"error": str(e)}

    def get_recent_changes(self, days=7):
        """è·å–æœ€è¿‘çš„ä»£ç å˜æ›´"""
        try:
            changes = subprocess.check_output(
                ["git", "log", f"--since={days} days ago", "--name-only", "--pretty=format:%H|%s"],
                cwd=self.project_root,
                text=True
            ).strip()

            if not changes:
                return {"message": "No recent changes"}

            lines = changes.split('\n')
            commits = []

            i = 0
            while i < len(lines):
                if '|' in lines[i]:
                    commit_hash, commit_message = lines[i].split('|', 1)
                    i += 1
                    changed_files = []
                    while i < len(lines) and '|' not in lines[i]:
                        if lines[i].strip():
                            changed_files.append(lines[i].strip())
                        i += 1
                    commits.append({
                        "hash": commit_hash,
                        "message": commit_message,
                        "files": changed_files
                    })
                else:
                    i += 1

            return commits
        except Exception as e:
            return {"error": str(e)}

    def get_priority_tasks(self):
        """ä» Notion è·å–ä¼˜å…ˆä»»åŠ¡"""
        try:
            # æŸ¥æ‰¾ Issues æ•°æ®åº“ä¸­çš„é«˜ä¼˜å…ˆçº§ä»»åŠ¡
            search_url = f"{self.notion_base_url}/search"
            search_data = {
                "query": "Issues",
                "filter": {
                    "property": "object",
                    "value": "database"
                }
            }

            response = requests.post(search_url, headers=self.notion_headers, json=search_data)

            if response.status_code == 200:
                results = response.json().get("results", [])
                for db in results:
                    title = db.get("title", [])
                    if title and "Issues" in title[0].get("plain_text", ""):
                        # è·å–æ•°æ®åº“å†…å®¹
                        db_id = db["id"]
                        query_url = f"{self.notion_base_url}/databases/{db_id}/query"

                        query_response = requests.post(
                            query_url,
                            headers=self.notion_headers,
                            json={}
                        )

                        if query_response.status_code == 200:
                            pages = query_response.json().get("results", [])
                            priority_tasks = []

                            for page in pages:
                                task_name = ""
                                priority = ""
                                status = ""

                                # æå–ä»»åŠ¡ä¿¡æ¯
                                props = page.get("properties", {})
                                if "Task Name" in props:
                                    name_list = props["Task Name"].get("title", [])
                                    if name_list:
                                        task_name = name_list[0].get("plain_text", "")

                                if "Priority" in props:
                                    priority_select = props["Priority"].get("select", {})
                                    priority = priority_select.get("name", "")

                                if "Status" in props:
                                    status_select = props["Status"].get("select", {})
                                    status = status_select.get("name", "")

                                if priority in ["P0", "P1"] and status != "Done":
                                    priority_tasks.append({
                                        "name": task_name,
                                        "priority": priority,
                                        "status": status,
                                        "page_id": page["id"]
                                    })

                            return priority_tasks
        except Exception as e:
            return {"error": str(e)}

    def get_code_context(self, file_paths=None):
        """è·å–ä»£ç ä¸Šä¸‹æ–‡"""
        if not file_paths:
            # è·å–æœ€è¿‘ä¿®æ”¹çš„æ ¸å¿ƒæ–‡ä»¶
            file_paths = [
                "src/strategy/risk_manager.py",
                "src/feature_engineering/",
                "src/models/",
                "bin/run_backtest.py",
                "nexus_with_proxy.py"
            ]

        code_context = {}

        for file_path in file_paths:
            try:
                full_path = os.path.join(self.project_root, file_path)

                if os.path.isdir(full_path):
                    # å¦‚æœæ˜¯ç›®å½•ï¼Œè·å–æ‰€æœ‰ Python æ–‡ä»¶
                    code_files = []
                    for root, dirs, files in os.walk(full_path):
                        for file in files:
                            if file.endswith('.py'):
                                rel_path = os.path.relpath(
                                    os.path.join(root, file),
                                    self.project_root
                                )
                                code_files.append(rel_path)

                    code_context[file_path] = {
                        "type": "directory",
                        "files": code_files
                    }

                elif os.path.exists(full_path):
                    # å¦‚æœæ˜¯æ–‡ä»¶ï¼Œè¯»å–å†…å®¹
                    with open(full_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                        # é™åˆ¶æ–‡ä»¶å¤§å°ä»¥é¿å… token é™åˆ¶
                        if len(content) > 8000:
                            content = content[:8000] + "\n... [æ–‡ä»¶è¿‡é•¿å·²æˆªæ–­]"

                        code_context[file_path] = {
                            "type": "file",
                            "content": content,
                            "size": len(content),
                            "last_modified": os.path.getmtime(full_path)
                        }

            except Exception as e:
                code_context[file_path] = {
                    "error": str(e)
                }

        return code_context

    def get_ai_command_center_tasks(self):
        """è·å– AI Command Center ä¸­çš„æœ€æ–°ä»»åŠ¡"""
        try:
            # æŸ¥æ‰¾ AI Command Center æ•°æ®åº“
            search_url = f"{self.notion_base_url}/search"
            search_data = {
                "query": "AI Command Center",
                "filter": {
                    "property": "object",
                    "value": "database"
                }
            }

            response = requests.post(search_url, headers=self.notion_headers, json=search_data)

            if response.status_code == 200:
                results = response.json().get("results", [])
                for db in results:
                    title = db.get("title", [])
                    if title and "AI Command Center" in title[0].get("plain_text", ""):
                        # è·å–æœ€æ–°çš„ä»»åŠ¡
                        db_id = db["id"]
                        query_url = f"{self.notion_base_url}/databases/{db_id}/query"

                        query_response = requests.post(
                            query_url,
                            headers=self.notion_headers,
                            json={"sorts": [{"property": "created_time", "direction": "descending"}]}
                        )

                        if query_response.status_code == 200:
                            pages = query_response.json().get("results", [])
                            recent_tasks = []

                            for page in pages[:10]:  # è·å–æœ€è¿‘10ä¸ªä»»åŠ¡
                                task_title = ""
                                context_files = []

                                props = page.get("properties", {})
                                if "Name" in props:
                                    name_list = props["Name"].get("title", [])
                                    if name_list:
                                        task_title = name_list[0].get("plain_text", "")

                                if "Context Files" in props:
                                    files_list = props["Context Files"].get("multi_select", [])
                                    context_files = [f.get("name", "") for f in files_list]

                                recent_tasks.append({
                                    "title": task_title,
                                    "context_files": context_files,
                                    "created_time": page.get("created_time"),
                                    "page_id": page["id"],
                                    "url": page.get("url", "")
                                })

                            return recent_tasks
        except Exception as e:
            return {"error": str(e)}

    def generate_review_prompt(self, focus_area=None):
        """ç”Ÿæˆ Gemini Pro å®¡æŸ¥æç¤º"""
        print("ğŸ“ ç”Ÿæˆ Gemini Pro å®¡æŸ¥æç¤º...")

        # è·å–é¡¹ç›®æ¦‚è§ˆ
        overview = self.get_project_overview()

        # è·å–ä»£ç ä¸Šä¸‹æ–‡
        code_context = self.get_code_context()

        # è·å– AI Command Center ä»»åŠ¡
        ai_tasks = self.get_ai_command_center_tasks()

        # è·å–ä¼˜å…ˆä»»åŠ¡
        priority_tasks = self.get_priority_tasks()

        # æ„å»ºå®¡æŸ¥æç¤º
        prompt = f"""# MT5-CRS é¡¹ç›®å®¡æŸ¥è¯·æ±‚

## é¡¹ç›®æ¦‚è§ˆ
- **é¡¹ç›®åç§°**: {overview['project_name']}
- **å½“å‰é˜¶æ®µ**: {overview['development_stage']}
- **æœ€åæ›´æ–°**: {overview['last_updated']}
- **å½“å‰ç„¦ç‚¹**: {overview['current_focus']}

## Git çŠ¶æ€
- **å½“å‰åˆ†æ”¯**: {overview['git_status'].get('current_branch', 'Unknown')}
- **æœ€æ–°æäº¤**: {overview['git_status'].get('latest_commit', {}).get('message', 'No commits')}
- **æœªæäº¤æ›´æ”¹**: {overview['git_status'].get('uncommitted_changes', 0)} ä¸ªæ–‡ä»¶

## ä¼˜å…ˆä»»åŠ¡
"""

        if isinstance(priority_tasks, list):
            for task in priority_tasks[:5]:  # æ˜¾ç¤ºå‰5ä¸ªä¼˜å…ˆä»»åŠ¡
                prompt += f"- **{task.get('priority', 'Unknown')}**: {task.get('name', 'Untitled')} ({task.get('status', 'Unknown')})\n"
        else:
            prompt += f"- æ— æ³•è·å–ä¼˜å…ˆä»»åŠ¡ä¿¡æ¯\n"

        prompt += f"""
## AI Command Center æœ€æ–°ä»»åŠ¡
"""

        if isinstance(ai_tasks, list):
            for task in ai_tasks[:5]:  # æ˜¾ç¤ºæœ€è¿‘5ä¸ªä»»åŠ¡
                prompt += f"- **{task.get('title', 'Untitled')}** (æ–‡ä»¶: {', '.join(task.get('context_files', []))})\n"
        else:
            prompt += f"- æ— æ³•è·å–AIä»»åŠ¡ä¿¡æ¯\n"

        prompt += f"""
## æ ¸å¿ƒä»£ç å®¡æŸ¥
"""

        # æ·»åŠ å…³é”®æ–‡ä»¶å†…å®¹
        key_files = [
            "src/strategy/risk_manager.py",
            "nexus_with_proxy.py",
            "src/feature_engineering/"
        ]

        for file_path in key_files:
            if file_path in code_context:
                file_info = code_context[file_path]
                if file_info.get("type") == "file":
                    prompt += f"\n### ğŸ“„ {file_path}\n"
                    prompt += f"```python\n{file_info.get('content', 'Unable to read file')[:2000]}...\n```\n"
                elif file_info.get("type") == "directory":
                    prompt += f"\n### ğŸ“ {file_path}\n"
                    files = file_info.get("files", [])
                    if files:
                        prompt += f"åŒ…å« {len(files)} ä¸ªæ–‡ä»¶: {', '.join(files[:10])}"
                        if len(files) > 10:
                            prompt += f" ç­‰ {len(files)} ä¸ªæ–‡ä»¶"

        prompt += f"""
## å®¡æŸ¥é‡ç‚¹
"""

        if focus_area:
            prompt += f"**é‡ç‚¹å…³æ³¨**: {focus_area}\n"
        else:
            prompt += """**é‡ç‚¹å…³æ³¨**:
1. MT5 API é›†æˆæ¶æ„è®¾è®¡
2. é£é™©ç®¡ç†ç³»ç»Ÿçš„ç¨³å®šæ€§
3. ä»£ç è´¨é‡å’Œæœ€ä½³å®è·µ
4. æ€§èƒ½ä¼˜åŒ–æœºä¼š
5. æ½œåœ¨çš„å®‰å…¨é£é™©
"""

        prompt += """
## å®¡æŸ¥è¦æ±‚
è¯·å¯¹ä»¥ä¸Šå†…å®¹è¿›è¡Œå…¨é¢å®¡æŸ¥ï¼Œé‡ç‚¹å…³æ³¨ï¼š

1. **æ¶æ„è¯„ä¼°**: å½“å‰è®¾è®¡æ˜¯å¦é€‚åˆ MT5 å®ç›˜äº¤æ˜“ç³»ç»Ÿï¼Ÿ
2. **ä»£ç è´¨é‡**: æ˜¯å¦å­˜åœ¨æ½œåœ¨ bugã€æ€§èƒ½é—®é¢˜æˆ–å®‰å…¨é—®é¢˜ï¼Ÿ
3. **æœ€ä½³å®è·µ**: æ˜¯å¦éµå¾ª Python å’Œé‡åŒ–å¼€å‘çš„æœ€ä½³å®è·µï¼Ÿ
4. **ä¼˜åŒ–å»ºè®®**: æœ‰å“ªäº›å…·ä½“çš„æ”¹è¿›å»ºè®®ï¼Ÿ
5. **é£é™©è¯„ä¼°**: å½“å‰å®ç°å¯èƒ½é¢ä¸´å“ªäº›æŠ€æœ¯æˆ–ä¸šåŠ¡é£é™©ï¼Ÿ

è¯·æä¾›å…·ä½“ã€å¯æ“ä½œçš„å»ºè®®ï¼Œå¹¶æ ‡æ³¨ä¼˜å…ˆçº§ã€‚
"""

        return prompt

    def send_to_gemini(self, prompt, save_response=True):
        """å‘é€æç¤ºåˆ° Gemini Pro"""
        print("ğŸ¤– å‘é€å®¡æŸ¥è¯·æ±‚åˆ° Gemini Pro...")

        # å°è¯•ä½¿ç”¨ä¸­è½¬æœåŠ¡
        if PROXY_API_URL and PROXY_API_KEY:
            try:
                return self._call_gemini_proxy(prompt, save_response)
            except Exception as e:
                print(f"âš ï¸ ä¸­è½¬æœåŠ¡å¤±è´¥: {e}")

        # å¤‡ç”¨ï¼šç›´æ¥è°ƒç”¨
        try:
            return self._call_gemini_direct(prompt, save_response)
        except Exception as e:
            return {"error": f"æ‰€æœ‰ API è°ƒç”¨å¤±è´¥: {e}"}

    def _call_gemini_proxy(self, prompt, save_response=True):
        """ä½¿ç”¨ä¸­è½¬æœåŠ¡è°ƒç”¨ Gemini"""
        url = f"{PROXY_API_URL}/v1/chat/completions"

        headers = {
            "Authorization": f"Bearer {PROXY_API_KEY}",
            "Content-Type": "application/json"
        }

        data = {
            "model": "gemini-2.5-pro",  # æ”¹è¿›: ä½¿ç”¨å®˜æ–¹æ”¯æŒçš„æœ€æ–°æ¨¡å‹ (2025 å¹´å‘å¸ƒ)
            "messages": [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„é‡åŒ–äº¤æ˜“ç³»ç»Ÿå’Œ Python å¼€å‘ä¸“å®¶ï¼Œè¯·æä¾›ä¸“ä¸šçš„ä»£ç å®¡æŸ¥å’Œæ¶æ„å»ºè®®ã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "temperature": 0.7,
            "max_tokens": 8000
        }

        response = requests.post(url, headers=headers, json=data, timeout=60)  # æ”¹è¿›: ç¼©çŸ­è¶…æ—¶åˆ°60ç§’

        if response.status_code == 200:
            result = response.json()
            if "choices" in result and result["choices"]:
                review_text = result["choices"][0]["message"]["content"]

                if save_response:
                    self._save_review_response(prompt, review_text)

                return {
                    "success": True,
                    "review": review_text,
                    "model": "gemini-2.5-pro (via proxy)",
                    "timestamp": datetime.now().isoformat()
                }

        return {"error": f"API è°ƒç”¨å¤±è´¥: {response.status_code}"}

    def _call_gemini_direct(self, prompt, save_response=True):
        """ç›´æ¥è°ƒç”¨ Gemini API"""
        # æ”¹è¿›: ä½¿ç”¨å®˜æ–¹æ”¯æŒçš„æœ€æ–°æ¨¡å‹ gemini-2.5-pro (ç¨³å®š/é«˜æ€§èƒ½) æˆ– gemini-2.5-flash (å¿«é€Ÿ)
        url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?key={GEMINI_API_KEY}"

        data = {
            "contents": [{
                "parts": [{
                    "text": prompt
                }]
            }]
        }

        response = requests.post(url, json=data, timeout=60)  # æ”¹è¿›: ç¼©çŸ­è¶…æ—¶åˆ°60ç§’

        if response.status_code == 200:
            result = response.json()
            if "candidates" in result and result["candidates"]:
                review_text = result["candidates"][0]["content"]["parts"][0]["text"]

                if save_response:
                    self._save_review_response(prompt, review_text)

                return {
                    "success": True,
                    "review": review_text,
                    "model": "gemini-2.5-pro (direct)",
                    "timestamp": datetime.now().isoformat()
                }

        return {"error": f"API è°ƒç”¨å¤±è´¥: {response.status_code}"}

    def _save_review_response(self, prompt, response):
        """ä¿å­˜å®¡æŸ¥å“åº”åˆ°æ–‡ä»¶"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            review_file = os.path.join(
                self.project_root,
                "docs",
                "reviews",
                f"gemini_review_{timestamp}.md"
            )

            os.makedirs(os.path.dirname(review_file), exist_ok=True)

            with open(review_file, 'w', encoding='utf-8') as f:
                f.write(f"# Gemini Pro ä»£ç å®¡æŸ¥æŠ¥å‘Š\n\n")
                f.write(f"**æ—¶é—´**: {datetime.now().isoformat()}\n\n")
                f.write(f"## å®¡æŸ¥è¯·æ±‚\n\n```\n{prompt[:1000]}...\n```\n\n")
                f.write(f"## å®¡æŸ¥ç»“æœ\n\n{response}\n")

            print(f"âœ… å®¡æŸ¥æŠ¥å‘Šå·²ä¿å­˜åˆ°: {review_file}")

        except Exception as e:
            print(f"âš ï¸ ä¿å­˜å®¡æŸ¥æŠ¥å‘Šå¤±è´¥: {e}")

    def create_review_task_in_notion(self, review_result):
        """åœ¨ Notion ä¸­åˆ›å»ºå®¡æŸ¥ä»»åŠ¡"""
        try:
            # æŸ¥æ‰¾ AI Command Center æ•°æ®åº“
            search_url = f"{self.notion_base_url}/search"
            search_data = {
                "query": "AI Command Center",
                "filter": {
                    "property": "object",
                    "value": "database"
                }
            }

            response = requests.post(search_url, headers=self.notion_headers, json=search_data)

            if response.status_code == 200:
                results = response.json().get("results", [])
                for db in results:
                    title = db.get("title", [])
                    if title and "AI Command Center" in title[0].get("plain_text", ""):
                        # åˆ›å»ºå®¡æŸ¥ä»»åŠ¡
                        create_url = f"{self.notion_base_url}/pages"

                        page_data = {
                            "parent": {"database_id": db["id"]},
                            "properties": {
                                "Name": {
                                    "title": [{"text": {"content": f"ğŸ” Gemini Pro ä»£ç å®¡æŸ¥ - {datetime.now().strftime('%m-%d %H:%M')}"}}]
                                },
                                "Context Files": {
                                    "multi_select": [
                                        {"name": "src/strategy/risk_manager.py"},
                                        {"name": "nexus_with_proxy.py"},
                                        {"name": "src/feature_engineering/"}
                                    ]
                                }
                            }
                        }

                        # æˆªå–å®¡æŸ¥ç»“æœä»¥é¿å…è¿‡å¤§
                        review_text = review_result.get("review", "")
                        if len(review_text) > 3000:
                            review_text = review_text[:3000] + "\n... [å®¡æŸ¥ç»“æœè¿‡é•¿å·²æˆªæ–­ï¼Œå®Œæ•´æŠ¥å‘Šè¯·æŸ¥çœ‹ docs/reviews/]"

                        children = [
                            {
                                "object": "block",
                                "type": "heading_2",
                                "heading_2": {
                                    "rich_text": [{"text": {"content": "ğŸ¤– Gemini Pro å®¡æŸ¥ç»“æœ"}}]
                                }
                            },
                            {
                                "object": "block",
                                "type": "paragraph",
                                "paragraph": {
                                    "rich_text": [{"text": {"content": f"**æ¨¡å‹**: {review_result.get('model', 'Unknown')}\n**æ—¶é—´**: {review_result.get('timestamp', 'Unknown')}\n"}}]
                                }
                            },
                            {
                                "object": "block",
                                "type": "divider",
                                "divider": {}
                            },
                            {
                                "object": "block",
                                "type": "paragraph",
                                "paragraph": {
                                    "rich_text": [{"text": {"content": review_text}}]
                                }
                            }
                        ]

                        page_data["children"] = children

                        create_response = requests.post(create_url, headers=self.notion_headers, json=page_data)

                        if create_response.status_code == 200:
                            result = create_response.json()
                            print(f"âœ… å®¡æŸ¥ä»»åŠ¡å·²åˆ›å»º: {result.get('url', 'Unknown')}")
                            return result["id"]
                        else:
                            print(f"âŒ åˆ›å»ºå®¡æŸ¥ä»»åŠ¡å¤±è´¥: {create_response.status_code}")
                            return None

        except Exception as e:
            print(f"âŒ åˆ›å»º Notion ä»»åŠ¡æ—¶å‡ºé”™: {e}")
            return None

def main():
    """ä¸»å‡½æ•° - æ‰§è¡Œå®Œæ•´çš„ Gemini å®¡æŸ¥æµç¨‹"""
    print("=" * 80)
    print("ğŸ¤– Gemini Pro å¤–éƒ¨å®¡æŸ¥ç³»ç»Ÿ")
    print("ğŸ“Š è·å–é¡¹ç›®æœ€æ–°çŠ¶æ€ + ğŸ” æ·±åº¦ä»£ç å®¡æŸ¥")
    print("=" * 80)

    bridge = GeminiReviewBridge()

    # 1. ç”Ÿæˆå®¡æŸ¥æç¤º
    print("\n" + "="*60)
    print("ğŸ“ æ­¥éª¤ 1: ç”Ÿæˆå®¡æŸ¥æç¤º")
    print("="*60)

    prompt = bridge.generate_review_prompt(focus_area="MT5 API é›†æˆå’Œå®ç›˜äº¤æ˜“ç³»ç»Ÿ")

    print(f"ğŸ“‹ æç¤ºé•¿åº¦: {len(prompt)} å­—ç¬¦")

    # 2. å‘é€åˆ° Gemini Pro
    print("\n" + "="*60)
    print("ğŸ¤– æ­¥éª¤ 2: å‘é€å®¡æŸ¥è¯·æ±‚åˆ° Gemini Pro")
    print("="*60)

    review_result = bridge.send_to_gemini(prompt)

    if review_result.get("success"):
        print("âœ… Gemini Pro å®¡æŸ¥å®Œæˆ")
        print(f"ğŸ“Š ä½¿ç”¨æ¨¡å‹: {review_result.get('model', 'Unknown')}")
        print(f"â° å®¡æŸ¥æ—¶é—´: {review_result.get('timestamp', 'Unknown')}")

        # 3. åœ¨ Notion ä¸­åˆ›å»ºå®¡æŸ¥ä»»åŠ¡
        print("\n" + "="*60)
        print("ğŸ“ æ­¥éª¤ 3: åœ¨ Notion ä¸­åˆ›å»ºå®¡æŸ¥ä»»åŠ¡")
        print("="*60)

        task_id = bridge.create_review_task_in_notion(review_result)

        if task_id:
            print("âœ… å®¡æŸ¥ä»»åŠ¡å·²æˆåŠŸåˆ›å»ºåˆ° AI Command Center")

        # 4. æ˜¾ç¤ºå®¡æŸ¥ç»“æœæ‘˜è¦
        print("\n" + "="*60)
        print("ğŸ“‹ å®¡æŸ¥ç»“æœæ‘˜è¦")
        print("="*60)

        review_text = review_result.get("review", "")
        if len(review_text) > 1000:
            print(review_text[:1000] + "\n... [å®Œæ•´æŠ¥å‘Šè¯·æŸ¥çœ‹ Notion æˆ– docs/reviews/]")
        else:
            print(review_text)

    else:
        print(f"âŒ Gemini å®¡æŸ¥å¤±è´¥: {review_result.get('error', 'Unknown error')}")

    print("\n" + "="*80)
    print("ğŸ¯ Gemini Pro å¤–éƒ¨å®¡æŸ¥å®Œæˆ")
    print("ğŸ’¡ æç¤º: æŸ¥çœ‹å®Œæ•´æŠ¥å‘Šè¯·è®¿é—® Notion AI Command Center")
    print("ğŸ“ æœ¬åœ°æŠ¥å‘Šå­˜å‚¨ä½ç½®: docs/reviews/")
    print("="*80)

if __name__ == "__main__":
    main()
```

## bin/run_backtest.py

```python
#!/usr/bin/env python3
"""
å›æµ‹ä¸»ç¨‹åº - æ‰§è¡Œ Walk-Forward Backtesting

ä½¿ç”¨æ–¹æ³•ï¼š
    python bin/run_backtest.py --symbol EURUSD --start-date 2023-01-01 --end-date 2024-12-31

ç‰¹ç‚¹ï¼š
    1. æ”¯æŒ Walk-Forward éªŒè¯ï¼ˆè®­ç»ƒé›†/æµ‹è¯•é›†æ—¶é—´åˆ†å‰²ï¼‰
    2. çœŸå®äº¤æ˜“æˆæœ¬æ¨¡æ‹Ÿï¼ˆç‚¹å·® + æ‰‹ç»­è´¹ + æ»‘ç‚¹ï¼‰
    3. è‡ªåŠ¨ç”Ÿæˆ HTML å›æµ‹æŠ¥å‘Š
    4. å¯¹æ¯”ä¹°å…¥æŒæœ‰åŸºå‡†ç­–ç•¥
"""

import argparse
import sys
import os
from pathlib import Path
from datetime import datetime, timedelta
import logging
from concurrent.futures import ProcessPoolExecutor, as_completed

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import backtrader as bt
import pandas as pd
import numpy as np

from src.strategy.ml_strategy import MLStrategy, BuyAndHoldStrategy
from src.strategy.risk_manager import KellySizer, DynamicRiskManager

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class MLDataFeed(bt.feeds.PandasData):
    """
    è‡ªå®šä¹‰ DataFeed - åŠ è½½ç‰¹å¾å’Œé¢„æµ‹æ•°æ®

    æ‰©å±•å­—æ®µï¼š
        - y_pred_proba_long: åšå¤šé¢„æµ‹æ¦‚ç‡
        - y_pred_proba_short: åšç©ºé¢„æµ‹æ¦‚ç‡
        - volatility: æ³¢åŠ¨ç‡ï¼ˆå¯é€‰ï¼‰
    """

    lines = ('y_pred_proba_long', 'y_pred_proba_short', 'volatility',)

    params = (
        ('datetime', None),
        ('open', 'open'),
        ('high', 'high'),
        ('low', 'low'),
        ('close', 'close'),
        ('volume', 'volume'),
        ('openinterest', -1),
        ('y_pred_proba_long', 'y_pred_proba_long'),
        ('y_pred_proba_short', 'y_pred_proba_short'),
        ('volatility', 'volatility'),
    )


def run_single_fold(fold_num: int, test_df: pd.DataFrame, train_dates: tuple,
                   test_dates: tuple, config: dict) -> dict:
    """
    æ‰§è¡Œå•ä¸ª Walk-Forward çª—å£çš„å›æµ‹ï¼ˆé¡¶å±‚å‡½æ•°ï¼Œç”¨äºå¤šè¿›ç¨‹ï¼‰

    Args:
        fold_num: çª—å£ç¼–å·
        test_df: æµ‹è¯•é›†æ•°æ®
        train_dates: (train_start, train_end)
        test_dates: (test_start, test_end)
        config: é…ç½®å­—å…¸

    Returns:
        dict: è¯¥çª—å£çš„å›æµ‹ç»“æœ
    """
    # åœ¨å­è¿›ç¨‹ä¸­åˆ›å»ºæ–°çš„ Cerebro å®ä¾‹ï¼ˆé¿å… pickle é—®é¢˜ï¼‰
    cerebro = bt.Cerebro()

    # æ·»åŠ ç­–ç•¥
    cerebro.addstrategy(MLStrategy)

    # æ·»åŠ æ•°æ®
    data = MLDataFeed(dataname=test_df)
    cerebro.adddata(data)

    # è®¾ç½®åˆå§‹èµ„é‡‘å’Œäº¤æ˜“æˆæœ¬
    initial_cash = config.get('initial_cash', 100000.0)
    cerebro.broker.setcash(initial_cash)

    cerebro.broker.setcommission(
        commission=config.get('commission', 0.0002),
        margin=None,
        mult=1.0,
    )

    cerebro.broker.set_slippage_perc(
        perc=config.get('slippage', 0.0005),
        slip_open=True,
        slip_limit=True,
        slip_match=True,
        slip_out=True
    )

    # æ·»åŠ  Kelly Sizer
    if config.get('use_kelly_sizer', True):
        cerebro.addsizer(KellySizer,
                       kelly_fraction=config.get('kelly_fraction', 0.25),
                       max_position_pct=config.get('max_position_pct', 0.50))

    # æ·»åŠ åˆ†æå™¨
    cerebro.addanalyzer(bt.analyzers.SharpeRatio, _name='sharpe',
                      timeframe=bt.TimeFrame.Days, compression=1, riskfreerate=0.02)
    cerebro.addanalyzer(bt.analyzers.DrawDown, _name='drawdown')
    cerebro.addanalyzer(bt.analyzers.Returns, _name='returns')
    cerebro.addanalyzer(bt.analyzers.TradeAnalyzer, _name='trades')

    # è¿è¡Œå›æµ‹
    logger.info(f"[çª—å£ {fold_num}] å¼€å§‹å›æµ‹ - æµ‹è¯•é›†: {test_dates[0].date()} è‡³ {test_dates[1].date()}")
    results = cerebro.run()

    # æå–ç»“æœ
    strat = results[0]
    final_value = cerebro.broker.getvalue()

    fold_result = {
        'fold': fold_num,
        'train_start': train_dates[0],
        'train_end': train_dates[1],
        'test_start': test_dates[0],
        'test_end': test_dates[1],
        'final_value': final_value,
        'sharpe': strat.analyzers.sharpe.get_analysis().get('sharperatio', None),
        'max_drawdown': strat.analyzers.drawdown.get_analysis().get('max', {}).get('drawdown', None),
        'total_trades': strat.analyzers.trades.get_analysis().get('total', {}).get('total', 0),
    }

    logger.info(f"[çª—å£ {fold_num}] å®Œæˆ - æ”¶ç›Šç‡={(final_value/initial_cash-1)*100:.2f}%")

    return fold_result


class BacktestRunner:
    """å›æµ‹è¿è¡Œå™¨"""

    def __init__(self, config: dict):
        self.config = config
        self.results = {}

    def load_data(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
        """
        åŠ è½½å›æµ‹æ•°æ®

        æ•°æ®æ¥æºï¼š
            1. å¦‚æœå­˜åœ¨é¢„æµ‹æ–‡ä»¶ï¼ˆpredictions.parquetï¼‰ï¼Œç›´æ¥åŠ è½½
            2. å¦åˆ™åŠ è½½ç‰¹å¾æ–‡ä»¶ï¼Œä½¿ç”¨æ¨¡æ‹Ÿé¢„æµ‹

        Returns:
            DataFrame: åŒ…å« OHLCV å’Œé¢„æµ‹æ¦‚ç‡çš„æ•°æ®
        """
        logger.info(f"åŠ è½½æ•°æ® - å“ç§: {symbol}, æ—¥æœŸ: {start_date} è‡³ {end_date}")

        # å°è¯•åŠ è½½é¢„æµ‹ç»“æœ
        predictions_path = project_root / f"ml_models/{symbol}/predictions.parquet"

        if predictions_path.exists():
            logger.info(f"åŠ è½½é¢„æµ‹æ•°æ®: {predictions_path}")
            df = pd.read_parquet(predictions_path)
        else:
            logger.warning(f"æœªæ‰¾åˆ°é¢„æµ‹æ–‡ä»¶ {predictions_path}ï¼Œå°†ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®")
            df = self._generate_sample_data(symbol, start_date, end_date)

        # ç¡®ä¿æœ‰å¿…è¦çš„åˆ—
        required_cols = ['open', 'high', 'low', 'close', 'volume']
        for col in required_cols:
            if col not in df.columns:
                raise ValueError(f"ç¼ºå°‘å¿…è¦åˆ—: {col}")

        # ç¡®ä¿æœ‰é¢„æµ‹æ¦‚ç‡åˆ—
        if 'y_pred_proba_long' not in df.columns:
            logger.warning("ç¼ºå°‘ y_pred_proba_longï¼Œä½¿ç”¨éšæœºæ¦‚ç‡")
            df['y_pred_proba_long'] = np.random.uniform(0.4, 0.7, len(df))

        if 'y_pred_proba_short' not in df.columns:
            logger.warning("ç¼ºå°‘ y_pred_proba_shortï¼Œä½¿ç”¨éšæœºæ¦‚ç‡")
            df['y_pred_proba_short'] = np.random.uniform(0.4, 0.7, len(df))

        # æ·»åŠ æ³¢åŠ¨ç‡åˆ—ï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
        if 'volatility' not in df.columns:
            df['volatility'] = df['close'].pct_change().rolling(20).std()

        # æ—¶é—´è¿‡æ»¤
        if 'datetime' in df.columns:
            df['datetime'] = pd.to_datetime(df['datetime'])
            df = df.set_index('datetime')

        df = df.sort_index()
        df = df.loc[start_date:end_date]

        logger.info(f"æ•°æ®åŠ è½½å®Œæˆ - æ€»è¡Œæ•°: {len(df)}, åˆ—: {df.columns.tolist()}")

        return df

    def _generate_sample_data(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
        """
        ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼ˆç”¨äºæ¼”ç¤ºï¼‰

        Returns:
            DataFrame: æ¨¡æ‹Ÿçš„ OHLCV å’Œé¢„æµ‹æ•°æ®
        """
        logger.warning("âš ï¸  ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œå›æµ‹ï¼")

        dates = pd.date_range(start=start_date, end=end_date, freq='H')
        n = len(dates)

        # ç”Ÿæˆéšæœºæ¸¸èµ°ä»·æ ¼
        np.random.seed(42)
        returns = np.random.normal(0.0001, 0.005, n)
        price = 1.1000 * np.exp(np.cumsum(returns))

        df = pd.DataFrame({
            'datetime': dates,
            'open': price,
            'high': price * (1 + np.random.uniform(0, 0.001, n)),
            'low': price * (1 - np.random.uniform(0, 0.001, n)),
            'close': price,
            'volume': np.random.randint(100, 1000, n),
            'y_pred_proba_long': np.random.uniform(0.4, 0.7, n),
            'y_pred_proba_short': np.random.uniform(0.4, 0.7, n),
            'volatility': np.random.uniform(0.005, 0.02, n)
        })

        df.set_index('datetime', inplace=True)

        return df

    def run_backtest(self, df: pd.DataFrame, strategy_class=MLStrategy,
                    strategy_params: dict = None) -> bt.Cerebro:
        """
        æ‰§è¡Œå•æ¬¡å›æµ‹

        Args:
            df: å›æµ‹æ•°æ®
            strategy_class: ç­–ç•¥ç±»
            strategy_params: ç­–ç•¥å‚æ•°

        Returns:
            bt.Cerebro: Backtrader å›æµ‹å¼•æ“
        """
        cerebro = bt.Cerebro()

        # æ·»åŠ ç­–ç•¥
        if strategy_params is None:
            strategy_params = {}
        cerebro.addstrategy(strategy_class, **strategy_params)

        # æ·»åŠ æ•°æ®
        data = MLDataFeed(dataname=df)
        cerebro.adddata(data)

        # è®¾ç½®åˆå§‹èµ„é‡‘
        initial_cash = self.config.get('initial_cash', 100000.0)
        cerebro.broker.setcash(initial_cash)

        # è®¾ç½®äº¤æ˜“æˆæœ¬
        # ç‚¹å·®ï¼š2 pips = 0.0002
        spread_cost = self.config.get('spread', 0.0002)
        # æ‰‹ç»­è´¹ï¼šä¸‡åˆ†ä¹‹äºŒ
        commission_pct = self.config.get('commission', 0.0002)

        cerebro.broker.setcommission(
            commission=commission_pct,
            margin=None,
            mult=1.0,
            # æ»‘ç‚¹é€šè¿‡ slip_perc æ¨¡æ‹Ÿ
        )

        # æ·»åŠ æ»‘ç‚¹ï¼ˆ0.05%ï¼‰
        cerebro.broker.set_slippage_perc(
            perc=self.config.get('slippage', 0.0005),
            slip_open=True,
            slip_limit=True,
            slip_match=True,
            slip_out=True
        )

        # æ·»åŠ  Kelly Sizer
        if self.config.get('use_kelly_sizer', True):
            cerebro.addsizer(KellySizer,
                           kelly_fraction=self.config.get('kelly_fraction', 0.25),
                           max_position_pct=self.config.get('max_position_pct', 0.20))

        # æ·»åŠ åˆ†æå™¨
        cerebro.addanalyzer(bt.analyzers.SharpeRatio, _name='sharpe',
                          timeframe=bt.TimeFrame.Days, compression=1, riskfreerate=0.02)
        cerebro.addanalyzer(bt.analyzers.DrawDown, _name='drawdown')
        cerebro.addanalyzer(bt.analyzers.Returns, _name='returns')
        cerebro.addanalyzer(bt.analyzers.TradeAnalyzer, _name='trades')

        # è¿è¡Œå›æµ‹
        logger.info(f"å¼€å§‹å›æµ‹ - åˆå§‹èµ„é‡‘: ${initial_cash:,.2f}")
        results = cerebro.run()

        # è·å–ç»“æœ
        strat = results[0]
        final_value = cerebro.broker.getvalue()
        total_return = (final_value - initial_cash) / initial_cash * 100

        logger.info(f"å›æµ‹å®Œæˆ - æœ€ç»ˆèµ„é‡‘: ${final_value:,.2f}, æ”¶ç›Šç‡: {total_return:.2f}%")

        return cerebro, strat

    def run_walkforward(self, df: pd.DataFrame, train_months: int = 6, test_months: int = 2,
                       parallel: bool = True, max_workers: int = None):
        """
        æ‰§è¡Œ Walk-Forward å›æµ‹ï¼ˆæ”¯æŒå¹¶è¡Œï¼‰

        Args:
            df: å®Œæ•´æ•°æ®é›†
            train_months: è®­ç»ƒé›†æœˆæ•°
            test_months: æµ‹è¯•é›†æœˆæ•°
            parallel: æ˜¯å¦å¹¶è¡Œæ‰§è¡Œï¼ˆé»˜è®¤ Trueï¼‰
            max_workers: æœ€å¤§å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆé»˜è®¤ä¸º CPU æ ¸å¿ƒæ•°ï¼‰

        Returns:
            list: å„çª—å£çš„å›æµ‹ç»“æœ
        """
        logger.info(f"å¼€å§‹ Walk-Forward å›æµ‹ - è®­ç»ƒ: {train_months}æœˆ, æµ‹è¯•: {test_months}æœˆ")

        total_data_months = (df.index[-1] - df.index[0]).days // 30

        # è®¡ç®—åˆ†å‰²ç‚¹
        n_folds = max(1, (total_data_months - train_months) // test_months)

        logger.info(f"æ€»æ•°æ®: {total_data_months}æœˆ, åˆ†å‰²: {n_folds} ä¸ªçª—å£")

        # å‡†å¤‡æ‰€æœ‰çª—å£çš„å‚æ•°
        fold_params = []
        for i in range(n_folds):
            # è®¡ç®—çª—å£æ—¥æœŸ
            train_start = df.index[0] + timedelta(days=i * test_months * 30)
            train_end = train_start + timedelta(days=train_months * 30)
            test_start = train_end
            test_end = test_start + timedelta(days=test_months * 30)

            if test_end > df.index[-1]:
                break

            # åˆ†å‰²æ•°æ®
            test_df = df.loc[test_start:test_end]

            fold_params.append({
                'fold_num': i + 1,
                'test_df': test_df,
                'train_dates': (train_start, train_end),
                'test_dates': (test_start, test_end),
                'config': self.config.copy()
            })

        logger.info(f"å°†æ‰§è¡Œ {len(fold_params)} ä¸ªçª—å£çš„å›æµ‹")

        # ============================================================
        # å¹¶è¡Œæ‰§è¡Œæˆ–ä¸²è¡Œæ‰§è¡Œ
        # ============================================================
        results = []

        if parallel and len(fold_params) > 1:
            import time
            start_time = time.time()

            logger.info(f"ğŸš€ å¯åŠ¨å¹¶è¡Œå›æµ‹ - æœ€å¤§è¿›ç¨‹æ•°: {max_workers or os.cpu_count()}")

            with ProcessPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                futures = {
                    executor.submit(
                        run_single_fold,
                        params['fold_num'],
                        params['test_df'],
                        params['train_dates'],
                        params['test_dates'],
                        params['config']
                    ): params['fold_num']
                    for params in fold_params
                }

                # æ”¶é›†ç»“æœ
                for future in as_completed(futures):
                    fold_num = futures[future]
                    try:
                        result = future.result()
                        results.append(result)
                        logger.info(f"âœ… çª—å£ {fold_num} å®Œæˆ")
                    except Exception as e:
                        logger.error(f"âŒ çª—å£ {fold_num} å¤±è´¥: {e}")

            elapsed = time.time() - start_time
            logger.info(f"â±ï¸  å¹¶è¡Œå›æµ‹å®Œæˆ - è€—æ—¶: {elapsed:.2f}s")

        else:
            # ä¸²è¡Œæ‰§è¡Œï¼ˆå•çº¿ç¨‹ï¼‰
            import time
            start_time = time.time()

            logger.info("ğŸ”„ å¯åŠ¨ä¸²è¡Œå›æµ‹ï¼ˆå•çº¿ç¨‹ï¼‰")

            for params in fold_params:
                try:
                    result = run_single_fold(
                        params['fold_num'],
                        params['test_df'],
                        params['train_dates'],
                        params['test_dates'],
                        params['config']
                    )
                    results.append(result)
                except Exception as e:
                    logger.error(f"âŒ çª—å£ {params['fold_num']} å¤±è´¥: {e}")

            elapsed = time.time() - start_time
            logger.info(f"â±ï¸  ä¸²è¡Œå›æµ‹å®Œæˆ - è€—æ—¶: {elapsed:.2f}s")

        # æŒ‰ fold ç¼–å·æ’åº
        results.sort(key=lambda x: x['fold'])

        # æ±‡æ€»ç»“æœ
        logger.info("\n" + "="*50)
        logger.info("Walk-Forward å›æµ‹æ±‡æ€»")
        logger.info("="*50)

        for r in results:
            sharpe_str = f"{r['sharpe']:.2f}" if r['sharpe'] else "N/A"
            dd_str = f"{r['max_drawdown']:.2f}" if r['max_drawdown'] else "N/A"
            logger.info(
                f"çª—å£ {r['fold']}: "
                f"æ”¶ç›Šç‡={(r['final_value']/self.config['initial_cash']-1)*100:.2f}%, "
                f"Sharpe={sharpe_str}, "
                f"å›æ’¤={dd_str}%, "
                f"äº¤æ˜“æ¬¡æ•°={r.get('total_trades', 'N/A')}"
            )

        logger.info("="*50)

        return results

    def compare_with_benchmark(self, df: pd.DataFrame):
        """
        å¯¹æ¯”ä¹°å…¥æŒæœ‰ç­–ç•¥

        Args:
            df: å›æµ‹æ•°æ®
        """
        logger.info("\n" + "="*50)
        logger.info("åŸºå‡†å¯¹æ¯” - ä¹°å…¥æŒæœ‰ vs ML ç­–ç•¥")
        logger.info("="*50)

        # è¿è¡Œ ML ç­–ç•¥
        logger.info("\n[1] ML ç­–ç•¥")
        cerebro_ml, strat_ml = self.run_backtest(df, strategy_class=MLStrategy)
        ml_final = cerebro_ml.broker.getvalue()
        ml_sharpe = strat_ml.analyzers.sharpe.get_analysis().get('sharperatio', None)

        # è¿è¡Œä¹°å…¥æŒæœ‰ç­–ç•¥
        logger.info("\n[2] ä¹°å…¥æŒæœ‰ç­–ç•¥")
        cerebro_bh, strat_bh = self.run_backtest(df, strategy_class=BuyAndHoldStrategy)
        bh_final = cerebro_bh.broker.getvalue()
        bh_sharpe = strat_bh.analyzers.sharpe.get_analysis().get('sharperatio', None)

        # å¯¹æ¯”
        initial = self.config['initial_cash']
        ml_return = (ml_final - initial) / initial * 100
        bh_return = (bh_final - initial) / initial * 100

        logger.info("\n" + "="*50)
        ml_sharpe_str = f"{ml_sharpe:.2f}" if ml_sharpe else "N/A"
        bh_sharpe_str = f"{bh_sharpe:.2f}" if bh_sharpe else "N/A"
        logger.info(f"ML ç­–ç•¥æ”¶ç›Šç‡: {ml_return:.2f}%, Sharpe: {ml_sharpe_str}")
        logger.info(f"ä¹°å…¥æŒæœ‰æ”¶ç›Šç‡: {bh_return:.2f}%, Sharpe: {bh_sharpe_str}")
        logger.info(f"è¶…é¢æ”¶ç›Š: {ml_return - bh_return:.2f}%")
        logger.info("="*50)

        return {
            'ml_return': ml_return,
            'bh_return': bh_return,
            'ml_sharpe': ml_sharpe,
            'bh_sharpe': bh_sharpe,
            'excess_return': ml_return - bh_return
        }


def main():
    parser = argparse.ArgumentParser(description='MT5-CRS ç­–ç•¥å›æµ‹å¼•æ“')

    parser.add_argument('--symbol', type=str, default='EURUSD', help='äº¤æ˜“å“ç§')
    parser.add_argument('--start-date', type=str, default='2023-01-01', help='å›æµ‹å¼€å§‹æ—¥æœŸ')
    parser.add_argument('--end-date', type=str, default='2024-12-31', help='å›æµ‹ç»“æŸæ—¥æœŸ')
    parser.add_argument('--initial-cash', type=float, default=100000.0, help='åˆå§‹èµ„é‡‘')
    parser.add_argument('--walk-forward', action='store_true', help='å¯ç”¨ Walk-Forward éªŒè¯')
    parser.add_argument('--benchmark', action='store_true', help='å¯¹æ¯”ä¹°å…¥æŒæœ‰ç­–ç•¥')
    parser.add_argument('--commission', type=float, default=0.0002, help='æ‰‹ç»­è´¹ç‡')
    parser.add_argument('--spread', type=float, default=0.0002, help='ç‚¹å·® (pips)')
    parser.add_argument('--slippage', type=float, default=0.0005, help='æ»‘ç‚¹æ¯”ä¾‹')
    parser.add_argument('--parallel', action='store_true', default=True, help='å¯ç”¨å¹¶è¡Œå›æµ‹ï¼ˆé»˜è®¤å¼€å¯ï¼‰')
    parser.add_argument('--no-parallel', dest='parallel', action='store_false', help='ç¦ç”¨å¹¶è¡Œå›æµ‹')
    parser.add_argument('--max-workers', type=int, default=None, help='æœ€å¤§å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆé»˜è®¤ä¸º CPU æ ¸å¿ƒæ•°ï¼‰')

    args = parser.parse_args()

    # é…ç½®
    config = {
        'initial_cash': args.initial_cash,
        'commission': args.commission,
        'spread': args.spread,
        'slippage': args.slippage,
        'use_kelly_sizer': True,
        'kelly_fraction': 0.25,
        'max_position_pct': 0.50,  # æ›´æ–°ä¸º 50%ï¼ˆKelly ä¿®æ­£åçš„æ–°é»˜è®¤å€¼ï¼‰
    }

    runner = BacktestRunner(config)

    # åŠ è½½æ•°æ®
    df = runner.load_data(args.symbol, args.start_date, args.end_date)

    # æ‰§è¡Œå›æµ‹
    if args.walk_forward:
        runner.run_walkforward(df, parallel=args.parallel, max_workers=args.max_workers)
    elif args.benchmark:
        runner.compare_with_benchmark(df)
    else:
        cerebro, strat = runner.run_backtest(df)

        # ç»˜å›¾ï¼ˆå¯é€‰ï¼‰
        # cerebro.plot(style='candlestick')

    logger.info("\nâœ… å›æµ‹å®Œæˆï¼")


if __name__ == '__main__':
    main()

```


---

**å¯¼å‡ºç»Ÿè®¡**: 7/7 ä¸ªæ–‡ä»¶
