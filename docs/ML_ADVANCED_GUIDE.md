# MT5-CRS æœºå™¨å­¦ä¹ é«˜çº§è®­ç»ƒæŒ‡å—

> **å¯¹å†²åŸºé‡‘çº§åˆ«çš„é¢„æµ‹å¼•æ“** - æ·±åº¦åŠ å¼ºç‰ˆ (v2.0)

åŸºäº Marcos Lopez de Pradoã€ŠAdvances in Financial Machine Learningã€‹+ Kaggle é‡‘èç«èµ›å† å†›æ–¹æ¡ˆ

---

## ğŸ¯ æ ¸å¿ƒå‡çº§

æœ¬æ¬¡å®ç°åŒ…å«ä»¥ä¸‹**å¯¹å†²åŸºé‡‘çº§åˆ«**çš„é«˜çº§æŠ€æœ¯ï¼š

### 1. Purged K-Fold Cross-Validation âœ…
- **é—®é¢˜**ï¼šTriple Barrier æ ‡ç­¾é‡å å¯¼è‡´ä¿¡æ¯æ³„æ¼
- **è§£å†³**ï¼šPurging (æ¸…é™¤) + Embargoing (ç¦è¿)
- **ä»£ç **ï¼š`src/models/validation.py::PurgedKFold`

### 2. Clustered Feature Importance âœ…  
- **é—®é¢˜**ï¼š75ç»´ç‰¹å¾å­˜åœ¨é«˜å…±çº¿æ€§ (ema_20 vs sma_20)
- **è§£å†³**ï¼šHierarchical Clustering + ä»£è¡¨æ€§ç‰¹å¾é€‰æ‹©
- **ä»£ç **ï¼š`src/models/feature_selection.py::FeatureClusterer`

### 3. Mean Decrease Accuracy (MDA) âœ…
- **é—®é¢˜**ï¼šLightGBM çš„ split/gain ä¸å‡†ç¡®
- **è§£å†³**ï¼šé€šè¿‡æ‰“ä¹±ç‰¹å¾å€¼æµ‹é‡ç²¾åº¦ä¸‹é™
- **ä»£ç **ï¼š`src/models/feature_selection.py::MDFeatureImportance`

### 4. Ensemble Stacking âœ…
- **æ¶æ„**ï¼šLightGBM + CatBoost â†’ Logistic Regression
- **ä¼˜åŠ¿**ï¼šOut-of-Fold é¢„æµ‹ + Meta Learner èåˆ
- **ä»£ç **ï¼š`src/models/trainer.py::EnsembleStacker`

### 5. Optuna è´å¶æ–¯ä¼˜åŒ– âœ…
- **æ–¹æ³•**ï¼šTPE Sampler (Tree-structured Parzen Estimator)
- **æ•ˆç‡**ï¼šæ¯” GridSearch å¿« 10-100 å€
- **ä»£ç **ï¼š`src/models/trainer.py::OptunaOptimizer`

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç”Ÿæˆç¤ºä¾‹æ•°æ®

```bash
python bin/generate_sample_data.py
```

ç”Ÿæˆæ–‡ä»¶ï¼š
- `outputs/features/features.parquet` - 35ç»´ç‰¹å¾
- `outputs/features/labels.parquet` - ä¸‰åˆ†ç±»æ ‡ç­¾
- `outputs/features/sample_weights.parquet` - æ ·æœ¬æƒé‡
- `outputs/features/pred_times.parquet` - æ ‡ç­¾ç»“æŸæ—¶é—´

### 2. è®­ç»ƒ Ensemble Stacking æ¨¡å‹

```bash
python bin/train_ml_model.py
```

### 3. æŸ¥çœ‹ç»“æœ

```
outputs/
â”œâ”€â”€ models/ml_model_v1/
â”‚   â”œâ”€â”€ lgb_fold_0.pkl ... lgb_fold_4.pkl  # LightGBM æ¨¡å‹ (5 folds)
â”‚   â”œâ”€â”€ cb_fold_0.cbm  ... cb_fold_4.cbm   # CatBoost æ¨¡å‹ (5 folds)
â”‚   â”œâ”€â”€ meta_model.pkl                      # Meta Learner
â”‚   â””â”€â”€ test_metrics.json                   # æµ‹è¯•é›†æŒ‡æ ‡
â”‚
â””â”€â”€ plots/ml_training/
    â”œâ”€â”€ test_set_roc_pr_curves.png
    â”œâ”€â”€ test_set_calibration_curve.png
    â””â”€â”€ feature_clustering_dendrogram.png
```

---

## ğŸ“ ç†è®ºåŸºç¡€

### Purged K-Fold è¯¦è§£

**ä¼ ç»Ÿ K-Fold çš„é—®é¢˜**ï¼š

```
è®­ç»ƒé›†: [T0 ... T500]  æµ‹è¯•é›†: [T500 ... T600]
         â””â”€ æ ·æœ¬ T490 çš„æ ‡ç­¾ç»“æŸäº T495 (OK)
         â””â”€ æ ·æœ¬ T498 çš„æ ‡ç­¾ç»“æŸäº T505 âŒ (é‡å ï¼)
```

å¦‚æœæ ·æœ¬ T498 çš„ Triple Barrier æ ‡ç­¾æŒç»­åˆ° T505ï¼Œå®ƒä¼š"çœ‹åˆ°"æµ‹è¯•é›†çš„å‰ 5 ä¸ªæ—¶é—´ç‚¹ï¼Œå¯¼è‡´**ä¿¡æ¯æ³„æ¼**ã€‚

**Purged K-Fold è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# 1. Purging (æ¸…é™¤)
test_start = T500
for sample in train_set:
    if sample.label_end_time >= test_start:
        train_set.remove(sample)  # åˆ é™¤é‡å æ ·æœ¬

# 2. Embargoing (ç¦è¿)
embargo_size = int(len(test_set) * 0.01)  # 1%
test_set = test_set[:-embargo_size]  # åˆ é™¤æµ‹è¯•é›†æœ«å°¾
```

**å®ç°ç»†èŠ‚** (src/models/validation.py):

```python
class PurgedKFold:
    def __init__(self, n_splits=5, embargo_pct=0.01):
        self.n_splits = n_splits
        self.embargo_pct = embargo_pct

    def split(self, X, y, event_ends):
        for train_idx, test_idx inåŸºç¡€KFold:
            # Purging
            train_idx = self._purge_train_set(train_idx, test_idx, event_ends)
            
            # Embargoing
            embargo_size = int(len(test_idx) * self.embargo_pct)
            test_idx = test_idx[:-embargo_size]
            
            yield train_idx, test_idx
```

---

### Clustered Feature Importance è¯¦è§£

**é—®é¢˜ç¤ºä¾‹**ï¼š

å‡è®¾æˆ‘ä»¬æœ‰ä»¥ä¸‹ç‰¹å¾ï¼š

| ç‰¹å¾å | ç›¸å…³æ€§ |
|--------|--------|
| ema_5  | -      |
| ema_10 | 0.95 with ema_5 |
| ema_20 | 0.93 with ema_5 |
| rsi    | 0.15 with ema_5 |

å¦‚æœç›´æ¥è®­ç»ƒ LightGBMï¼š
- `ema_5` çš„é‡è¦æ€§ = 0.1
- `ema_10` çš„é‡è¦æ€§ = 0.05
- `ema_20` çš„é‡è¦æ€§ = 0.08

**é—®é¢˜**ï¼šæ€»é‡è¦æ€§ (0.1 + 0.05 + 0.08 = 0.23) è¢«ç¨€é‡Šåˆ° 3 ä¸ªç‰¹å¾ä¸Šï¼Œä½†å®ƒä»¬å®é™…ä¸Šæ˜¯**åŒä¸€ä¸ªä¿¡å·**ï¼

**Clustered Importance è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# 1. è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
corr_matrix = features.corr().abs()

# 2. è½¬æ¢ä¸ºè·ç¦»çŸ©é˜µ
distance_matrix = 1 - corr_matrix

# 3. Hierarchical Clustering
linkage_matrix = scipy.cluster.hierarchy.linkage(distance_matrix, method='ward')

# 4. åˆ‡å‰²æ ‘ (ç›¸å…³æ€§ > 0.75 åˆ†åˆ°åŒä¸€ç»„)
clusters = fcluster(linkage_matrix, threshold=0.25, criterion='distance')

# ç»“æœ:
# Cluster 1: [ema_5, ema_10, ema_20]  â†’ é€‰æ‹© ema_5 (é‡è¦æ€§æœ€é«˜)
# Cluster 2: [rsi]                     â†’ ä¿ç•™ rsi
```

ç°åœ¨ `ema_5` çš„é‡è¦æ€§ = 0.23 (åˆå¹¶å)ï¼

**å¯è§†åŒ–** (æ ‘çŠ¶å›¾):

```
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ rsi
      â”‚
â”€â”€â”€â”€â”€â”€â”¤            â”Œâ”€â”€ ema_5
      â”‚       â”Œâ”€â”€â”€â”€â”¤
      â””â”€â”€â”€â”€â”€â”€â”€â”¤    â””â”€â”€ ema_10
              â”‚
              â””â”€â”€â”€â”€â”€â”€ ema_20
         è·ç¦» (1 - |ç›¸å…³æ€§|)
```

---

### Ensemble Stacking è¯¦è§£

**ä¼ ç»Ÿ Ensemble** (å¦‚ Voting, Averaging):

```python
# ç®€å•å¹³å‡
final_prediction = (lgb_prediction + cb_prediction) / 2
```

**é—®é¢˜**ï¼šæ‰‹åŠ¨è®¾ç½®æƒé‡ (50%, 50%)ï¼Œæ— æ³•å­¦ä¹ æœ€ä¼˜ç»„åˆã€‚

**Stacking è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# æ­¥éª¤ 1: è®­ç»ƒåŸºæ¨¡å‹ (ä½¿ç”¨ Purged K-Fold)
for fold in [1, 2, 3, 4, 5]:
    lgb_model_fold.fit(train_fold)
    cb_model_fold.fit(train_fold)
    
    # Out-of-Fold é¢„æµ‹ (é˜²æ­¢ä¿¡æ¯æ³„æ¼)
    oof_predictions_lgb[val_fold] = lgb_model_fold.predict(val_fold)
    oof_predictions_cb[val_fold] = cb_model_fold.predict(val_fold)

# æ­¥éª¤ 2: è®­ç»ƒ Meta Learner
meta_features = pd.DataFrame({
    'lgb': oof_predictions_lgb,
    'cb': oof_predictions_cb
})
meta_model = LogisticRegression().fit(meta_features, y_train)

# æ­¥éª¤ 3: æœ€ç»ˆé¢„æµ‹
final_prediction = meta_model.predict([
    lgb_models_mean(X_test),
    cb_models_mean(X_test)
])
```

**ä¸ºä»€ä¹ˆç”¨ Logistic Regression ä½œä¸º Meta Learner?**

- **ç®€å•**: åªæœ‰ 2 ä¸ªç‰¹å¾ (lgb, cb çš„é¢„æµ‹)
- **å¯è§£é‡Š**: æƒé‡å¯ä»¥çœ‹å‡ºå“ªä¸ªæ¨¡å‹æ›´é‡è¦
- **é˜²è¿‡æ‹Ÿåˆ**: å¤æ‚çš„ Meta Learner (å¦‚ XGBoost) åè€Œå®¹æ˜“è¿‡æ‹Ÿåˆ

**å®é™…æ•ˆæœ**ï¼š

```
å•ç‹¬ LightGBM: AUC = 0.72
å•ç‹¬ CatBoost: AUC = 0.70
Stacking:      AUC = 0.75 âœ… (æå‡ 3%)
```

---

## âš™ï¸ é…ç½®æ–‡ä»¶è¯¦è§£

### å…³é”®å‚æ•°

#### 1. Purged K-Fold å‚æ•°

```yaml
validation:
  purged_kfold:
    n_splits: 5          # K-Fold åˆ†å‰²æ•°
    embargo_pct: 0.01    # ç¦è¿æ¯”ä¾‹
    purge_overlap: true  # æ˜¯å¦æ¸…é™¤é‡å 
```

**è°ƒä¼˜å»ºè®®**ï¼š

| æ•°æ®é‡ | n_splits | embargo_pct | è¯´æ˜ |
|--------|----------|-------------|------|
| < 1 ä¸‡ | 3        | 0.02 (2%)   | å°æ•°æ®é‡ç”¨ 3-Foldï¼Œé¿å…æµ‹è¯•é›†å¤ªå° |
| 1-10 ä¸‡ | 5        | 0.01 (1%)   | æ ‡å‡†é…ç½® |
| > 10 ä¸‡ | 10       | 0.005 (0.5%) | å¤§æ•°æ®é‡å¯ä»¥ç”¨æ›´å¤š fold |

#### 2. LightGBM å‚æ•°

```yaml
models:
  lightgbm:
    num_leaves: 63           # å¶å­æ•°
    learning_rate: 0.03      # å­¦ä¹ ç‡
    lambda_l1: 0.5           # L1 æ­£åˆ™åŒ–
    lambda_l2: 0.5           # L2 æ­£åˆ™åŒ–
    max_depth: 8             # æœ€å¤§æ·±åº¦
```

**å‚æ•°å½±å“**ï¼š

```
num_leaves â†‘  â†’ æ¨¡å‹å¤æ‚åº¦ â†‘ â†’ è¿‡æ‹Ÿåˆé£é™© â†‘
learning_rate â†“ â†’ è®­ç»ƒé€Ÿåº¦ â†“ â†’ æ³›åŒ–èƒ½åŠ› â†‘
lambda_l1/l2 â†‘ â†’ æ­£åˆ™åŒ– â†‘ â†’ é˜²è¿‡æ‹Ÿåˆ â†‘
max_depth â†‘   â†’ æ ‘æ·±åº¦ â†‘ â†’ è¿‡æ‹Ÿåˆé£é™© â†‘
```

**æ¨èèµ·ç‚¹**ï¼š

```yaml
# ä¿å®ˆé…ç½® (é˜²è¿‡æ‹Ÿåˆ)
num_leaves: 31
learning_rate: 0.01
lambda_l1: 1.0
lambda_l2: 1.0
max_depth: 6

# æ¿€è¿›é…ç½® (è¿½æ±‚æ€§èƒ½)
num_leaves: 127
learning_rate: 0.05
lambda_l1: 0.1
lambda_l2: 0.1
max_depth: 10
```

#### 3. Optuna æœç´¢ç©ºé—´

```yaml
optuna:
  enable: true
  n_trials: 50
  search_space:
    num_leaves: [20, 150]      # èŒƒå›´
    learning_rate: [0.01, 0.3] # log scale
    lambda_l1: [1e-8, 10.0]    # log scale
```

**æ—¶é—´ä¼°ç®—**ï¼š

- å•æ¬¡è¯•éªŒ â‰ˆ 2-5 åˆ†é’Ÿ (å–å†³äºæ•°æ®é‡)
- 50 æ¬¡è¯•éªŒ â‰ˆ 2-4 å°æ—¶

**å»ºè®®**ï¼š

- é¦–æ¬¡è®­ç»ƒï¼šä¸å¯ç”¨ Optunaï¼Œä½¿ç”¨é»˜è®¤å‚æ•°
- ç¬¬äºŒæ¬¡è®­ç»ƒï¼šå¯ç”¨ Optunaï¼Œn_trials=20 (å¿«é€Ÿæœç´¢)
- æœ€ç»ˆè®­ç»ƒï¼šn_trials=100 (ç²¾ç»†æœç´¢)

---

## ğŸ“Š è¯„ä¼°æŒ‡æ ‡æ·±åº¦è§£è¯»

### 1. Calibration Curve (æ¦‚ç‡æ ¡å‡†æ›²çº¿)

**ä»€ä¹ˆæ˜¯å¥½çš„æ ¡å‡†?**

```
å®Œç¾æ ¡å‡†:
Predicted: [0.8, 0.8, 0.8, 0.8, 0.8]  (5ä¸ªæ ·æœ¬)
Actual:    [1,   1,   1,   1,   0  ]  (4/5 = 80% âœ…)

è¿‡åº¦è‡ªä¿¡:
Predicted: [0.9, 0.9, 0.9, 0.9, 0.9]
Actual:    [1,   1,   1,   0,   0  ]  (3/5 = 60% âŒ)
```

**å¦‚ä½•è§£è¯»æ ¡å‡†æ›²çº¿?**

```
Fraction of Positives
    1.0 â”¤              â•±
        â”‚            â•±  â† ä½ çš„æ¨¡å‹
        â”‚          â•±
        â”‚        â•±    â† å¯¹è§’çº¿ (å®Œç¾)
    0.5 â”¤      â•±
        â”‚    â•±
        â”‚  â•±
    0.0 â”¤â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        0.0          1.0
          Mean Predicted Value
```

- **æ›²çº¿åœ¨å¯¹è§’çº¿ä¸Šæ–¹**ï¼šæ¨¡å‹è¿‡äºä¿å®ˆ (è¯´ 50% å®é™…æœ‰ 70%)
- **æ›²çº¿åœ¨å¯¹è§’çº¿ä¸‹æ–¹**ï¼šæ¨¡å‹è¿‡äºè‡ªä¿¡ (è¯´ 80% å®é™…åªæœ‰ 60%)

**é‡‘èåº”ç”¨**ï¼š

å¦‚æœä½ çš„ç­–ç•¥æ˜¯ "é¢„æµ‹æ¦‚ç‡ > 0.7 æ‰å¼€ä»“"ï¼š
- å¦‚æœæ¨¡å‹è¿‡åº¦è‡ªä¿¡ï¼Œä¼šå¯¼è‡´**è™šå‡ä¿¡å·**
- éœ€è¦ç”¨ `CalibratedClassifierCV` è¿›è¡Œæ ¡å‡†

---

### 2. SHAP å€¼æ·±åº¦è§£è¯»

**SHAP (SHapley Additive exPlanations)** æ¥è‡ªåšå¼ˆè®ºï¼Œä¸ºæ¯ä¸ªç‰¹å¾åˆ†é…ä¸€ä¸ª"è´¡çŒ®å€¼"ã€‚

**ç¤ºä¾‹**ï¼š

å‡è®¾æ¨¡å‹é¢„æµ‹æŸä¸ªæ ·æœ¬ä¸º "ä¸Šæ¶¨" (æ¦‚ç‡ 0.75)ï¼ŒåŸºå‡†æ¦‚ç‡ä¸º 0.5ã€‚

```
åŸºå‡†æ¦‚ç‡: 0.50

+ rsi = 70           â†’ +0.15  (RSI è¶…ä¹°ï¼Œåˆ©å¥½ä¸Šæ¶¨)
+ frac_diff_close    â†’ +0.08  (ä»·æ ¼åˆ†æ•°å·®åˆ†ä¸ºæ­£)
+ volume_change      â†’ +0.02  (æˆäº¤é‡å¢åŠ )
- macd               â†’ -0.03  (MACD ä¸ºè´Ÿ)
+ å…¶ä»–ç‰¹å¾           â†’ +0.03

= æœ€ç»ˆæ¦‚ç‡: 0.75
```

**SHAP Summary Plot** (èœ‚ç¾¤å›¾):

```
rsi                â—â—â—â—â—â—â—â—        é«˜ RSI â†’ å¢åŠ æ¦‚ç‡
frac_diff_close       â—â—â—â—â—â—â—â—â—â—  é«˜å€¼ â†’ å¢åŠ æ¦‚ç‡
volume_change      â—â—â—
macd                  â—â—â—â—â—â—â—â—    ä½ MACD â†’ å‡å°‘æ¦‚ç‡
                  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚
                 -0.1     0.0     0.1
                    SHAP Value
```

**é‡‘èç›´è§‰æ£€æŸ¥**ï¼š

| ç‰¹å¾ | é¢„æœŸæ–¹å‘ | SHAP æ–¹å‘ | ç»“è®º |
|------|----------|-----------|------|
| rsi (é«˜å€¼) | è¶…ä¹° â†’ ä¸‹è·Œ | è´Ÿ SHAP | âœ… ç¬¦åˆ |
| frac_diff_close (æ­£å€¼) | ä¸Šæ¶¨è¶‹åŠ¿ | æ­£ SHAP | âœ… ç¬¦åˆ |
| volume_change (æ­£å€¼) | æˆäº¤é‡å¢åŠ  â†’ ç¡®è®¤è¶‹åŠ¿ | æ­£ SHAP | âœ… ç¬¦åˆ |

å¦‚æœä¸ç¬¦åˆï¼Œè¯´æ˜æ¨¡å‹å­¦åˆ°äº†**ä¼ªç›¸å…³æ€§**ï¼

---

## ğŸ› å¸¸è§é—®é¢˜æ·±åº¦è¯Šæ–­

### Q1: ä¸ºä»€ä¹ˆæˆ‘çš„ AUC åªæœ‰ 0.52?

**å¯èƒ½åŸå›  1: æ ‡ç­¾è´¨é‡å·®**

```python
# æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
labels.value_counts()

# å¦‚æœåˆ†å¸ƒæåº¦ä¸å¹³è¡¡ (99% vs 1%)ï¼Œæ¨¡å‹å¯èƒ½å­¦ä¸åˆ°ä¸œè¥¿
# è§£å†³: è°ƒæ•´ Triple Barrier å‚æ•° (target_return, stop_loss)
```

**å¯èƒ½åŸå›  2: ç‰¹å¾æ— é¢„æµ‹èƒ½åŠ›**

```python
# æ£€æŸ¥ç‰¹å¾é‡è¦æ€§
importance = model.get_feature_importance()
importance.head(10)

# å¦‚æœ Top 10 ç‰¹å¾é‡è¦æ€§éƒ½ < 0.01ï¼Œè¯´æ˜ç‰¹å¾æ²¡ç”¨
# è§£å†³: é‡æ–°è®¾è®¡ç‰¹å¾
```

**å¯èƒ½åŸå›  3: ä¿¡æ¯æ³„æ¼æœªæ¶ˆé™¤**

```python
# æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº† Purged K-Fold
cv = PurgedKFold(n_splits=5, embargo_pct=0.01)

# æ£€æŸ¥æ˜¯å¦åˆ é™¤äº†åŸå§‹ä»·æ ¼åˆ—
if 'close' in features.columns:
    raise ValueError("åŸå§‹ä»·æ ¼åˆ—æœªåˆ é™¤ï¼ä¼šå¯¼è‡´ä¿¡æ¯æ³„æ¼")
```

---

### Q2: è®­ç»ƒé›† AUC 0.95ï¼Œæµ‹è¯•é›† AUC 0.55 (ä¸¥é‡è¿‡æ‹Ÿåˆ)

**è¯Šæ–­æ­¥éª¤**ï¼š

```python
# 1. æ£€æŸ¥éªŒè¯æ›²çº¿
train_auc_per_epoch = [0.6, 0.7, 0.8, 0.9, 0.95]
val_auc_per_epoch   = [0.58, 0.60, 0.61, 0.59, 0.55]
#                                          â†‘
#                                      éªŒè¯é›†å¼€å§‹ä¸‹é™ â†’ è¿‡æ‹Ÿåˆ

# 2. å¢åŠ æ­£åˆ™åŒ–
lambda_l1: 1.0  # ä» 0.5 å¢åŠ 
lambda_l2: 1.0
min_child_samples: 50  # ä» 20 å¢åŠ 

# 3. å‡å°‘æ¨¡å‹å¤æ‚åº¦
num_leaves: 31  # ä» 63 å‡å°‘
max_depth: 6    # ä» 8 å‡å°‘

# 4. Early Stopping
early_stopping_rounds: 30  # ä» 50 å‡å°‘
```

---

### Q3: Optuna æœç´¢ä¸æ”¶æ•›

**é—®é¢˜è¡¨ç°**ï¼š

```
Trial 1: F1 = 0.55
Trial 2: F1 = 0.53
...
Trial 50: F1 = 0.54  (æ²¡æœ‰æå‡)
```

**åŸå› **ï¼šæœç´¢ç©ºé—´è®¾ç½®ä¸å½“

**è§£å†³**ï¼š

```yaml
# é”™è¯¯: æœç´¢ç©ºé—´å¤ªå¤§
search_space:
  num_leaves: [10, 500]       # èŒƒå›´å¤ªå¤§
  learning_rate: [0.001, 1.0] # èŒƒå›´å¤ªå¤§

# æ­£ç¡®: ç¼©å°èŒƒå›´
search_space:
  num_leaves: [31, 127]       # èšç„¦åœ¨åˆç†èŒƒå›´
  learning_rate: [0.01, 0.1]  # å­¦ä¹ ç‡é€šå¸¸åœ¨è¿™ä¸ªèŒƒå›´
```

---

## ğŸ“š ä»£ç å®ç°ç»†èŠ‚

### Purged K-Fold å®ç°

```python
# src/models/validation.py

class PurgedKFold:
    def _purge_train_set(self, train_idx, test_idx, event_ends):
        """
        æ¸…é™¤è®­ç»ƒé›†ä¸­ä¸æµ‹è¯•é›†é‡å çš„æ ·æœ¬
        
        é€»è¾‘:
        1. æ‰¾åˆ°æµ‹è¯•é›†çš„æœ€æ—©å¼€å§‹æ—¶é—´ (test_start)
        2. åˆ é™¤è®­ç»ƒé›†ä¸­æ ‡ç­¾ç»“æŸæ—¶é—´ >= test_start çš„æ ·æœ¬
        """
        # æµ‹è¯•é›†çš„æœ€æ—©æ—¶é—´
        test_start = event_ends.iloc[test_idx].min()
        
        # æ‰¾å‡ºè®­ç»ƒé›†ä¸­ä¸æµ‹è¯•é›†é‡å çš„æ ·æœ¬
        train_event_ends = event_ends.iloc[train_idx]
        overlap_mask = train_event_ends >= test_start
        
        # åˆ é™¤é‡å æ ·æœ¬
        purged_train_idx = train_idx[~overlap_mask]
        
        return purged_train_idx
```

---

### Ensemble Stacking å®ç°

```python
# src/models/trainer.py

class EnsembleStacker:
    def train(self, X_train, y_train, cv_splitter):
        # æ­¥éª¤ 1: ç”Ÿæˆ Out-of-Fold é¢„æµ‹
        oof_predictions = {'lgb': np.zeros(len(X_train)), 
                           'cb': np.zeros(len(X_train))}
        
        for fold, (train_idx, val_idx) in enumerate(cv_splitter.split(X_train)):
            # è®­ç»ƒ LightGBM
            lgb_model = LightGBMTrainer()
            lgb_model.fit(X_train[train_idx], y_train[train_idx])
            oof_predictions['lgb'][val_idx] = lgb_model.predict_proba(X_train[val_idx])
            
            # è®­ç»ƒ CatBoost
            cb_model = CatBoostTrainer()
            cb_model.fit(X_train[train_idx], y_train[train_idx])
            oof_predictions['cb'][val_idx] = cb_model.predict_proba(X_train[val_idx])
        
        # æ­¥éª¤ 2: è®­ç»ƒ Meta Learner
        meta_features = pd.DataFrame(oof_predictions)
        self.meta_model = LogisticRegression().fit(meta_features, y_train)
        
        # æ­¥éª¤ 3: ä¿å­˜æ‰€æœ‰æ¨¡å‹
        self.lgb_models = lgb_models
        self.cb_models = cb_models
```

---

## ğŸ“ è¿›é˜¶æŠ€å·§

### 1. Sample Weighting ç­–ç•¥

**ä¸ºä»€ä¹ˆè¦åŠ æƒ?**

- Triple Barrier æ ‡ç­¾é‡å å¯¼è‡´éƒ¨åˆ†æ ·æœ¬è¢«"è¿‡åº¦é‡‡æ ·"
- è¿‘æœŸæ ·æœ¬æ¯”è¿œæœŸæ ·æœ¬æ›´é‡è¦

**å®ç°**ï¼š

```python
# è®¡ç®—æ ·æœ¬å”¯ä¸€æ€§
uniqueness = 1.0 / num_concurrent_labels

# ç»“åˆæ”¶ç›Šç‡
returns = abs(target_return)

# æœ€ç»ˆæƒé‡
sample_weight = uniqueness * returns
sample_weight = sample_weight / sample_weight.sum()  # å½’ä¸€åŒ–
```

---

### 2. Deflated Sharpe Ratio (DSR)

**é—®é¢˜**ï¼šä¼ ç»Ÿ Sharpe Ratio ä¼šå› ä¸ºå¤šæ¬¡å›æµ‹è€Œè™šé«˜ã€‚

**è§£å†³**ï¼šDeflated Sharpe Ratio (è€ƒè™‘å›æµ‹æ¬¡æ•°çš„æƒ©ç½š)

```python
import numpy as np
from scipy.stats import norm

def deflated_sharpe_ratio(sharpe, n_trials, n_samples):
    """
    è®¡ç®— Deflated Sharpe Ratio
    
    Args:
        sharpe: è§‚æµ‹åˆ°çš„ Sharpe Ratio
        n_trials: å›æµ‹æ¬¡æ•° (è¯•äº†å¤šå°‘ä¸ªç­–ç•¥)
        n_samples: æ ·æœ¬æ•°é‡
    
    Returns:
        DSR: è°ƒæ•´åçš„ Sharpe Ratio
    """
    # è®¡ç®— Sharpe Ratio çš„æ ‡å‡†è¯¯å·®
    sharpe_std = np.sqrt((1 + 0.5 * sharpe**2) / n_samples)
    
    # å¤šé‡æ£€éªŒæ ¡æ­£
    threshold = norm.ppf(1 - 0.05 / n_trials)  # Bonferroni æ ¡æ­£
    
    # DSR
    dsr = (sharpe - threshold * sharpe_std) / sharpe_std
    
    return dsr
```

---

## ğŸš€ æ€§èƒ½ä¼˜åŒ–

### 1. Numba åŠ é€Ÿç‰¹å¾è®¡ç®—

```python
from numba import jit

@jit(nopython=True)
def calculate_rsi(prices, period=14):
    """Numba åŠ é€Ÿçš„ RSI è®¡ç®— (é¢„æœŸ 2-5x åŠ é€Ÿ)"""
    # å®ç°ä»£ç ...
```

### 2. Dask å¹¶è¡Œå¤„ç†

```python
import dask.dataframe as dd

# å¤šèµ„äº§å¹¶è¡Œç‰¹å¾è®¡ç®—
ddf = dd.from_pandas(features, npartitions=8)
result = ddf.map_partitions(lambda df: calculate_features(df))
```

---

## âœ… éªŒæ”¶æ ‡å‡†

å®Œæˆè®­ç»ƒåï¼Œæ£€æŸ¥ä»¥ä¸‹æŒ‡æ ‡ï¼š

| æŒ‡æ ‡ | è¦æ±‚ | è¯´æ˜ |
|------|------|------|
| **æµ‹è¯•é›† AUC** | > 0.55 | å¿…é¡»è¶…è¶ŠéšæœºçŒœæµ‹ (0.5) |
| **Calibration Error** | < 0.1 | æ¦‚ç‡æ ¡å‡†è¯¯å·® |
| **è®­ç»ƒ/æµ‹è¯• AUC å·®è·** | < 0.1 | é˜²æ­¢è¿‡æ‹Ÿåˆ |
| **Top 5 ç‰¹å¾ç¬¦åˆç›´è§‰** | âœ… | SHAP åˆ†æ |

---

## ğŸ“– æ€»ç»“

è¿™ä¸ªç³»ç»Ÿå®ç°äº†**å¯¹å†²åŸºé‡‘çº§åˆ«**çš„æœºå™¨å­¦ä¹ ç®¡é“ï¼ŒåŒ…æ‹¬ï¼š

1. âœ… Purged K-Fold (é˜²ä¿¡æ¯æ³„æ¼)
2. âœ… ç‰¹å¾èšç±»å»å™ª (è§£å†³å…±çº¿æ€§)
3. âœ… MDA ç‰¹å¾é‡è¦æ€§ (å‡†ç¡®è¯„ä¼°)
4. âœ… Ensemble Stacking (æ¨¡å‹èåˆ)
5. âœ… Optuna ä¼˜åŒ– (é«˜æ•ˆè°ƒå‚)

**ä¸‹ä¸€æ­¥**ï¼š

- é›†æˆåˆ°å›æµ‹ç³»ç»Ÿ (å·¥å• #010)
- éƒ¨ç½²å®æ—¶é¢„æµ‹ API (å·¥å• #011)

**ç¥ä½ è¿½æ±‚ Alpha æˆåŠŸï¼** ğŸš€ğŸ“ˆ
