# Task #034: EODHD Async Ingestion Engine Specification

**Phase**: 2 (Data Intelligence - æ•°æ®æ™ºèƒ½)
**Protocol**: v2.6 (CLI --plan Integration)
**Status**: Ready for Implementation
**Dependencies**:
- Task #033: Database Schema âœ…
- Task #032: Data Nexus Infrastructure âœ…

---

## ğŸ¯ ç›®æ ‡

æ„å»ºç”Ÿäº§çº§å¼‚æ­¥ ETL ç®¡é“ï¼Œå®ç°ä¸¤å¤§åŠŸèƒ½ï¼š
1. **èµ„äº§å‘ç° (Asset Discovery)**: è·å–äº¤æ˜“æ‰€è‚¡ç¥¨åˆ—è¡¨ï¼Œç»´æŠ¤ `assets` è¡¨
2. **å†å²æ•°æ®åŠ è½½ (History Loader)**: é€ä¸ªä¸‹è½½ OHLCV æ•°æ®ï¼Œå¢é‡æ›´æ–° `market_data` è¶…è¡¨

**æ ¸å¿ƒæ¶æ„è®¾è®¡**: åˆ©ç”¨ `Asset.last_synced` ä½œä¸ºæ¸¸æ ‡ï¼Œå®ç°æ–­ç‚¹ç»­ä¼ å’Œæš‚åœ/æ¢å¤åŠŸèƒ½ã€‚

---

## ğŸ›¡ï¸ çº¦æŸæ¡ä»¶ä¸ç­–ç•¥

### 1. å­˜å‚¨çº¦æŸ: 80GB PL0 ç£ç›˜ (ä½ IOPS)

| é—®é¢˜ | å¯¹ç­– |
|------|------|
| IOPS ä½ â†’ å¹¶å‘è¿‡é«˜å¯¼è‡´ I/O é” | é»˜è®¤å¹¶å‘æ•° = 5-8 (å¯è°ƒ) |
| å•æ¬¡æäº¤å¤§é‡è¡Œå¯¼è‡´å†…å­˜æº¢å‡º | æ‰¹å†™å…¥: å†…å­˜ç´¯ç§¯ 5000+ è¡Œå†æäº¤ |
| é¢‘ç¹çš„å° INSERT è¯­å¥ä½æ•ˆ | ä½¿ç”¨ SQLAlchemy `bulk_save_objects()` |

### 2. API çº¦æŸ: æ—  Bulk API

| é—®é¢˜ | å¯¹ç­– |
|------|------|
| æ— æ³•ä¸€æ¬¡æ€§ä¸‹è½½æ‰€æœ‰è‚¡ç¥¨çš„æ•°æ® | ç¬¦å·-é€ç¬¦å·å¾ªç¯ (Symbol-by-Symbol Loop) |
| éœ€è¦è·Ÿè¸ª N ä¸ªç¬¦å·çš„åŒæ­¥çŠ¶æ€ | `Asset.last_synced` æ¸¸æ ‡æœºåˆ¶ |
| ä¸­æ–­åéœ€è¦ä»ä¸Šæ¬¡æ–­ç‚¹ç»§ç»­ | æŸ¥è¯¢ `WHERE last_synced IS NULL OR last_synced < NOW() - INTERVAL '1 day'` |

### 3. ç½‘ç»œç¨³å®šæ€§çº¦æŸ

| é—®é¢˜ | å¯¹ç­– |
|------|------|
| API æœåŠ¡å¶å°”è¿”å› 5xx é”™è¯¯ | æŒ‡æ•°é€€é¿é‡è¯• (Exponential Backoff) |
| è¿æ¥è¶…æ—¶ | è®¾ç½®åˆç†çš„ timeout (30s) |
| ä¸€ä¸ªç¬¦å·å¤±è´¥ä¸åº”ä¸­æ­¢æ•´ä¸ªè¿‡ç¨‹ | Per-asset é”™è¯¯å¤„ç†å’Œæ—¥å¿— |

---

## âœ… äº¤ä»˜å†…å®¹

### 1. èµ„äº§å‘ç°æœåŠ¡ (`src/data_nexus/ingestion/asset_discovery.py`)

**èŒè´£**: ä» EODHD è·å–äº¤æ˜“æ‰€ç¬¦å·åˆ—è¡¨å¹¶ä¿å­˜åˆ°æ•°æ®åº“ã€‚

```python
class AssetDiscovery:
    """Exchange symbol list discovery and ingestion."""

    def __init__(self, api_key: str, db_config: DatabaseConfig):
        self.api_key = api_key
        self.db_config = db_config

    async def discover_exchange(self, exchange: str) -> int:
        """
        Fetch symbols from EODHD exchange endpoint.

        Args:
            exchange: Exchange code (e.g., 'US', 'LSE', 'TSE')

        Returns:
            Number of new assets added to database

        Endpoint: /api/exchange-symbol-list/{exchange}
        Response format: CSV with columns [Code, Exchange, Name, Type, Country, Currency, ISIN, ...]

        Logic:
            1. Fetch symbol list from API
            2. Parse CSV response
            3. For each symbol:
               - Upsert into assets table
               - Set is_active=True
               - Set last_synced=NULL (needs data)
            4. Return count of newly added assets
        """
```

**æ•°æ®åº“æ“ä½œ**:
```python
def _upsert_asset(self, session, symbol: str, exchange: str, asset_type: str):
    """Upsert single asset into database."""
    asset = session.query(Asset).filter_by(symbol=symbol).first()
    if asset:
        # Asset exists, reactivate if needed
        asset.is_active = True
    else:
        # New asset, create with last_synced=NULL
        asset = Asset(
            symbol=symbol,
            exchange=exchange,
            asset_type=asset_type,
            is_active=True,
            last_synced=None  # Signals: needs data
        )
        session.add(asset)
```

### 2. æ ¸å¿ƒåŠ è½½å™¨ (`src/data_nexus/ingestion/history_loader.py`)

**èŒè´£**: å¼‚æ­¥ä¸‹è½½å†å² OHLCV æ•°æ®å¹¶æ‰¹é‡å†™å…¥æ•°æ®åº“ã€‚

```python
class EODHistoryLoader:
    """
    High-performance async OHLCV data loader.

    Design Principles:
    1. Low Concurrency (5-8): Respect PL0 disk constraints
    2. Aggressive Batching: Accumulate 5000+ rows before insert
    3. Cursor-based: Use Asset.last_synced for resume capability
    4. Error Resilience: Per-asset error handling, exponential backoff
    """

    def __init__(self, api_key: str, db_config: DatabaseConfig):
        self.api_key = api_key
        self.db_config = db_config
        self.batch_size = 100  # Rows to fetch per API call
        self.write_batch_size = 5000  # Rows to accumulate before DB insert
        self.concurrency = 5  # Semaphore limit for concurrent requests
        self.timeout = 30  # Seconds per request

    async def run_cycle(self, limit: int = None, days_old: int = 1):
        """
        Run one cycle of incremental data loading.

        Args:
            limit: Max assets to process in this run (prevents OOM)
            days_old: Only sync assets where last_synced < NOW - {days_old} days

        Logic:
            1. Query assets to sync:
               SELECT * FROM assets
               WHERE is_active=True
               AND (last_synced IS NULL OR last_synced < NOW - INTERVAL '{days_old} days')
               ORDER BY last_synced ASC NULLS FIRST
               LIMIT {limit}

            2. Create asyncio.Queue with these assets

            3. Spawn N worker coroutines (controlled by Semaphore):
               - Fetch /api/eod/{symbol}.{exchange}
               - Parse response (handle both JSON and CSV formats)
               - Transform to MarketData objects
               - Accumulate in batch buffer
               - When batch reaches write_batch_size:
                 * Execute bulk_save_objects()
                 * Clear buffer

            4. Update Asset.last_synced = datetime.now()

            5. Return summary: {total_assets: N, total_rows: M, failed: L}
        """
```

**å…³é”®å®ç°ç»†èŠ‚**:

**å¤„ç†ç©ºå€¼å’Œå¼‚å¸¸**:
```python
async def _fetch_symbol(self, session: Session, asset: Asset) -> list[MarketData]:
    """Fetch and parse OHLCV data for single symbol."""
    try:
        # Construct URL: /api/eod/{symbol} with proper parameters
        url = f"{EODHD_API_URL}/eod/{asset.symbol}"
        params = {
            "api_token": self.api_key,
            "fmt": "json",
            "period": "d"  # Daily
        }

        # If asset.last_synced is not None, fetch only newer data
        if asset.last_synced:
            from_date = (asset.last_synced.date() + timedelta(days=1)).isoformat()
            params["from"] = from_date

        async with aiohttp.ClientSession() as session:
            async with session.get(url, params=params, timeout=self.timeout) as resp:
                if resp.status == 404:
                    logger.warning(f"Asset {asset.symbol} not found (404)")
                    return []
                elif resp.status == 429:
                    logger.warning(f"Rate limited on {asset.symbol}, backing off")
                    await asyncio.sleep(60)  # Back off 1 minute
                    return []
                elif resp.status >= 500:
                    logger.error(f"Server error {resp.status} for {asset.symbol}")
                    return []  # Don't update last_synced, retry next cycle

                data = await resp.json()

        # Parse response
        if isinstance(data, dict) and "error" in data:
            logger.error(f"API error for {asset.symbol}: {data['error']}")
            return []

        if not isinstance(data, list):
            logger.error(f"Unexpected response format for {asset.symbol}")
            return []

        # Transform to MarketData objects
        market_data_list = []
        for row in data:
            try:
                md = MarketData(
                    time=datetime.fromisoformat(row["date"]).replace(tzinfo=timezone.utc),
                    symbol=asset.symbol,
                    open=Decimal(str(row["open"])),
                    high=Decimal(str(row["high"])),
                    low=Decimal(str(row["low"])),
                    close=Decimal(str(row["close"])),
                    adjusted_close=Decimal(str(row.get("adjusted_close", row["close"]))),
                    volume=int(row["volume"])
                )
                market_data_list.append(md)
            except (KeyError, ValueError, TypeError) as e:
                logger.error(f"Parse error in {asset.symbol} row {row}: {e}")
                continue

        return market_data_list

    except asyncio.TimeoutError:
        logger.error(f"Timeout fetching {asset.symbol}")
        return []
    except Exception as e:
        logger.error(f"Unexpected error fetching {asset.symbol}: {e}")
        return []
```

**æ‰¹å†™å…¥é€»è¾‘ (Critical for PL0)**:
```python
async def _run_workers(self, assets_queue: asyncio.Queue, session: Session):
    """
    Worker coroutines that fetch data and batch-write to DB.

    Key: Accumulate rows in memory, write in large batches.
    """
    semaphore = asyncio.Semaphore(self.concurrency)
    batch_buffer = []
    assets_to_update = []

    async def worker():
        nonlocal batch_buffer, assets_to_update

        while True:
            try:
                asset = assets_queue.get_nowait()
            except asyncio.QueueEmpty:
                break

            async with semaphore:
                logger.info(f"Fetching {asset.symbol}...")
                market_data_list = await self._fetch_symbol(session, asset)

                if market_data_list:
                    batch_buffer.extend(market_data_list)
                    assets_to_update.append(asset)
                    logger.info(f"  â†’ {len(market_data_list)} rows for {asset.symbol}")

                    # Check if we should flush batch to DB
                    if len(batch_buffer) >= self.write_batch_size:
                        logger.info(f"Flushing {len(batch_buffer)} rows to database...")
                        session.bulk_save_objects(batch_buffer)
                        session.commit()
                        batch_buffer.clear()
                else:
                    logger.warning(f"  â†’ No data for {asset.symbol}")

                assets_queue.task_done()

    # Run workers
    workers = [asyncio.create_task(worker()) for _ in range(self.concurrency)]
    await asyncio.gather(*workers)

    # Final flush for remaining rows
    if batch_buffer:
        logger.info(f"Final flush: {len(batch_buffer)} rows to database...")
        session.bulk_save_objects(batch_buffer)
        session.commit()

    # Update asset.last_synced
    for asset in assets_to_update:
        asset.last_synced = datetime.now(timezone.utc)
    session.commit()
```

### 3. å‘½ä»¤è¡Œæ¥å£ (`bin/run_ingestion.py`)

```python
"""
CLI for asset discovery and historical data ingestion.

Usage:
    python3 bin/run_ingestion.py discover --exchange US
    python3 bin/run_ingestion.py backfill --limit 100 --concurrency 5 --days-old 1
"""

import asyncio
import click
from src.data_nexus.ingestion.asset_discovery import AssetDiscovery
from src.data_nexus.ingestion.history_loader import EODHistoryLoader
from src.data_nexus.config import DatabaseConfig

@click.group()
def cli():
    """EODHD Ingestion CLI"""
    pass

@cli.command()
@click.option('--exchange', required=True, help='Exchange code (US, LSE, TSE, etc.)')
def discover(exchange):
    """Discover and ingest assets for an exchange."""
    api_key = os.environ.get('EODHD_API_KEY')
    if not api_key:
        click.echo("âŒ EODHD_API_KEY not found")
        return 1

    db_config = DatabaseConfig()
    discovery = AssetDiscovery(api_key, db_config)

    click.echo(f"ğŸ“¥ Discovering assets for {exchange}...")
    count = asyncio.run(discovery.discover_exchange(exchange))
    click.echo(f"âœ… Added/updated {count} assets")
    return 0

@cli.command()
@click.option('--limit', default=100, help='Max assets to process (prevents OOM)')
@click.option('--concurrency', default=5, help='Concurrent API requests')
@click.option('--days-old', default=1, help='Sync assets older than N days')
def backfill(limit, concurrency, days_old):
    """
    Download historical OHLCV data for assets.

    Example:
        python3 bin/run_ingestion.py backfill --limit 500 --concurrency 5
    """
    api_key = os.environ.get('EODHD_API_KEY')
    if not api_key:
        click.echo("âŒ EODHD_API_KEY not found")
        return 1

    db_config = DatabaseConfig()
    loader = EODHistoryLoader(api_key, db_config)
    loader.concurrency = concurrency

    click.echo(f"â³ Starting backfill: limit={limit}, concurrency={concurrency}...")
    start_time = datetime.now()

    summary = asyncio.run(loader.run_cycle(limit=limit, days_old=days_old))

    elapsed = (datetime.now() - start_time).total_seconds()
    click.echo(f"âœ… Backfill complete in {elapsed:.1f}s")
    click.echo(f"   Assets processed: {summary['total_assets']}")
    click.echo(f"   Rows inserted: {summary['total_rows']}")
    click.echo(f"   Failed: {summary['failed']}")

    return 0

if __name__ == '__main__':
    cli()
```

### 4. éªŒè¯è„šæœ¬ (`scripts/verify_ingestion.py`)

```python
"""
Verify asset discovery and data ingestion.

Tests:
1. Discover: Fetch symbols and verify they're in DB
2. Backfill: Load data for 10 test assets
3. Query: Verify market_data rows exist and last_synced was updated
"""

def test_discover():
    """Test asset discovery for US exchange."""
    print("ğŸ§ª Testing asset discovery...")
    result = subprocess.run(
        ["python3", "bin/run_ingestion.py", "discover", "--exchange", "US"],
        capture_output=True,
        text=True
    )

    if result.returncode != 0:
        print(f"âŒ Discovery failed: {result.stderr}")
        return False

    # Verify assets were added
    conn = PostgresConnection()
    count = conn.query_scalar("SELECT COUNT(*) FROM assets WHERE exchange='US'")

    if count > 0:
        print(f"âœ… Discovery successful: {count} US assets in database")
        return True
    else:
        print("âŒ No assets found after discovery")
        return False

def test_backfill():
    """Test data ingestion for small subset."""
    print("ğŸ§ª Testing data backfill...")
    result = subprocess.run(
        ["python3", "bin/run_ingestion.py", "backfill", "--limit", "10", "--concurrency", "2"],
        capture_output=True,
        text=True
    )

    if result.returncode != 0:
        print(f"âŒ Backfill failed: {result.stderr}")
        return False

    # Verify data was inserted
    conn = PostgresConnection()
    rows = conn.query_scalar("SELECT COUNT(*) FROM market_data")

    if rows > 0:
        print(f"âœ… Backfill successful: {rows} OHLCV rows in database")
        return True
    else:
        print("âŒ No market data found after backfill")
        return False

def test_last_synced_updated():
    """Verify Asset.last_synced was updated."""
    print("ğŸ§ª Testing last_synced tracking...")
    conn = PostgresConnection()
    result = conn.query_scalar(
        "SELECT COUNT(*) FROM assets WHERE last_synced IS NOT NULL"
    )

    if result > 0:
        print(f"âœ… last_synced updated for {result} assets")
        return True
    else:
        print("âŒ last_synced not updated after backfill")
        return False
```

---

## ğŸ“Š é¡¹ç›®ç»“æ„

```
src/data_nexus/
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ asset_discovery.py      # Exchange symbol discovery
â”‚   â”œâ”€â”€ history_loader.py       # Async OHLCV downloader
â”‚   â””â”€â”€ retry_policy.py         # Exponential backoff logic
â”œâ”€â”€ models.py                   # SQLAlchemy ORM (from Task #033)
â”œâ”€â”€ database/
â”‚   â””â”€â”€ connection.py           # PostgreSQL connection pool
â””â”€â”€ config.py                   # Configuration (API key, DB URL)

bin/
â””â”€â”€ run_ingestion.py           # CLI entry point

scripts/
â””â”€â”€ verify_ingestion.py        # End-to-end tests
```

---

## ğŸ”§ æŠ€æœ¯æ ˆ

| ç»„ä»¶ | åº“ | ç‰ˆæœ¬ | ç”¨é€” |
|------|-----|------|------|
| å¼‚æ­¥è¿è¡Œæ—¶ | `asyncio` | stdlib | Event loop å’Œ coroutine ç®¡ç† |
| HTTP å®¢æˆ·ç«¯ | `aiohttp` | 3.9+ | å¼‚æ­¥ HTTP è¯·æ±‚ |
| DNS è§£æ | `aiodns` | 3.1+ | å¼‚æ­¥ DNS (å¯é€‰ï¼ŒåŠ å¿« DNS æŸ¥è¯¢) |
| æ•°æ®åº“ | `SQLAlchemy` | 2.0+ | ORM å’Œ bulk operations |
| æ•°æ®åº“é©±åŠ¨ | `psycopg2` | 2.9+ | PostgreSQL é€‚é…å™¨ |
| CLI | `click` | 8.1+ | å‘½ä»¤è¡Œå‚æ•°è§£æ |
| æ—¥å¿— | `logging` | stdlib | æ ‡å‡†æ—¥å¿— |

**æ›´æ–° requirements.txt**:
```
aiohttp>=3.9.0
aiodns>=3.1.0
```

---

## ğŸš€ é¢„æœŸå·¥ä½œé‡

| éƒ¨åˆ† | æ—¶é—´ | ä¼˜å…ˆçº§ |
|------|------|--------|
| èµ„äº§å‘ç°æœåŠ¡ | 1.5 hours | â­â­â­ |
| å†å²åŠ è½½å™¨ | 2.5 hours | â­â­â­ |
| å‘½ä»¤è¡Œæ¥å£ | 1 hour | â­â­â­ |
| éªŒè¯è„šæœ¬ | 1 hour | â­â­ |
| **æ€»è®¡** | **6 hours** | |

---

## ğŸ›¡ï¸ æˆåŠŸæ ‡å‡†

| æ ‡å‡† | éªŒæ”¶æ¡ä»¶ |
|------|---------|
| Asset Discovery | `python3 bin/run_ingestion.py discover --exchange US` æ·»åŠ  10,000+ èµ„äº§ |
| Data Ingestion | `python3 bin/run_ingestion.py backfill --limit 100` åŠ è½½ 50,000+ è¡Œ OHLCV æ•°æ® |
| Batch Writing | è§‚å¯Ÿæ—¥å¿—æ˜¾ç¤ºæ‰¹é‡å†™å…¥ (5000+ è¡Œ/æ¬¡) è€Œéè¡Œ-é€-è¡Œ |
| Error Handling | å•ä¸ªèµ„äº§å¤±è´¥ä¸ä¸­æ­¢æ•´ä¸ªè¿‡ç¨‹ï¼Œå¤±è´¥è¢«è®°å½• |
| Resume Capability | ç¬¬äºŒæ¬¡è¿è¡Œ backfill åº”ä»…å¤„ç† `last_synced IS NULL` æˆ–è€åŒ–çš„èµ„äº§ |
| Performance | 100 èµ„äº§çš„å®Œæ•´å‘¨æœŸ < 60 ç§’ (å¹¶å‘=5) |

---

## ğŸ“ å…³é”®è®¾è®¡å†³ç­–

### ä¸ºä»€ä¹ˆæ˜¯å¼‚æ­¥è€Œéå¤šè¿›ç¨‹?
- **ç†ç”±**: aiohttp è½»é‡çº§ï¼Œé€‚åˆ I/O å¯†é›†çš„ API è°ƒç”¨ã€‚Python multiprocessing å¯¹äºç½‘ç»œ I/O å¼€é”€å¤§ã€‚
- **æƒè¡¡**: GIL é™åˆ¶ CPU å¹¶è¡Œï¼Œä½†æˆ‘ä»¬ä¸»è¦æ˜¯ç­‰å¾… I/Oï¼Œæ‰€ä»¥ asyncio è¶³å¤Ÿã€‚

### ä¸ºä»€ä¹ˆæ‰¹å†™å…¥è€Œéé€è¡Œæ’å…¥?
- **ç†ç”±**: PL0 ç£ç›˜ IOPS ä½ã€‚æ¯ä¸ª INSERT è¯­å¥éƒ½æ˜¯ä¸€æ¬¡æ•°æ®åº“å¾€è¿”ï¼Œéå¸¸ä½æ•ˆã€‚
- **SQLAlchemy bulk_save_objects()**: ç”Ÿæˆå•ä¸ª INSERT...VALUES(...), (...), ... è¯­å¥ï¼Œ1000 è¡Œ = 1 ä¸ªå¾€è¿”ã€‚
- **5000 è¡Œæ‰¹å¤§å°**: åœ¨å†…å­˜å’Œæ•°æ®åº“æ€§èƒ½ä¹‹é—´çš„å¹³è¡¡ã€‚

### ä¸ºä»€ä¹ˆå¹¶å‘æ•°é™åˆ¶ä¸º 5-8?
- **ç†ç”±**: PL0 ç£ç›˜ IOPS å—é™ï¼Œè¿‡å¤šçš„å¹¶å‘è¯·æ±‚ä¼šå¯¼è‡´æ•°æ®åº“è¿æ¥æ±  stallã€‚
- **è§‚å¯Ÿ**: Semaphore(5) é™åˆ¶å¹¶å‘ HTTP è¯·æ±‚ï¼Œç¡®ä¿æ¯ä¸ªæ•°æ®åº“å†™æ“ä½œæœ‰è¶³å¤Ÿçš„æ—¶é—´å®Œæˆã€‚
- **è°ƒä¼˜ç©ºé—´**: å¦‚æœç›‘æ§æ˜¾ç¤º CPU/I/O æœ‰ä½™é‡ï¼Œå¯ä»¥æå‡åˆ° 10-20ã€‚

### ä¸ºä»€ä¹ˆç”¨ Asset.last_synced è€Œéå•ç‹¬çš„çŠ¶æ€è¡¨?
- **ç†ç”±**: Simple is better. ä¸€ä¸ªå­—æ®µèƒœè¿‡é¢å¤–çš„è¡¨ã€‚`NULL` è¡¨ç¤ºæœªåŒæ­¥ï¼Œtimestamp è¡¨ç¤ºæœ€åæˆåŠŸæ—¶é—´ã€‚
- **æ¢å¤**: é‡æ–°å¯åŠ¨ loader åï¼Œè‡ªåŠ¨ç»§ç»­å¤„ç† `last_synced IS NULL` æˆ– `< NOW() - INTERVAL '1 day'` çš„èµ„äº§ã€‚

---

## ğŸ“ éƒ¨ç½²æ¸…å•

- [ ] å®‰è£…ä¾èµ–: `pip install -r requirements.txt`
- [ ] éªŒè¯ EODHD_API_KEY åœ¨ç¯å¢ƒä¸­è®¾ç½®
- [ ] éªŒè¯æ•°æ®åº“è¿æ¥ (Task #032)
- [ ] è¿è¡Œ Alembic è¿ç§» (Task #033): `alembic upgrade head`
- [ ] è¿è¡ŒéªŒè¯è„šæœ¬: `python3 scripts/verify_ingestion.py`
- [ ] é¦–æ¬¡è¿è¡Œ: `python3 bin/run_ingestion.py discover --exchange US`
- [ ] é¦–æ¬¡åŠ è½½: `python3 bin/run_ingestion.py backfill --limit 1000 --concurrency 5`

---

**Created**: 2025-12-28
**For Task**: #034
**Phase**: 2 (Data Intelligence)
**Protocol**: v2.6 (CLI --plan Integration)
**Critical Constraints**: PL0 Disk (IOPS Low), No Bulk API
