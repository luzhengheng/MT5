#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
XGBoost Hyperparameter Optimizer

ä½¿ç”¨ Optuna æ¡†æ¶è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–ã€‚

åè®®: v2.2 (æœ¬åœ°å­˜å‚¨ï¼Œæ–‡æ¡£ä¼˜å…ˆ)
"""

import logging
import json
import sys
from pathlib import Path
from typing import Dict, Optional
import numpy as np
import pandas as pd
from xgboost import XGBClassifier
from sklearn.metrics import (
    roc_auc_score, accuracy_score, precision_score,
    recall_score, f1_score
)

try:
    import optuna
    from optuna.samplers import TPESampler
    from optuna.pruners import MedianPruner
except ImportError:
    print("âŒ Optuna æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install optuna")
    sys.exit(1)

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Color codes
GREEN = "\033[92m"
RED = "\033[91m"
YELLOW = "\033[93m"
CYAN = "\033[96m"
BLUE = "\033[94m"
MAGENTA = "\033[95m"
RESET = "\033[0m"


class HyperparameterOptimizer:
    """XGBoost è¶…å‚æ•°ä¼˜åŒ–å™¨"""

    def __init__(
        self,
        X_train: np.ndarray,
        X_test: np.ndarray,
        y_train: pd.Series,
        y_test: pd.Series,
        n_trials: int = 50,
        random_state: int = 42
    ):
        """
        åˆå§‹åŒ–ä¼˜åŒ–å™¨
        
        å‚æ•°:
            X_train: è®­ç»ƒç‰¹å¾ (æ ‡å‡†åŒ–å)
            X_test: æµ‹è¯•ç‰¹å¾ (æ ‡å‡†åŒ–å)
            y_train: è®­ç»ƒæ ‡ç­¾
            y_test: æµ‹è¯•æ ‡ç­¾
            n_trials: Optuna è¯•éªŒæ¬¡æ•°
            random_state: éšæœºç§å­
        """
        self.X_train = X_train
        self.X_test = X_test
        self.y_train = y_train
        self.y_test = y_test
        self.n_trials = n_trials
        self.random_state = random_state

        self.study = None
        self.best_params = None
        self.best_model = None
        self.best_score = None

        logger.info(f"{GREEN}âœ… HyperparameterOptimizer å·²åˆå§‹åŒ–{RESET}")
        logger.info(f"  è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
        logger.info(f"  æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
        logger.info(f"  è¯•éªŒæ¬¡æ•°: {n_trials}")

    def objective(self, trial):
        """
        Optuna ç›®æ ‡å‡½æ•°
        
        å‚æ•°:
            trial: Optuna Trial å¯¹è±¡
            
        è¿”å›:
            AUC-ROC åˆ†æ•° (è¶Šé«˜è¶Šå¥½)
        """
        # é‡‡æ ·è¶…å‚æ•°
        params = {
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
            'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 2.0),
            'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 2.0),
            'objective': 'binary:logistic',
            'random_state': self.random_state,
            'n_jobs': -1,
            'verbosity': 0
        }

        try:
            # è®­ç»ƒæ¨¡å‹
            model = XGBClassifier(**params)
            model.fit(self.X_train, self.y_train, verbose=False)

            # é¢„æµ‹æµ‹è¯•é›†
            y_pred_proba = model.predict_proba(self.X_test)[:, 1]

            # è®¡ç®— AUC-ROC
            auc = roc_auc_score(self.y_test, y_pred_proba)

            return auc

        except Exception as e:
            logger.warning(f"{YELLOW}âš ï¸  Trial {trial.number} å¤±è´¥: {e}{RESET}")
            return 0.0

    def optimize(self) -> Dict:
        """
        è¿è¡Œè¶…å‚æ•°ä¼˜åŒ–
        
        è¿”å›:
            æœ€ä½³è¶…å‚æ•°å­—å…¸
        """
        logger.info(f"{BLUE}{'=' * 80}{RESET}")
        logger.info(f"{BLUE}ğŸ”§ XGBoost è¶…å‚æ•°ä¼˜åŒ– (Optuna){RESET}")
        logger.info(f"{BLUE}{'=' * 80}{RESET}")
        logger.info("")

        logger.info(f"{CYAN}ğŸš€ å¼€å§‹ä¼˜åŒ–...{RESET}")
        logger.info(f"  è¯•éªŒæ¬¡æ•°: {self.n_trials}")
        logger.info(f"  é‡‡æ ·å™¨: TPESampler")
        logger.info(f"  å‰ªæå™¨: MedianPruner")
        logger.info(f"  ç›®æ ‡: æœ€å¤§åŒ– AUC-ROC")
        logger.info("")

        # åˆ›å»º Study
        self.study = optuna.create_study(
            study_name='xgboost_optimization_v1',
            direction='maximize',
            sampler=TPESampler(seed=self.random_state),
            pruner=MedianPruner(
                n_startup_trials=10,
                n_warmup_steps=5
            )
        )

        # è¿è¡Œä¼˜åŒ–
        self.study.optimize(
            self.objective,
            n_trials=self.n_trials,
            show_progress_bar=True
        )

        # è·å–æœ€ä½³å‚æ•°
        self.best_params = self.study.best_params
        self.best_score = self.study.best_value

        logger.info("")
        logger.info(f"{GREEN}âœ… ä¼˜åŒ–å®Œæˆ{RESET}")
        logger.info(f"  æœ€ä½³ AUC: {self.best_score:.4f}")
        logger.info(f"  æœ€ä½³è¯•éªŒ: Trial {self.study.best_trial.number}")
        logger.info(f"  æœ€ä½³å‚æ•°:")
        for key, value in self.best_params.items():
            if isinstance(value, float):
                logger.info(f"    {key}: {value:.4f}")
            else:
                logger.info(f"    {key}: {value}")

        return self.best_params

    def train_best_model(self) -> XGBClassifier:
        """
        ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        
        è¿”å›:
            è®­ç»ƒå¥½çš„ XGBClassifier
        """
        if self.best_params is None:
            raise ValueError("å¿…é¡»å…ˆè¿è¡Œ optimize() æ–¹æ³•")

        logger.info("")
        logger.info(f"{CYAN}ğŸš€ ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæ¨¡å‹...{RESET}")

        # æ„å»ºå®Œæ•´å‚æ•°
        params = {
            **self.best_params,
            'objective': 'binary:logistic',
            'random_state': self.random_state,
            'n_jobs': -1,
            'verbosity': 0
        }

        # è®­ç»ƒæ¨¡å‹
        self.best_model = XGBClassifier(**params)
        self.best_model.fit(self.X_train, self.y_train, verbose=False)

        logger.info(f"{GREEN}âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ{RESET}")

        return self.best_model

    def evaluate_best_model(self) -> Dict:
        """
        è¯„ä¼°æœ€ä½³æ¨¡å‹æ€§èƒ½
        
        è¿”å›:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        if self.best_model is None:
            raise ValueError("å¿…é¡»å…ˆè¿è¡Œ train_best_model() æ–¹æ³•")

        logger.info("")
        logger.info(f"{CYAN}ğŸ“Š è¯„ä¼°æœ€ä½³æ¨¡å‹...{RESET}")

        # é¢„æµ‹
        y_pred = self.best_model.predict(self.X_test)
        y_pred_proba = self.best_model.predict_proba(self.X_test)[:, 1]

        # è®¡ç®—æŒ‡æ ‡
        results = {
            'accuracy': float(accuracy_score(self.y_test, y_pred)),
            'precision': float(precision_score(self.y_test, y_pred)),
            'recall': float(recall_score(self.y_test, y_pred)),
            'f1_score': float(f1_score(self.y_test, y_pred)),
            'auc_roc': float(roc_auc_score(self.y_test, y_pred_proba)),
            'test_samples': int(len(self.y_test)),
            'train_samples': int(len(self.y_train))
        }

        logger.info(f"{GREEN}âœ… è¯„ä¼°å®Œæˆ{RESET}")
        logger.info("")
        logger.info(f"  Accuracy:  {results['accuracy']:.4f}")
        logger.info(f"  Precision: {results['precision']:.4f}")
        logger.info(f"  Recall:    {results['recall']:.4f}")
        logger.info(f"  F1-Score:  {results['f1_score']:.4f}")
        logger.info(f"  AUC-ROC:   {results['auc_roc']:.4f}")

        return results

    def save_best_params(self, path: str = "models/best_params_v1.json") -> str:
        """
        ä¿å­˜æœ€ä½³è¶…å‚æ•°åˆ° JSON æ–‡ä»¶
        
        å‚æ•°:
            path: ä¿å­˜è·¯å¾„
            
        è¿”å›:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if self.best_params is None:
            raise ValueError("å¿…é¡»å…ˆè¿è¡Œ optimize() æ–¹æ³•")

        logger.info("")
        logger.info(f"{CYAN}ğŸ’¾ ä¿å­˜æœ€ä½³å‚æ•°...{RESET}")

        filepath = PROJECT_ROOT / path
        filepath.parent.mkdir(parents=True, exist_ok=True)

        # æ·»åŠ å…ƒæ•°æ®
        output = {
            "best_params": self.best_params,
            "best_auc": float(self.best_score),
            "n_trials": self.n_trials,
            "random_state": self.random_state,
            "timestamp": pd.Timestamp.now().isoformat()
        }

        with open(filepath, 'w') as f:
            json.dump(output, f, indent=2)

        logger.info(f"{GREEN}âœ… æœ€ä½³å‚æ•°å·²ä¿å­˜{RESET}")
        logger.info(f"  è·¯å¾„: {filepath}")

        return str(filepath)

    def compare_with_baseline(self, baseline_results: Dict) -> None:
        """
        å¯¹æ¯”ä¼˜åŒ–åæ¨¡å‹ä¸åŸºçº¿æ¨¡å‹
        
        å‚æ•°:
            baseline_results: åŸºçº¿æ¨¡å‹çš„è¯„ä¼°ç»“æœ
        """
        if self.best_model is None:
            raise ValueError("å¿…é¡»å…ˆè¿è¡Œ train_best_model() æ–¹æ³•")

        optimized_results = self.evaluate_best_model()

        logger.info("")
        logger.info(f"{MAGENTA}{'=' * 80}{RESET}")
        logger.info(f"{MAGENTA}ğŸ“ˆ Before vs After å¯¹æ¯”{RESET}")
        logger.info(f"{MAGENTA}{'=' * 80}{RESET}")
        logger.info("")

        metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc']

        for metric in metrics:
            baseline_val = baseline_results.get(metric, 0.0)
            optimized_val = optimized_results.get(metric, 0.0)
            improvement = optimized_val - baseline_val
            improvement_pct = (improvement / baseline_val * 100) if baseline_val > 0 else 0.0

            color = GREEN if improvement > 0 else (RED if improvement < 0 else YELLOW)
            sign = "+" if improvement > 0 else ""

            logger.info(
                f"  {metric.upper():12s}: "
                f"{baseline_val:.4f} â†’ {optimized_val:.4f} "
                f"{color}({sign}{improvement:.4f}, {sign}{improvement_pct:.2f}%){RESET}"
            )

        logger.info("")
        logger.info(f"{MAGENTA}{'=' * 80}{RESET}")


def load_baseline_results(path: str = "models/baseline_v1_results.json") -> Dict:
    """
    åŠ è½½åŸºçº¿æ¨¡å‹çš„è¯„ä¼°ç»“æœ
    
    å‚æ•°:
        path: åŸºçº¿ç»“æœæ–‡ä»¶è·¯å¾„
        
    è¿”å›:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    filepath = PROJECT_ROOT / path

    if not filepath.exists():
        logger.warning(f"{YELLOW}âš ï¸  åŸºçº¿ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {filepath}{RESET}")
        return {}

    with open(filepath, 'r') as f:
        results = json.load(f)

    logger.info(f"{GREEN}âœ… å·²åŠ è½½åŸºçº¿ç»“æœ: {filepath}{RESET}")

    return results
