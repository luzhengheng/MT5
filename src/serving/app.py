#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Feature Serving API (FastAPI)

æš´éœ² Feast Feature Store ä¸º HTTP REST APIã€‚

åè®®: v2.2 (æœ¬åœ°å­˜å‚¨ï¼Œæ–‡æ¡£ä¼˜å…ˆ)

ä½¿ç”¨æ–¹æ³•:
    python3 -m uvicorn src.serving.app:app --host 0.0.0.0 --port 8000 --reload
"""

import logging
import sys
from pathlib import Path
from datetime import datetime
from typing import List

from fastapi import FastAPI, HTTPException, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import ValidationError

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.serving.models import (
    HistoricalRequest, LatestRequest,
    HistoricalResponse, LatestResponse, ErrorResponse, HealthResponse,
    FeatureDataPoint, InvocationRequest, InvocationResponse
)
from src.serving.handlers import FeatureService

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Color codes
GREEN = "\033[92m"
RED = "\033[91m"
CYAN = "\033[96m"
RESET = "\033[0m"

# ============================================================================
# FastAPI åº”ç”¨åˆå§‹åŒ–
# ============================================================================

app = FastAPI(
    title="MT5 Feature Serving API",
    description="ç‰¹å¾ä»“åº“ HTTP æœåŠ¡ - æä¾›å†å²å’Œå®æ—¶ç‰¹å¾æ£€ç´¢",
    version="1.0.0",
    docs_url="/docs",
    openapi_url="/openapi.json"
)

# é…ç½® CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================================================
# å…¨å±€å˜é‡
# ============================================================================

feature_service = None
model_predictor = None  # Global model instance


def get_feature_service() -> FeatureService:
    """è·å–æˆ–åˆå§‹åŒ–ç‰¹å¾æœåŠ¡"""
    global feature_service
    if feature_service is None:
        try:
            feature_service = FeatureService(repo_path="src/feature_store")
        except Exception as e:
            logger.error(f"{RED}âŒ æ— æ³•åˆå§‹åŒ–ç‰¹å¾æœåŠ¡: {e}{RESET}")
            raise
    return feature_service


def get_model_predictor():
    """è·å–æˆ–åˆå§‹åŒ–æ¨¡å‹é¢„æµ‹å™¨ (Task #080: Real Model Loading)"""
    global model_predictor
    if model_predictor is None:
        # Import here to avoid circular dependencies
        from src.model.predict import PricePredictor
        import os

        enable_mock = os.getenv("ENABLE_MOCK_INFERENCE", "false").lower() == "true"

        if enable_mock:
            logger.warning(f"{RED}âš ï¸ [MOCK MODE ENABLED] Using mock predictions{RESET}")
            return None  # Signal mock mode
        else:
            try:
                # Task #080: Load real model from disk
                model_predictor = PricePredictor()
                logger.info(f"{GREEN}âœ… çœŸå®æ¨¡å‹é¢„æµ‹å™¨å·²åˆå§‹åŒ– (Task #080){RESET}")
                logger.info(f"   Model: {model_predictor.model_path}")
                logger.info(f"   Features: {len(model_predictor.feature_names)}")
                logger.info(f"   Test Accuracy: {model_predictor.metadata.get('metrics', {}).get('accuracy', 'N/A')}")
            except Exception as e:
                logger.error(f"{RED}âŒ æ— æ³•åŠ è½½æ¨¡å‹: {e}{RESET}")
                # Fail fast - do not fallback silently
                raise RuntimeError(f"Model initialization failed (Task #080): {e}")

    return model_predictor


# ============================================================================
# å¯åŠ¨å’Œå…³é—­äº‹ä»¶
# ============================================================================

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨äº‹ä»¶"""
    logger.info(f"{CYAN}ğŸš€ Feature Serving API å¯åŠ¨ä¸­...{RESET}")
    try:
        get_feature_service()
        logger.info(f"{GREEN}âœ… ç‰¹å¾æœåŠ¡å·²åˆå§‹åŒ–{RESET}")

        # Task #080: Load model at startup
        import os
        if os.getenv("ENABLE_MOCK_INFERENCE", "false").lower() != "true":
            logger.info(f"{CYAN}ğŸ“¦ åŠ è½½å®æ—¶æ¨¡å‹ (Task #080)...{RESET}")
            get_model_predictor()
            logger.info(f"{GREEN}âœ… å®æ—¶æ¨¡å‹å·²åŠ è½½{RESET}")
        else:
            logger.warning(f"{RED}âš ï¸  [MOCK MODE] è·³è¿‡æ¨¡å‹åŠ è½½{RESET}")

    except Exception as e:
        logger.error(f"{RED}âŒ å¯åŠ¨å¤±è´¥: {e}{RESET}")
        raise


@app.on_event("shutdown")
async def shutdown_event():
    """åº”ç”¨å…³é—­äº‹ä»¶"""
    logger.info(f"{CYAN}ğŸ›‘ Feature Serving API å…³é—­ä¸­...{RESET}")


# ============================================================================
# å¥åº·æ£€æŸ¥ç«¯ç‚¹
# ============================================================================

@app.get(
    "/health",
    response_model=HealthResponse,
    tags=["Health"],
    summary="å¥åº·æ£€æŸ¥",
    description="æ£€æŸ¥ API å’Œç‰¹å¾ä»“åº“çš„å¥åº·çŠ¶æ€"
)
async def health_check():
    """
    å¥åº·æ£€æŸ¥ç«¯ç‚¹

    è¿”å›:
        - status: å¥åº·çŠ¶æ€ (healthy/degraded)
        - feature_store: ç‰¹å¾ä»“åº“çŠ¶æ€ (ready/error)
        - database: æ•°æ®åº“è¿æ¥çŠ¶æ€ (connected/disconnected)
        - timestamp: æ£€æŸ¥æ—¶é—´æˆ³
    """
    try:
        service = get_feature_service()
        health = service.health_check()
        return HealthResponse(**health)
    except Exception as e:
        logger.error(f"{RED}âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {e}{RESET}")
        return HealthResponse(
            status="degraded",
            feature_store="error",
            database="disconnected",
            timestamp=datetime.utcnow()
        )


# ============================================================================
# å†å²ç‰¹å¾æ£€ç´¢ç«¯ç‚¹
# ============================================================================

@app.post(
    "/features/historical",
    response_model=HistoricalResponse,
    responses={
        400: {"model": ErrorResponse, "description": "æ— æ•ˆçš„è¯·æ±‚å‚æ•°"},
        500: {"model": ErrorResponse, "description": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"}
    },
    tags=["Features"],
    summary="å†å²ç‰¹å¾æ£€ç´¢ (ç¦»çº¿)",
    description="è·å–å†å²ç‰¹å¾æ•°æ®ç”¨äºæ¨¡å‹è®­ç»ƒ (æ‰¹é‡ç¦»çº¿æ£€ç´¢)"
)
async def get_historical_features(request: HistoricalRequest):
    """
    å†å²ç‰¹å¾æ£€ç´¢ç«¯ç‚¹

    è·å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„ç‰¹å¾æ•°æ®ã€‚

    å‚æ•°:
        - symbols: äº¤æ˜“å¯¹åˆ—è¡¨ï¼Œä¾‹å¦‚ ['EURUSD', 'GBPUSD']
        - features: ç‰¹å¾åç§°åˆ—è¡¨ï¼Œä¾‹å¦‚ ['sma_20', 'rsi_14']
        - start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
        - end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)

    è¿”å›:
        - status: 'success' æˆ– 'error'
        - data: ç‰¹å¾æ•°æ®åˆ—è¡¨
        - row_count: è¿”å›çš„æ•°æ®è¡Œæ•°
        - execution_time_ms: æ‰§è¡Œæ—¶é—´

    ç¤ºä¾‹:
        ```json
        {
            "symbols": ["EURUSD"],
            "features": ["sma_20", "rsi_14"],
            "start_date": "2024-01-01",
            "end_date": "2024-01-31"
        }
        ```
    """
    try:
        logger.info(f"ğŸ“¥ æ”¶åˆ°å†å²ç‰¹å¾è¯·æ±‚")
        logger.info(f"  Symbols: {request.symbols}")
        logger.info(f"  Features: {request.features}")
        logger.info(f"  Date range: {request.start_date} to {request.end_date}")

        # è·å–ç‰¹å¾æœåŠ¡
        service = get_feature_service()

        # è°ƒç”¨ç‰¹å¾æœåŠ¡
        data, execution_time = service.get_historical_features(
            symbols=request.symbols,
            features=request.features,
            start_date=request.start_date,
            end_date=request.end_date
        )

        # æ ¼å¼åŒ–å“åº”
        feature_data_points = []
        for record in data:
            point = FeatureDataPoint(
                symbol=record['symbol'],
                time=record['time'],
                values=record['values']
            )
            feature_data_points.append(point)

        response = HistoricalResponse(
            status="success",
            data=feature_data_points,
            row_count=len(feature_data_points),
            execution_time_ms=execution_time
        )

        logger.info(f"{GREEN}âœ… å†å²ç‰¹å¾æ£€ç´¢æˆåŠŸ: {len(feature_data_points)} è¡Œ{RESET}")
        return response

    except ValidationError as e:
        logger.error(f"{RED}âŒ è¯·æ±‚éªŒè¯å¤±è´¥: {e}{RESET}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={
                "status": "error",
                "message": str(e),
                "error_code": "VALIDATION_ERROR"
            }
        )

    except Exception as e:
        logger.error(f"{RED}âŒ å¤„ç†è¯·æ±‚å¤±è´¥: {e}{RESET}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={
                "status": "error",
                "message": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}",
                "error_code": "INTERNAL_ERROR"
            }
        )


# ============================================================================
# å®æ—¶ç‰¹å¾æ£€ç´¢ç«¯ç‚¹
# ============================================================================

@app.post(
    "/features/latest",
    response_model=LatestResponse,
    responses={
        400: {"model": ErrorResponse, "description": "æ— æ•ˆçš„è¯·æ±‚å‚æ•°"},
        500: {"model": ErrorResponse, "description": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"}
    },
    tags=["Features"],
    summary="å®æ—¶ç‰¹å¾æ£€ç´¢ (åœ¨çº¿)",
    description="è·å–æœ€æ–°ç‰¹å¾æ•°æ®ç”¨äºå®æ—¶æ¨ç† (åœ¨çº¿æ¨¡æ‹Ÿ)"
)
async def get_latest_features(request: LatestRequest):
    """
    å®æ—¶ç‰¹å¾æ£€ç´¢ç«¯ç‚¹

    è·å–æœ€æ–°çš„ç‰¹å¾æ•°æ®ï¼Œç”¨äºå®æ—¶æ¨ç†ã€‚

    æ³¨æ„: è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿå®ç°ã€‚çœŸå®çš„åœ¨çº¿æœåŠ¡ä¼šä½¿ç”¨ Feast çš„åœ¨çº¿å­˜å‚¨ã€‚

    å‚æ•°:
        - symbols: äº¤æ˜“å¯¹åˆ—è¡¨ï¼Œä¾‹å¦‚ ['EURUSD', 'GBPUSD']
        - features: ç‰¹å¾åç§°åˆ—è¡¨ï¼Œä¾‹å¦‚ ['rsi_14', 'bb_upper']

    è¿”å›:
        - status: 'success' æˆ– 'error'
        - data: äº¤æ˜“å¯¹ -> ç‰¹å¾å€¼çš„æ˜ å°„
        - execution_time_ms: æ‰§è¡Œæ—¶é—´

    ç¤ºä¾‹:
        ```json
        {
            "symbols": ["EURUSD", "GBPUSD"],
            "features": ["rsi_14", "bb_upper", "bb_lower"]
        }
        ```
    """
    try:
        logger.info(f"ğŸ“¥ æ”¶åˆ°å®æ—¶ç‰¹å¾è¯·æ±‚")
        logger.info(f"  Symbols: {request.symbols}")
        logger.info(f"  Features: {request.features}")

        # è·å–ç‰¹å¾æœåŠ¡
        service = get_feature_service()

        # è°ƒç”¨ç‰¹å¾æœåŠ¡
        data, execution_time = service.get_latest_features(
            symbols=request.symbols,
            features=request.features
        )

        response = LatestResponse(
            status="success",
            data=data,
            execution_time_ms=execution_time
        )

        logger.info(f"{GREEN}âœ… å®æ—¶ç‰¹å¾æ£€ç´¢æˆåŠŸ: {len(data)} ä¸ªç¬¦å·{RESET}")
        return response

    except ValidationError as e:
        logger.error(f"{RED}âŒ è¯·æ±‚éªŒè¯å¤±è´¥: {e}{RESET}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={
                "status": "error",
                "message": str(e),
                "error_code": "VALIDATION_ERROR"
            }
        )

    except Exception as e:
        logger.error(f"{RED}âŒ å¤„ç†è¯·æ±‚å¤±è´¥: {e}{RESET}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={
                "status": "error",
                "message": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}",
                "error_code": "INTERNAL_ERROR"
            }
        )


# ============================================================================
# æ¨¡å‹æ¨ç†ç«¯ç‚¹
# ============================================================================

@app.post(
    "/invocations",
    response_model=InvocationResponse,
    tags=["Inference"],
    summary="æ¨¡å‹æ¨ç† (MLflow Compatible)",
    description="æ¥æ”¶ç‰¹å¾æ•°æ®å¹¶è¿”å›æ¨¡å‹é¢„æµ‹ç»“æœ (å…¼å®¹ MLflow serving API)"
)
async def invocations(request: InvocationRequest):
    """
    MLflowå…¼å®¹çš„æ¨ç†ç«¯ç‚¹

    æ”¯æŒå¤šç§è¾“å…¥æ ¼å¼:
    1. dataframe_split (Sentinelå‘é€æ ¼å¼)
    2. instances (æ ‡å‡† MLflow)
    3. inputs (æ›¿ä»£æ ¼å¼)
    """
    try:
        logger.info(f"ğŸ“¥ æ”¶åˆ°æ¨ç†è¯·æ±‚")

        # Extract input data (support multiple MLflow formats)
        instances = None
        format_used = "unknown"

        if request.dataframe_split is not None:
            instances = request.dataframe_split.get("data", [])
            format_used = "dataframe_split"
        elif request.instances is not None:
            instances = request.instances
            format_used = "instances"
        elif request.inputs is not None:
            instances = request.inputs
            format_used = "inputs"
        else:
            raise ValueError("Missing 'dataframe_split', 'instances', or 'inputs' field")

        if not instances:
            raise ValueError("No data instances provided")

        logger.info(f"  Format: {format_used}, Input count: {len(instances)}")

        # Check if mock mode is enabled
        import os
        use_mock = os.getenv("ENABLE_MOCK_INFERENCE", "false").lower() == "true"

        predictions = []

        if use_mock:
            # Mock mode enabled - for integration testing only
            logger.warning(f"{RED}âš ï¸ [MOCK MODE] Generating random predictions - NOT FOR PRODUCTION{RESET}")
            import random
            for _ in instances:
                prob = random.uniform(0.4, 0.8)
                predictions.append([prob])
        else:
            # Real model inference (Task #080)
            from src.serving.feature_map import map_sentinel_to_model

            predictor = get_model_predictor()
            if predictor is None:
                raise HTTPException(
                    status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                    detail="Model predictor not available"
                )

            # Process each instance
            for instance_data in instances:
                try:
                    # Extract features from Sentinel's format
                    # Sentinel sends: [[X_tabular, X_sequential]] where X_tabular is (23,)
                    if isinstance(instance_data, list) and len(instance_data) >= 1:
                        # Get tabular features (first element)
                        sentinel_features = instance_data[0]  # Should be (23,) array

                        # Map Sentinel's 23 features to model's 15 features
                        mapped_features = map_sentinel_to_model(
                            sentinel_features,
                            model_features=predictor.feature_names
                        )

                        if mapped_features is None:
                            logger.error("Feature mapping failed")
                            raise ValueError("Feature mapping returned None")

                        # Run model inference
                        result = predictor.predict(mapped_features)

                        # Extract probability (model returns dict with 'probability' key)
                        probability = result.get('probability', 0.5)
                        predictions.append([probability])

                    else:
                        raise ValueError(
                            f"Invalid instance format: expected list with >=1 elements, "
                            f"got {type(instance_data)}"
                        )

                except Exception as e:
                    logger.error(f"Instance processing error: {e}")
                    # Fail gracefully with neutral prediction
                    predictions.append([0.5])

        logger.info(f"{GREEN}âœ… æ¨ç†å®Œæˆ: {len(predictions)} predictions{RESET}")

        return InvocationResponse(predictions=predictions)

    except Exception as e:
        logger.error(f"{RED}âŒ æ¨ç†å¤±è´¥: {e}{RESET}")
        # Fail fast - do not fallback
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )


# ============================================================================
# æ ¹ç«¯ç‚¹
# ============================================================================

@app.get(
    "/",
    tags=["Info"],
    summary="API ä¿¡æ¯",
    description="è·å– API åŸºæœ¬ä¿¡æ¯"
)
async def root():
    """æ ¹ç«¯ç‚¹ï¼Œè¿”å› API ä¿¡æ¯"""
    return {
        "name": "MT5 Feature Serving API",
        "version": "1.0.0",
        "docs_url": "/docs",
        "health_url": "/health",
        "endpoints": {
            "health": "GET /health",
            "historical_features": "POST /features/historical",
            "latest_features": "POST /features/latest",
            "invocations": "POST /invocations"
        }
    }


# ============================================================================
# é”™è¯¯å¤„ç†
# ============================================================================

@app.exception_handler(ValidationError)
async def validation_exception_handler(request, exc):
    """å¤„ç†éªŒè¯é”™è¯¯"""
    return JSONResponse(
        status_code=status.HTTP_400_BAD_REQUEST,
        content={
            "status": "error",
            "message": "è¯·æ±‚éªŒè¯å¤±è´¥",
            "error_code": "VALIDATION_ERROR",
            "details": str(exc)
        }
    )


@app.exception_handler(Exception)
async def general_exception_handler(request, exc):
    """å¤„ç†ä¸€èˆ¬å¼‚å¸¸"""
    logger.error(f"{RED}âŒ æœªå¤„ç†çš„å¼‚å¸¸: {exc}{RESET}")
    return JSONResponse(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        content={
            "status": "error",
            "message": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯",
            "error_code": "INTERNAL_ERROR"
        }
    )


if __name__ == "__main__":
    import uvicorn

    logger.info(f"{CYAN}ğŸš€ å¯åŠ¨ Feature Serving API...{RESET}")
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info"
    )
