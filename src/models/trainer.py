"""
æœºå™¨å­¦ä¹ è®­ç»ƒå™¨ - LightGBM + Optuna è¶…å‚æ•°ä¼˜åŒ–

æ ¸å¿ƒåŠŸèƒ½:
1. LightGBM æ¨¡å‹è®­ç»ƒ (æ”¯æŒæ ·æœ¬æƒé‡)
2. Optuna è´å¶æ–¯è¶…å‚æ•°ä¼˜åŒ–
3. Early Stopping é˜²æ­¢è¿‡æ‹Ÿåˆ
4. æ¨¡å‹æŒä¹…åŒ–

æ¥æº: "Advances in Financial Machine Learning" by Marcos Lopez de Prado
"""

import logging
import numpy as np
import pandas as pd
import pickle
import json
import lightgbm as lgb
import optuna
from typing import Dict, Optional, Tuple, Any, List
from pathlib import Path
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, log_loss
from sklearn.linear_model import LogisticRegression
try:
    import catboost as cb
    CATBOOST_AVAILABLE = True
except ImportError:
    CATBOOST_AVAILABLE = False
    logger.warning("CatBoost æœªå®‰è£…ï¼Œå°†è·³è¿‡ CatBoost æ¨¡å‹")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LightGBMTrainer:
    """
    LightGBM è®­ç»ƒå™¨ - æ”¯æŒæ ·æœ¬æƒé‡å’Œ Early Stopping

    å…³é”®ç‰¹æ€§:
    1. æ”¯æŒ sample_weight (æ¥è‡ª Triple Barrier)
    2. Early Stopping (é˜²æ­¢è¿‡æ‹Ÿåˆ)
    3. ç±»åˆ«ä¸å¹³è¡¡å¤„ç† (scale_pos_weight)
    4. éªŒè¯é›†è¯„ä¼°
    """

    def __init__(
        self,
        params: Optional[Dict[str, Any]] = None,
        early_stopping_rounds: int = 50,
        verbose: int = 50
    ):
        """
        Args:
            params: LightGBM å‚æ•°å­—å…¸
            early_stopping_rounds: Early Stopping è½®æ•°
            verbose: æ—¥å¿—è¾“å‡ºé¢‘ç‡
        """
        self.params = params or self._get_default_params()
        self.early_stopping_rounds = early_stopping_rounds
        self.verbose = verbose
        self.model = None
        self.best_iteration = None
        self.feature_names = None

    @staticmethod
    def _get_default_params() -> Dict[str, Any]:
        """è·å–é»˜è®¤å‚æ•°"""
        return {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'lambda_l1': 0.1,
            'lambda_l2': 0.1,
            'min_child_samples': 20,
            'max_depth': -1,
            'verbose': -1,
            'n_jobs': -1
        }

    def train(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_val: pd.DataFrame,
        y_val: pd.Series,
        sample_weight_train: Optional[pd.Series] = None,
        sample_weight_val: Optional[pd.Series] = None,
        num_boost_round: int = 1000
    ) -> 'LightGBMTrainer':
        """
        è®­ç»ƒ LightGBM æ¨¡å‹

        Args:
            X_train: è®­ç»ƒé›†ç‰¹å¾
            y_train: è®­ç»ƒé›†æ ‡ç­¾
            X_val: éªŒè¯é›†ç‰¹å¾
            y_val: éªŒè¯é›†æ ‡ç­¾
            sample_weight_train: è®­ç»ƒé›†æ ·æœ¬æƒé‡
            sample_weight_val: éªŒè¯é›†æ ·æœ¬æƒé‡
            num_boost_round: æœ€å¤§è¿­ä»£è½®æ•°

        Returns:
            self
        """
        logger.info(
            f"å¼€å§‹è®­ç»ƒ LightGBM: "
            f"è®­ç»ƒé›† {X_train.shape[0]} æ ·æœ¬, "
            f"éªŒè¯é›† {X_val.shape[0]} æ ·æœ¬, "
            f"ç‰¹å¾æ•° {X_train.shape[1]}"
        )

        # ä¿å­˜ç‰¹å¾å
        self.feature_names = X_train.columns.tolist()

        # åˆ›å»º LightGBM Dataset
        train_data = lgb.Dataset(
            X_train,
            label=y_train,
            weight=sample_weight_train,
            feature_name=self.feature_names
        )

        val_data = lgb.Dataset(
            X_val,
            label=y_val,
            weight=sample_weight_val,
            feature_name=self.feature_names,
            reference=train_data
        )

        # è®­ç»ƒæ¨¡å‹
        evals_result = {}
        self.model = lgb.train(
            self.params,
            train_data,
            num_boost_round=num_boost_round,
            valid_sets=[train_data, val_data],
            valid_names=['train', 'valid'],
            callbacks=[
                lgb.early_stopping(self.early_stopping_rounds),
                lgb.log_evaluation(self.verbose),
                lgb.record_evaluation(evals_result)
            ]
        )

        self.best_iteration = self.model.best_iteration
        logger.info(f"è®­ç»ƒå®Œæˆ! æœ€ä½³è¿­ä»£: {self.best_iteration}")

        # è¯„ä¼°æ€§èƒ½
        self._evaluate(X_val, y_val)

        return self

    def _evaluate(self, X_val: pd.DataFrame, y_val: pd.Series):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        y_pred = self.predict(X_val)
        y_pred_proba = self.predict_proba(X_val)[:, 1]

        accuracy = accuracy_score(y_val, y_pred)
        f1 = f1_score(y_val, y_pred, average='weighted')
        try:
            auc = roc_auc_score(y_val, y_pred_proba)
        except:
            auc = 0.0
        logloss = log_loss(y_val, y_pred_proba)

        logger.info(f"éªŒè¯é›†æ€§èƒ½:")
        logger.info(f"  Accuracy: {accuracy:.4f}")
        logger.info(f"  F1-Score: {f1:.4f}")
        logger.info(f"  AUC-ROC: {auc:.4f}")
        logger.info(f"  Log Loss: {logloss:.4f}")

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """é¢„æµ‹ç±»åˆ«"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒ,è¯·å…ˆè°ƒç”¨ train()")

        y_pred_proba = self.model.predict(X, num_iteration=self.best_iteration)
        return (y_pred_proba > 0.5).astype(int)

    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """é¢„æµ‹æ¦‚ç‡"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒ,è¯·å…ˆè°ƒç”¨ train()")

        y_pred_proba = self.model.predict(X, num_iteration=self.best_iteration)
        # è¿”å› [prob_class_0, prob_class_1] æ ¼å¼ (sklearn å…¼å®¹)
        return np.vstack([1 - y_pred_proba, y_pred_proba]).T

    def get_feature_importance(self, importance_type: str = 'gain') -> pd.Series:
        """
        è·å–ç‰¹å¾é‡è¦æ€§

        Args:
            importance_type: 'split' (åˆ†è£‚æ¬¡æ•°) æˆ– 'gain' (å¢ç›Š)

        Returns:
            ç‰¹å¾é‡è¦æ€§ Series
        """
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒ")

        importance = self.model.feature_importance(importance_type=importance_type)
        return pd.Series(importance, index=self.feature_names).sort_values(ascending=False)

    def save(self, model_path: str, metadata: Optional[Dict] = None):
        """
        ä¿å­˜æ¨¡å‹

        Args:
            model_path: æ¨¡å‹ä¿å­˜è·¯å¾„ (.pkl)
            metadata: é¢å¤–çš„å…ƒæ•°æ® (å¦‚ç‰¹å¾åç§°ã€è®­ç»ƒæ—¥æœŸç­‰)
        """
        model_dir = Path(model_path).parent
        model_dir.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜æ¨¡å‹
        with open(model_path, 'wb') as f:
            pickle.dump({
                'model': self.model,
                'params': self.params,
                'feature_names': self.feature_names,
                'best_iteration': self.best_iteration,
                'metadata': metadata or {}
            }, f)

        logger.info(f"æ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")

        # ä¿å­˜ LightGBM åŸç”Ÿæ ¼å¼ (ç”¨äºéƒ¨ç½²)
        lgb_model_path = model_path.replace('.pkl', '.txt')
        self.model.save_model(lgb_model_path, num_iteration=self.best_iteration)
        logger.info(f"LightGBM åŸç”Ÿæ¨¡å‹å·²ä¿å­˜è‡³: {lgb_model_path}")

    @classmethod
    def load(cls, model_path: str) -> 'LightGBMTrainer':
        """
        åŠ è½½æ¨¡å‹

        Args:
            model_path: æ¨¡å‹è·¯å¾„

        Returns:
            è®­ç»ƒå™¨å®ä¾‹
        """
        with open(model_path, 'rb') as f:
            data = pickle.load(f)

        trainer = cls(params=data['params'])
        trainer.model = data['model']
        trainer.feature_names = data['feature_names']
        trainer.best_iteration = data['best_iteration']

        logger.info(f"æ¨¡å‹å·²åŠ è½½: {model_path}")
        return trainer


class OptunaOptimizer:
    """
    Optuna è¶…å‚æ•°ä¼˜åŒ–å™¨

    ä½¿ç”¨ TPE (Tree-structured Parzen Estimator) é‡‡æ ·å™¨
    è¿›è¡Œè´å¶æ–¯ä¼˜åŒ–,æ¯” GridSearch é«˜æ•ˆ 10-100 å€
    """

    def __init__(
        self,
        n_trials: int = 100,
        timeout: Optional[int] = None,
        direction: str = 'maximize'
    ):
        """
        Args:
            n_trials: ä¼˜åŒ–è¯•éªŒæ¬¡æ•°
            timeout: è¶…æ—¶æ—¶é—´ (ç§’)
            direction: ä¼˜åŒ–æ–¹å‘ ('maximize' æˆ– 'minimize')
        """
        self.n_trials = n_trials
        self.timeout = timeout
        self.direction = direction
        self.study = None
        self.best_params = None

    def optimize(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_val: pd.DataFrame,
        y_val: pd.Series,
        sample_weight_train: Optional[pd.Series] = None,
        sample_weight_val: Optional[pd.Series] = None,
        metric: str = 'f1'
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œè¶…å‚æ•°ä¼˜åŒ–

        Args:
            X_train: è®­ç»ƒé›†ç‰¹å¾
            y_train: è®­ç»ƒé›†æ ‡ç­¾
            X_val: éªŒè¯é›†ç‰¹å¾
            y_val: éªŒè¯é›†æ ‡ç­¾
            sample_weight_train: è®­ç»ƒé›†æ ·æœ¬æƒé‡
            sample_weight_val: éªŒè¯é›†æ ·æœ¬æƒé‡
            metric: ä¼˜åŒ–æŒ‡æ ‡ ('f1', 'accuracy', 'auc', 'logloss')

        Returns:
            æœ€ä½³å‚æ•°å­—å…¸
        """
        logger.info(f"å¼€å§‹ Optuna è¶…å‚æ•°ä¼˜åŒ–: {self.n_trials} æ¬¡è¯•éªŒ, ä¼˜åŒ–æŒ‡æ ‡: {metric}")

        def objective(trial):
            # å®šä¹‰æœç´¢ç©ºé—´
            params = {
                'objective': 'binary',
                'metric': 'binary_logloss',
                'boosting_type': 'gbdt',
                'num_leaves': trial.suggest_int('num_leaves', 20, 150),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),
                'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),
                'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),
                'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),
                'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),
                'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
                'max_depth': trial.suggest_int('max_depth', 3, 12),
                'verbose': -1,
                'n_jobs': -1
            }

            # è®­ç»ƒæ¨¡å‹
            trainer = LightGBMTrainer(params=params, early_stopping_rounds=30, verbose=-1)
            trainer.train(
                X_train, y_train, X_val, y_val,
                sample_weight_train, sample_weight_val,
                num_boost_round=500
            )

            # è®¡ç®—æŒ‡æ ‡
            y_pred = trainer.predict(X_val)
            y_pred_proba = trainer.predict_proba(X_val)[:, 1]

            if metric == 'f1':
                score = f1_score(y_val, y_pred, average='weighted')
            elif metric == 'accuracy':
                score = accuracy_score(y_val, y_pred)
            elif metric == 'auc':
                try:
                    score = roc_auc_score(y_val, y_pred_proba)
                except:
                    score = 0.5
            elif metric == 'logloss':
                score = -log_loss(y_val, y_pred_proba)  # è´Ÿå·,å› ä¸ºè¦æœ€å¤§åŒ–
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æŒ‡æ ‡: {metric}")

            return score

        # åˆ›å»º study å¹¶ä¼˜åŒ–
        self.study = optuna.create_study(direction=self.direction)
        self.study.optimize(
            objective,
            n_trials=self.n_trials,
            timeout=self.timeout,
            show_progress_bar=True
        )

        self.best_params = self.study.best_params
        logger.info(f"ä¼˜åŒ–å®Œæˆ! æœ€ä½³ {metric}: {self.study.best_value:.4f}")
        logger.info(f"æœ€ä½³å‚æ•°: {self.best_params}")

        return self.best_params

    def save_study(self, output_path: str):
        """ä¿å­˜ä¼˜åŒ–å†å²"""
        if self.study is None:
            raise ValueError("æœªè¿è¡Œä¼˜åŒ–")

        study_df = self.study.trials_dataframe()
        study_df.to_csv(output_path, index=False)
        logger.info(f"ä¼˜åŒ–å†å²å·²ä¿å­˜è‡³: {output_path}")


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("=" * 80)
    print("æµ‹è¯•è®­ç»ƒå™¨æ¨¡å—")
    print("=" * 80)

    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    np.random.seed(42)
    n_samples = 2000

    X = pd.DataFrame({
        f'feature_{i}': np.random.randn(n_samples)
        for i in range(10)
    })
    y = pd.Series((X.sum(axis=1) + np.random.randn(n_samples) > 0).astype(int))

    # åˆ†å‰²æ•°æ®
    split_idx = int(n_samples * 0.8)
    X_train, X_val = X[:split_idx], X[split_idx:]
    y_train, y_val = y[:split_idx], y[split_idx:]

    # æµ‹è¯• LightGBM è®­ç»ƒå™¨
    print("\n1. æµ‹è¯• LightGBM è®­ç»ƒå™¨:")
    trainer = LightGBMTrainer(early_stopping_rounds=20, verbose=-1)
    trainer.train(X_train, y_train, X_val, y_val, num_boost_round=100)

    # ç‰¹å¾é‡è¦æ€§
    importance = trainer.get_feature_importance()
    print(f"\nTop 5 é‡è¦ç‰¹å¾:")
    print(importance.head(5))

    # ä¿å­˜å’ŒåŠ è½½
    model_path = '/opt/mt5-crs/outputs/models/test_model.pkl'
    trainer.save(model_path)
    loaded_trainer = LightGBMTrainer.load(model_path)
    print(f"\næ¨¡å‹ä¿å­˜å’ŒåŠ è½½æµ‹è¯•: {'âœ…' if loaded_trainer.model is not None else 'âŒ'}")

    print("\nâœ… è®­ç»ƒå™¨æµ‹è¯•å®Œæˆ!")


class CatBoostTrainer:
    """
    CatBoost è®­ç»ƒå™¨

    ä¼˜åŠ¿:
    - è‡ªåŠ¨å¤„ç†ç±»åˆ«ç‰¹å¾
    - å†…ç½®é˜²è¿‡æ‹Ÿåˆæœºåˆ¶ (Ordered Boosting)
    - è®­ç»ƒé€Ÿåº¦å¿«
    - å¯¹è¶…å‚æ•°ä¸æ•æ„Ÿ
    """

    def __init__(
        self,
        params: Optional[Dict[str, Any]] = None,
        early_stopping_rounds: int = 50,
        verbose: int = 50
    ):
        """
        Args:
            params: CatBoost å‚æ•°å­—å…¸
            early_stopping_rounds: Early Stopping è½®æ•°
            verbose: æ—¥å¿—è¾“å‡ºé¢‘ç‡
        """
        if not CATBOOST_AVAILABLE:
            raise ImportError("CatBoost æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install catboost")

        self.params = params or self._get_default_params()
        self.early_stopping_rounds = early_stopping_rounds
        self.verbose = verbose
        self.model = None
        self.feature_names = None

    @staticmethod
    def _get_default_params() -> Dict[str, Any]:
        """è·å–é»˜è®¤å‚æ•°"""
        return {
            'loss_function': 'Logloss',
            'eval_metric': 'Logloss',
            'iterations': 1000,
            'learning_rate': 0.05,
            'depth': 6,
            'l2_leaf_reg': 3.0,
            'random_strength': 1.0,
            'bagging_temperature': 1.0,
            'border_count': 128,
            'verbose': False,
            'thread_count': -1
        }

    def train(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_val: pd.DataFrame,
        y_val: pd.Series,
        sample_weight_train: Optional[pd.Series] = None,
        sample_weight_val: Optional[pd.Series] = None,
        cat_features: Optional[List[str]] = None
    ) -> 'CatBoostTrainer':
        """
        è®­ç»ƒ CatBoost æ¨¡å‹

        Args:
            X_train: è®­ç»ƒé›†ç‰¹å¾
            y_train: è®­ç»ƒé›†æ ‡ç­¾
            X_val: éªŒè¯é›†ç‰¹å¾
            y_val: éªŒè¯é›†æ ‡ç­¾
            sample_weight_train: è®­ç»ƒé›†æ ·æœ¬æƒé‡
            sample_weight_val: éªŒè¯é›†æ ·æœ¬æƒé‡
            cat_features: ç±»åˆ«ç‰¹å¾åˆ—ååˆ—è¡¨

        Returns:
            self
        """
        logger.info(
            f"å¼€å§‹è®­ç»ƒ CatBoost: "
            f"è®­ç»ƒé›† {X_train.shape[0]} æ ·æœ¬, "
            f"éªŒè¯é›† {X_val.shape[0]} æ ·æœ¬, "
            f"ç‰¹å¾æ•° {X_train.shape[1]}"
        )

        self.feature_names = X_train.columns.tolist()

        # åˆ›å»º CatBoost Pool
        train_pool = cb.Pool(
            X_train,
            label=y_train,
            weight=sample_weight_train,
            cat_features=cat_features
        )

        val_pool = cb.Pool(
            X_val,
            label=y_val,
            weight=sample_weight_val,
            cat_features=cat_features
        )

        # è®­ç»ƒæ¨¡å‹
        self.model = cb.CatBoostClassifier(**self.params)
        self.model.fit(
            train_pool,
            eval_set=val_pool,
            early_stopping_rounds=self.early_stopping_rounds,
            verbose=self.verbose
        )

        logger.info(f"è®­ç»ƒå®Œæˆ! æœ€ä½³è¿­ä»£: {self.model.best_iteration_}")

        # è¯„ä¼°æ€§èƒ½
        self._evaluate(X_val, y_val)

        return self

    def _evaluate(self, X_val: pd.DataFrame, y_val: pd.Series):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        y_pred = self.predict(X_val)
        y_pred_proba = self.predict_proba(X_val)[:, 1]

        accuracy = accuracy_score(y_val, y_pred)
        f1 = f1_score(y_val, y_pred, average='weighted')
        try:
            auc = roc_auc_score(y_val, y_pred_proba)
        except:
            auc = 0.0
        logloss = log_loss(y_val, y_pred_proba)

        logger.info(f"éªŒè¯é›†æ€§èƒ½:")
        logger.info(f"  Accuracy: {accuracy:.4f}")
        logger.info(f"  F1-Score: {f1:.4f}")
        logger.info(f"  AUC-ROC: {auc:.4f}")
        logger.info(f"  Log Loss: {logloss:.4f}")

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """é¢„æµ‹ç±»åˆ«"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒ,è¯·å…ˆè°ƒç”¨ train()")
        return self.model.predict(X).astype(int)

    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """é¢„æµ‹æ¦‚ç‡"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒ,è¯·å…ˆè°ƒç”¨ train()")
        return self.model.predict_proba(X)

    def get_feature_importance(self) -> pd.Series:
        """è·å–ç‰¹å¾é‡è¦æ€§"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒ")
        importance = self.model.get_feature_importance()
        return pd.Series(importance, index=self.feature_names).sort_values(ascending=False)

    def save(self, model_path: str):
        """ä¿å­˜æ¨¡å‹"""
        model_dir = Path(model_path).parent
        model_dir.mkdir(parents=True, exist_ok=True)

        self.model.save_model(model_path)
        logger.info(f"CatBoost æ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")

    @classmethod
    def load(cls, model_path: str) -> 'CatBoostTrainer':
        """åŠ è½½æ¨¡å‹"""
        trainer = cls()
        trainer.model = cb.CatBoostClassifier()
        trainer.model.load_model(model_path)
        logger.info(f"CatBoost æ¨¡å‹å·²åŠ è½½: {model_path}")
        return trainer


class EnsembleStacker:
    """
    Ensemble Stacking æ¨¡å‹èåˆå™¨

    æ¶æ„:
    - ç¬¬ä¸€å±‚ (Base Learners): LightGBM + CatBoost
    - ç¬¬äºŒå±‚ (Meta Learner): Logistic Regression

    ä¼˜åŠ¿:
    - èåˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ï¼Œæå‡æ³›åŒ–èƒ½åŠ›
    - Meta Learner å­¦ä¹ å¦‚ä½•ç»„åˆåŸºæ¨¡å‹çš„è¾“å‡º
    - é™ä½å•ä¸€æ¨¡å‹çš„è¿‡æ‹Ÿåˆé£é™©

    æµç¨‹:
    1. ä½¿ç”¨ Purged K-Fold è®­ç»ƒå¤šä¸ªåŸºæ¨¡å‹
    2. æ”¶é›†åŸºæ¨¡å‹çš„ Out-of-Fold é¢„æµ‹
    3. ä½¿ç”¨ OOF é¢„æµ‹è®­ç»ƒ Meta Learner
    4. æœ€ç»ˆé¢„æµ‹ = Meta Learner(Base Model Predictions)
    """

    def __init__(
        self,
        use_catboost: bool = True,
        lgb_params: Optional[Dict] = None,
        cb_params: Optional[Dict] = None
    ):
        """
        Args:
            use_catboost: æ˜¯å¦ä½¿ç”¨ CatBoost (å¦‚æœæœªå®‰è£…åˆ™è‡ªåŠ¨è·³è¿‡)
            lgb_params: LightGBM å‚æ•°
            cb_params: CatBoost å‚æ•°
        """
        self.use_catboost = use_catboost and CATBOOST_AVAILABLE
        self.lgb_params = lgb_params
        self.cb_params = cb_params

        self.lgb_models = []
        self.cb_models = []
        self.meta_model = None
        self.feature_names = None

    def train(
        self,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_val: pd.DataFrame,
        y_val: pd.Series,
        sample_weight_train: Optional[pd.Series] = None,
        cv_splitter = None,
        pred_times: Optional[pd.Series] = None
    ) -> 'EnsembleStacker':
        """
        è®­ç»ƒ Stacking æ¨¡å‹

        Args:
            X_train: è®­ç»ƒé›†ç‰¹å¾
            y_train: è®­ç»ƒé›†æ ‡ç­¾
            X_val: éªŒè¯é›†ç‰¹å¾
            y_val: éªŒè¯é›†æ ‡ç­¾
            sample_weight_train: è®­ç»ƒé›†æ ·æœ¬æƒé‡
            cv_splitter: äº¤å‰éªŒè¯åˆ†å‰²å™¨ (å¦‚ PurgedKFold)
            pred_times: æ ‡ç­¾ç»“æŸæ—¶é—´ (ç”¨äº Purging)

        Returns:
            self
        """
        logger.info("=" * 80)
        logger.info("å¼€å§‹è®­ç»ƒ Ensemble Stacking æ¨¡å‹")
        logger.info("=" * 80)

        self.feature_names = X_train.columns.tolist()

        # æ­¥éª¤ 1: è®­ç»ƒåŸºæ¨¡å‹å¹¶ç”Ÿæˆ Out-of-Fold é¢„æµ‹
        logger.info("\n[æ­¥éª¤ 1/3] è®­ç»ƒåŸºæ¨¡å‹...")

        oof_predictions = {}

        # LightGBM åŸºæ¨¡å‹
        logger.info("\nè®­ç»ƒ LightGBM åŸºæ¨¡å‹...")
        lgb_oof = self._train_base_model(
            'lightgbm',
            X_train, y_train, sample_weight_train,
            cv_splitter, pred_times
        )
        oof_predictions['lgb'] = lgb_oof

        # CatBoost åŸºæ¨¡å‹
        if self.use_catboost:
            logger.info("\nè®­ç»ƒ CatBoost åŸºæ¨¡å‹...")
            cb_oof = self._train_base_model(
                'catboost',
                X_train, y_train, sample_weight_train,
                cv_splitter, pred_times
            )
            oof_predictions['cb'] = cb_oof

        # æ­¥éª¤ 2: è®­ç»ƒ Meta Learner
        logger.info("\n[æ­¥éª¤ 2/3] è®­ç»ƒ Meta Learner...")

        # æ„å»º Meta ç‰¹å¾ (åŸºæ¨¡å‹çš„ OOF é¢„æµ‹)
        meta_features_train = pd.DataFrame(oof_predictions)

        # ç®€å•çš„ Logistic Regression ä½œä¸º Meta Learner
        self.meta_model = LogisticRegression(
            penalty='l2',
            C=1.0,
            max_iter=1000,
            random_state=42
        )
        self.meta_model.fit(meta_features_train, y_train)

        logger.info("Meta Learner è®­ç»ƒå®Œæˆ!")
        logger.info(f"Meta ç‰¹å¾æƒé‡: {dict(zip(meta_features_train.columns, self.meta_model.coef_[0]))}")

        # æ­¥éª¤ 3: åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
        logger.info("\n[æ­¥éª¤ 3/3] éªŒè¯é›†è¯„ä¼°...")
        self._evaluate(X_val, y_val)

        logger.info("\nâœ… Ensemble Stacking è®­ç»ƒå®Œæˆ!")

        return self

    def _train_base_model(
        self,
        model_type: str,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        sample_weight: Optional[pd.Series],
        cv_splitter,
        pred_times: Optional[pd.Series]
    ) -> np.ndarray:
        """
        è®­ç»ƒå•ä¸ªåŸºæ¨¡å‹å¹¶ç”Ÿæˆ Out-of-Fold é¢„æµ‹

        Args:
            model_type: 'lightgbm' æˆ– 'catboost'
            X_train: è®­ç»ƒé›†ç‰¹å¾
            y_train: è®­ç»ƒé›†æ ‡ç­¾
            sample_weight: æ ·æœ¬æƒé‡
            cv_splitter: äº¤å‰éªŒè¯åˆ†å‰²å™¨
            pred_times: æ ‡ç­¾ç»“æŸæ—¶é—´

        Returns:
            Out-of-Fold é¢„æµ‹æ¦‚ç‡ (shape: [n_samples])
        """
        oof_predictions = np.zeros(len(X_train))

        # å¦‚æœæ²¡æœ‰æä¾› cv_splitterï¼Œä½¿ç”¨ç®€å•çš„ train/val åˆ†å‰²
        if cv_splitter is None:
            split_idx = int(len(X_train) * 0.8)
            folds = [(np.arange(split_idx), np.arange(split_idx, len(X_train)))]
        else:
            folds = list(cv_splitter.split(X_train, y_train, event_ends=pred_times))

        for fold_num, (train_idx, val_idx) in enumerate(folds, 1):
            logger.info(f"  Fold {fold_num}/{len(folds)}: è®­ç»ƒ {model_type}...")

            X_fold_train = X_train.iloc[train_idx]
            y_fold_train = y_train.iloc[train_idx]
            X_fold_val = X_train.iloc[val_idx]
            y_fold_val = y_train.iloc[val_idx]

            weight_fold_train = sample_weight.iloc[train_idx] if sample_weight is not None else None
            weight_fold_val = sample_weight.iloc[val_idx] if sample_weight is not None else None

            # è®­ç»ƒæ¨¡å‹
            if model_type == 'lightgbm':
                trainer = LightGBMTrainer(
                    params=self.lgb_params,
                    early_stopping_rounds=30,
                    verbose=-1
                )
                trainer.train(
                    X_fold_train, y_fold_train,
                    X_fold_val, y_fold_val,
                    weight_fold_train, weight_fold_val,
                    num_boost_round=500
                )
                self.lgb_models.append(trainer)

            elif model_type == 'catboost':
                trainer = CatBoostTrainer(
                    params=self.cb_params,
                    early_stopping_rounds=30,
                    verbose=0
                )
                trainer.train(
                    X_fold_train, y_fold_train,
                    X_fold_val, y_fold_val,
                    weight_fold_train, weight_fold_val
                )
                self.cb_models.append(trainer)

            # ç”Ÿæˆ OOF é¢„æµ‹
            oof_predictions[val_idx] = trainer.predict_proba(X_fold_val)[:, 1]

        logger.info(f"  {model_type} OOF é¢„æµ‹ç”Ÿæˆå®Œæˆ!")

        return oof_predictions

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """é¢„æµ‹ç±»åˆ«"""
        proba = self.predict_proba(X)
        return (proba[:, 1] > 0.5).astype(int)

    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """é¢„æµ‹æ¦‚ç‡"""
        if self.meta_model is None:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒ,è¯·å…ˆè°ƒç”¨ train()")

        # æ”¶é›†åŸºæ¨¡å‹é¢„æµ‹
        base_predictions = {}

        # LightGBM é¢„æµ‹ (å–å¤šä¸ª fold çš„å¹³å‡)
        lgb_preds = np.mean([
            model.predict_proba(X)[:, 1]
            for model in self.lgb_models
        ], axis=0)
        base_predictions['lgb'] = lgb_preds

        # CatBoost é¢„æµ‹
        if self.use_catboost and len(self.cb_models) > 0:
            cb_preds = np.mean([
                model.predict_proba(X)[:, 1]
                for model in self.cb_models
            ], axis=0)
            base_predictions['cb'] = cb_preds

        # æ„å»º Meta ç‰¹å¾
        meta_features = pd.DataFrame(base_predictions)

        # Meta Learner é¢„æµ‹
        return self.meta_model.predict_proba(meta_features)

    def _evaluate(self, X_val: pd.DataFrame, y_val: pd.Series):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        y_pred = self.predict(X_val)
        y_pred_proba = self.predict_proba(X_val)[:, 1]

        accuracy = accuracy_score(y_val, y_pred)
        f1 = f1_score(y_val, y_pred, average='weighted')
        try:
            auc = roc_auc_score(y_val, y_pred_proba)
        except:
            auc = 0.0
        logloss = log_loss(y_val, y_pred_proba)

        logger.info(f"\nğŸ¯ Ensemble éªŒè¯é›†æ€§èƒ½:")
        logger.info(f"  Accuracy: {accuracy:.4f}")
        logger.info(f"  F1-Score: {f1:.4f}")
        logger.info(f"  AUC-ROC: {auc:.4f}")
        logger.info(f"  Log Loss: {logloss:.4f}")

    def save(self, output_dir: str):
        """ä¿å­˜ Ensemble æ¨¡å‹"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜ LightGBM æ¨¡å‹
        for i, model in enumerate(self.lgb_models):
            model.save(str(output_path / f'lgb_fold_{i}.pkl'))

        # ä¿å­˜ CatBoost æ¨¡å‹
        if self.use_catboost:
            for i, model in enumerate(self.cb_models):
                model.save(str(output_path / f'cb_fold_{i}.cbm'))

        # ä¿å­˜ Meta Learner
        with open(output_path / 'meta_model.pkl', 'wb') as f:
            pickle.dump(self.meta_model, f)

        logger.info(f"Ensemble æ¨¡å‹å·²ä¿å­˜è‡³: {output_dir}")

    @classmethod
    def load(cls, input_dir: str) -> 'EnsembleStacker':
        """åŠ è½½ Ensemble æ¨¡å‹"""
        input_path = Path(input_dir)

        stacker = cls()

        # åŠ è½½ LightGBM æ¨¡å‹
        lgb_files = sorted(input_path.glob('lgb_fold_*.pkl'))
        for lgb_file in lgb_files:
            model = LightGBMTrainer.load(str(lgb_file))
            stacker.lgb_models.append(model)

        # åŠ è½½ CatBoost æ¨¡å‹
        if CATBOOST_AVAILABLE:
            cb_files = sorted(input_path.glob('cb_fold_*.cbm'))
            for cb_file in cb_files:
                model = CatBoostTrainer.load(str(cb_file))
                stacker.cb_models.append(model)
            stacker.use_catboost = len(stacker.cb_models) > 0

        # åŠ è½½ Meta Learner
        with open(input_path / 'meta_model.pkl', 'rb') as f:
            stacker.meta_model = pickle.load(f)

        logger.info(f"Ensemble æ¨¡å‹å·²åŠ è½½: {input_dir}")
        return stacker
